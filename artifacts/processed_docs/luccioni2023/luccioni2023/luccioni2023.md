# Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning

ALEXANDRA SASHA LUCCIONI, Hugging Face, Montreal, Canada ALEX HERNANDEZ-GARCIA, Mila, Université de Montréal, Montreal, Canada

Machine learning (ML) requires using energy to carry out computations during the model training process. The generation of this energy comes with an environmental cost in terms of greenhouse gas emissions, depending on quantity used and the energy source. Existing research on the environmental impacts of ML has been limited to analyses covering a small number of models and does not adequately represent the diversity of ML models and tasks. In the current study, we present a survey of the carbon emissions of 95 ML models across time and different tasks in natural language processing and computer vision. We analyze them in terms of the energy sources used, the amount of CO2 emissions produced, how these emissions evolve across time and how they relate to model performance. We conclude with a discussion regarding the carbon footprint of our field and propose the creation of a centralized repository for reporting and tracking these emissions.

# <span id="page-0-0"></span>1 INTRODUCTION

In recent years, machine learning (ML) models have achieved high performance in a multitude of tasks such as image classification, machine translation, and object detection. However, this progress also comes with a cost in terms of energy, since developing and deploying ML models requires access to computational resources such as Graphical Processing Units (GPUs) and therefore energy to power them. In turn, producing this energy comes with a cost to the environment, given that energy generation often entails the emission of greenhouse gases (GHG) such as carbon dioxide (CO2) [\[40\]](#page-14-0). On a global scale, electricity generation represents over a quarter of the global GHG emissions, adding up to 33.1 gigatonnes of CO2 in 2019 [\[24\]](#page-13-0). Recent estimates put the contribution of the information and communications technology (ICT) sector – which includes the data centers, devices and networks used for training and deploying ML models – at 2–6 % of global GHG emissions, although the exact number is still debated [\[25,](#page-13-1) [32,](#page-14-1) [36\]](#page-14-2). In fact, there is limited information about the overall energy consumption and carbon footprint of our field, how it is evolving, and how it correlates with performance on different tasks.

The goal of the current paper is to analyze the main factors influencing the carbon emissions of our field, to study the evolution across time, and to contribute towards a better understanding of the carbon emissions generated by ML models trained on different tasks and as a function of their performance. As such, our research aims to answer the following research questions:

- (1) What are the main sources of energy used for training ML models?
- (2) What is the order of magnitude of CO2 emissions produced by training ML models?
- (3) How do the CO2 emissions produced by training ML models evolve over time?
- (4) Does more energy and CO2 lead to better model performance?

Authors' addresses: Alexandra Sasha Luccioni, Hugging Face, Montreal, Canada, sasha.luccioni@huggingface.co; Alex Hernandez-Garcia, Mila, Université de Montréal, Montreal, Canada, alex.hernandez-garcia@mila.quebec.

We start our article with a survey of related work in Section [2,](#page-1-0) followed by a presentation of our methodology in Section [3.](#page-2-0) In Section [4](#page-4-0) we present our analysis, and we conclude with our proposals for future work, including a centralized hub for reporting the carbon footprint of machine learning..

# <span id="page-1-0"></span>2 RELATED WORK

Measuring the environmental impact of ML models is a relatively new undertaking, but one that has been gathering momentum in recent years. In the current section, we present several directions pursued in this domain, from empirical studies of specific models to the development of efficient algorithms and hardware.

Empirical studies on carbon emissions. A large proportion of research has focused on estimating the carbon emissions of specific model architectures and/or comparing the carbon emissions of two or more models and approaches. The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars. [\[48\]](#page-14-3). This perspective has since been explored further via analyses of the carbon footprint of different neural network architectures [\[31,](#page-14-4) [37,](#page-14-5) [38\]](#page-14-6) and the relative efficiency of different methods [\[35,](#page-14-7) [56\]](#page-15-0). These empirical studies are very recent (post-2019), remain relatively sparse and biased towards certain research areas (i.e. Natural Language Processing), and there are many aspects of the emissions of model training that remain unexplored. In sum, there is a need for a more broad and multi-faceted analysis in order to better understand the scale and variation of carbon emissions in our community.

Tools and approaches for measuring carbon emissions. Developing standardized approaches for estimating the carbon emissions of model training has also been the focus of much work [\[5,](#page-13-2) [20,](#page-13-3) [26,](#page-13-4) [27,](#page-14-8) [30,](#page-14-9) [45,](#page-14-10) [51\]](#page-14-11). As a result, there are several tools that exist for this purpose, such as [Code Carbon](https://github.com/mlco2/codecarbon) and the [Experiment Impact Tracker,](https://github.com/Breakend/experiment-impact-tracker) which can be used during the model training process, or the [ML CO2 Calculator,](https://mlco2.github.io/impact/) which can be used after training, all of which provide an estimate of the amount of carbon emitted. However, a recent study on different carbon estimation tools concluded that the estimates produced by different tools vary significantly and consistently under-report emissions [\[7\]](#page-13-5). To date, there is no single, accepted approach for estimating the carbon emissions of the field, making standardized reporting and comparisons difficult [\[31\]](#page-14-4).

Broader impacts of ML models. Several papers have been written in recent years regarding the broader societal impacts of ML models, which includes their environmental footprint. This spans research on how the size and computational demands of ML models in general [\[50\]](#page-14-12) and large language models in particular [\[8,](#page-13-6) [11\]](#page-13-7) have grown in recent years. Many strategies and directions forward have been proposed, ranging from advocating for more environmentally-conscious practice of AI [\[46\]](#page-14-13) to adopting a sustainability mindset for the community [\[54\]](#page-15-1). However, while the documentation of aspects such as bias and safety has begun to be described in reports and articles accompanying certain recent ML models (e.g. [\[12,](#page-13-8) [22\]](#page-13-9)), environmental impacts have yet to be consistently tracked and reported. Notable exceptions include recent language models such as OPT [\[57\]](#page-15-2), T0 [\[43\]](#page-14-14) and BLOOM [\[31\]](#page-14-4).

Efficient algorithms and hardware. A related and complementary direction of research is the development of more efficient model architectures and approaches. For instance, approaches such as Eyeriss [\[13\]](#page-13-10) and DistilBERT [\[42\]](#page-14-15) have made significant progress in terms of computing efficiency, enabling faster training and inference, which results in less energy usage and, indirectly, less carbon emissions, during model training. This research is gathering attention within the community, with workshops such as [SustaiNLP](https://sites.google.com/view/sustainlp2022/home) and [EMC2](https://www.emc2-ai.org/neurips-19) growing in scope and popularity, although efficiency Manuscript pending review

has yet to be a central consideration when it comes to evaluating and comparing models. However, energy-efficient benchmarks such as HULK [\[58\]](#page-15-3) have also been proposed, which take computational requirements and environmental impacts into account during model evaluation, allowing a comparison of models based on multiple criteria.

Other aspects of the carbon impact of ML. Finally, efforts have been made to quantify other factors that have an influence on the overall carbon footprint of the field of ML, including in-person versus virtual conference attendance [\[47\]](#page-14-16), the manufacturing of computing hardware [\[19\]](#page-13-11), life cycle analysis of the entire ML development and deployment cycle [\[28\]](#page-14-17), as well as some initial studies regarding the carbon footprint of model deployment in production settings [\[31\]](#page-14-4). The relative contribution of each of these factors is still unclear, which suggests that further research is needed in order to further disentangle these factors.

### <span id="page-2-0"></span>3 METHODOLOGY

As stated in Section [1,](#page-0-0) the goal of this paper is descriptive – to observe the evolution of the carbon emissions of our field of ML across time and to analyze the different aspects of the carbon emissions produced by training ML models. In this section, we present the different aspects and details of our methodology.

# 3.1 Data collection

In order to gather data from a diverse set of ML models from a variety of domains and tasks, we leveraged the dataset collected by Thompson et al. [\[50\]](#page-14-12) in the scope of a recent study on the computational requirements of ML. From this dataset, we equally sampled 500 papers published from 2012 to 2021 spanning 5 tasks: Image Classification, Object Detection, Machine Translation, Question Answering and Named Entity Recognition. We then contacted the first author of each of the papers and asked them to provide missing training details regarding their model (See Supplementary Materials [A.1](#page-16-0) for the email text). We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4 %.

<span id="page-2-1"></span>Task Dataset Number of Models Publication dates Image Classification ImageNet [\[14\]](#page-13-12) 35 2012-2021 Machine Translation WMT2014 [\[10\]](#page-13-13) 30 2016-2021 Named Entity Recognition CoNLL 2003 [\[41\]](#page-14-18) 11 2015-2021 Question Answering SQuAD 1.1 [\[39\]](#page-14-19) 10 2016-2021 Object Detection MS COCO [\[29\]](#page-14-20) 9 2019-2021

Table 1. Summary of the models analyzed in our study

The models in our sample cover a diversity of tasks spanning nine years of research in the field and a variety of conferences and journals. They all represent novel architectures at the time of publication, achieving high performance in their respective tasks: on average, the models are within 8 % of SOTA performance according to [Papers With Code](https://paperswithcode.com/) leaderboards at the time of their publication . This sample represents the largest amount of information regarding the carbon footprint of ML model training to date, and provides us with opportunities to analyze it from a variety of angles, which we present in Section [4.](#page-4-0) In the remaining of this section, we describe our method for estimating carbon emissions.

### 3.2 Estimating carbon emissions

The unit of measurement typically used for quantifying and comparing carbon emissions is CO2 equivalents. This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of CO<sup>2</sup> emitted per kilowatt hour of electricity generated (gCO2eq/kWh) [1](#page-3-0) .

The amount of CO2eq () emitted during model training can be decomposed into three relevant factors: the power consumption of the hardware used (), the training time ( ) and the carbon intensity of the energy grid (); or equivalently, the energy consumed () and the carbon intensity:

$$C = P \times T \times I = E \times I. \tag{1}$$

For instance, a model trained on a single GPU consuming 300 W for 100 hours on a grid that emits 500 gCO2eq/kWh will emit 0.3 kW × 100 h × 500 g/kWh = 15000 g = 15 kg of CO2eq. The same model trained on a less carbon-intensive energy grid, emitting only 100 gCO2eq/kWh, will only emit 0.3 × 100 × 100 = 3000 g = 3 kg of CO2eq, i.e. five times less overall. In our email to authors, we asked them to provide the details we needed to carry out this calculation, i.e the location of the computer or server where their model was trained (either cloud or local), the hardware used, and the total model training time. We describe how we estimate each of the relevant factors in the paragraphs below:

Carbon Intensity. Based on the training location provided by authors, we were able to estimate the carbon intensity of the energy grid that was utilized, based on publicly-available sources such as the [International Energy Agency](https://www.iea.org/) and the [Energy Information Administration.](https://www.eia.gov/) The granularity of information available ranges widely depending on the location – whereas in countries such as the United States, it is available at a sub-state (sometimes even at a sub-zip code) level, in others such as China, only country-level information is available. The carbon intensity figures that we use are yearly averages for the year the model was trained, given that these can evolve over time. In cases when the authors indicated that they used a computing infrastructure internal to a company, we consulted company reports and publications (e.g. [\[16,](#page-13-14) [38\]](#page-14-6)) to obtain more precise information regarding the carbon intensity, including the usage of local renewable energy sources. In cases when models were trained on commercial cloud computing platforms such as Google Cloud or Amazon Web Services (AWS), we used the information provided by the companies themselves to estimate emission factors [\[4,](#page-13-15) [18\]](#page-13-16).

Hardware power. In order to calculate the power consumption of the hardware used for model training, we refer to its Thermal Design Power, or TDP, which indicates the energy it needs under the maximum theoretical load. That is, the higher the TDP, the more power is consumed. While in practice GPUs are not always fully utilized during all parts of the training process, gathering more precise information regarding real-time power consumption is only possible by using a tool like Code Carbon during the training process [\[45\]](#page-14-10). Nonetheless, the TDP-based approach is often used in practice when estimating the carbon emissions of AI model training [\[38\]](#page-14-6) and it remains a fair approximation of the actual energy consumption of many hardware models. We provide more information about TDP and the hardware used for training the models in our sample in Section [A.2](#page-16-1) of the Appendix.

Training Time. Training time was computed as the total number of hardware hours, which is different from the "wall time" of ML model training, since most models were trained on multiple units at once. For instance, if training a

<span id="page-3-0"></span><sup>1</sup> For instance, methane is 28 times more potent than CO2 based on its 100-year global warming potential, so energy generation emitting 1 gram of methane per kWh will emit 28 grams of CO2eq per kWh.

model used 16 GPUs for 24 hours, this equals a training time of 384 GPU hours; a model using 8 GPUs for 48 hours will therefore have an equivalent training time.

# <span id="page-4-0"></span>4 DATA ANALYSIS

In the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the main sources of energy used for training (§ [4.1\)](#page-4-1), the order of magnitude of CO2 emissions produced (§ [4.2\)](#page-5-0), the evolution of these emissions over time (§ [4.3\)](#page-7-0) and the relationship between carbon emissions and model performance (§ [4.4\)](#page-8-0) [2](#page-4-2) .

# <span id="page-4-1"></span>4.1 What are the main sources of energy used for training ML models?

The primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity of that grid, in the face of the large differences between energy sources. For instance, renewable energy sources like hydroelectricity, solar and wind have low carbon intensity (ranging from 11 to 147 gCO2eq/kWh), whereas nonrenewable energy sources like coal, natural gas and oil are generally orders of magnitude more carbon-intensive (ranging from 360 to 680 gCO2eq/kWh) [\[24,](#page-13-0) [44\]](#page-14-21). That means that the energy source that powers the hardware to train ML models can result in differences of up to 60 times more CO2eq in terms of total emissions.

<span id="page-4-3"></span>Main energy source Number of Models Low-Carbon? Average Carbon Intensity (gCO2eq/kWh) Coal 38 No 512.3 Natural Gas 23 No 350.5 Hydroelectricity 19 Yes 100.6 Oil 12 No 453.6 Nuclear 3 Yes 147.2

Table 2. Main Energy Sources for the models analyzed and their carbon intensities [\[24,](#page-13-0) [52\]](#page-14-22)

In Table [2,](#page-4-3) we show the principal energy source used by the models from our sample, as well as its average carbon intensity. We found that the majority of models (61) from our sample used high-carbon energy sources such as coal and natural gas as their primary energy source. whereas less than a quarter of the models (34) used low-carbon energy sources like hydroelectricity and nuclear energy [3](#page-4-4) . While the average carbon intensity used for training the models from our sample (372 gCO2eq/kWh) is lower than the average global carbon intensity (475 gCO2eq/kWh), this still leaves much to improve in terms of carbon emissions of our field by switching to renewable energy sources (we discuss this further in Section [5\)](#page-9-0).

In Figure [1,](#page-5-1) we show the model training locations reported by authors on a country-level, with the median carbon intensity of each country indicated below. In terms of the model training locations reported by authors, we found a very imbalanced distribution, with the vast majority of models being trained in a small number of countries – half of the models in our sample were trained in the United States (48), followed by China (18), with the rest of the models distributed across 9 other countries, with only a few papers in each. Regarding the primary energy sources, based on this country-level analysis of energy grids used for training the models in our sample, we found that most common

<span id="page-4-2"></span><sup>2</sup>We have made the data used for our analysis available in a [GitHub repository.](https://github.com/alexhernandezgarcia/co2ml)

<span id="page-4-4"></span><sup>3</sup>Although the sustainability of nuclear energy is debated, it is one of the least carbon-intensive sources of electricity that currently exists. More information about nuclear energy and its long-term impacts on the environment can be found in [\[6\]](#page-13-17) and [\[49\]](#page-14-23).

countries where model training was carried out (e.g. the US and China), are on the high end of the carbon spectrum, with emissions of 350 gCO2eq/kWh and above. On the other end, the countries with the lowest carbon intensity in our sample are Canada (which ranges between 1.30 and 52.89 gCO2eq/kWh, depending on the province) and Spain (which has a single national energy grid with a median carbon intensity of 220.26 gCO2eq/kWh), but they only represents a total of 7 models from our sample. This is similar to patterns in emissions worldwide, where a small number of highly industrialized countries produce the majority of the world's greenhouse gases [\[17\]](#page-13-18).

<span id="page-5-1"></span>![](_page_5_Figure_3.jpeg)

# <span id="page-5-0"></span>4.2 What is the order of magnitude of CO2 emissions produced by training ML models?

As explained in Section [3,](#page-2-0) there is a linear relationship between the energy consumed and the carbon emissions produced, with the energy source (discussed in the Section above) influencing the magnitude of this relationship. In Figure [2,](#page-6-0) we plot the energy consumed (X axis, logarithmic scale) and the CO2 emitted (Y axis, logarithmic scale) of every model in our data set, color-coded with the main energy source, which are the same as those presented in Table [2.](#page-4-3) First, we can observe differences of several orders of magnitude in the energy used by models in our sample, ranging from just about 10 kWh to more than 10,000 kWh, which results in similar differences in the total quantity of CO2 emitted. As expected, the relationship between energy consumed and carbon emitted is largely linear. However, Figure [2](#page-6-0) Manuscript pending review

also shows that models trained with cleaner energy sources, such hydroelectricity, largely deviate from the main trend, with orders of magnitude less carbon emissions compared to models trained using coal and gas. In other words, models trained with low carbon-intensive energy sources, result in much less carbon emissions, ceteris paribus.

<span id="page-6-0"></span>![](_page_6_Figure_3.jpeg)

# <span id="page-7-0"></span>4.3 How do the CO2 emissions produced by training ML models evolve over time?

Some recent analyses have predicted that the carbon emissions of our field will increase in the future, estimating that achieving further progress on benchmarks such as ImageNet will require emitting thousands of tons of CO2 [\[50\]](#page-14-12), whereas others have predicted a plateau in future emissions due to increased hardware efficiency and carbon offsetting [\[37\]](#page-14-5). Therefore, one of the goals of our study was to observe the evolution of carbon emissions over time and study whether there are clear trends. Given that the papers from our study span from 2012 to the present time, we aimed to specifically compare whether new generations of ML models from our sample consistently used more energy and emitted more carbon than previous ones.

<span id="page-7-1"></span>![](_page_7_Figure_4.jpeg)

# <span id="page-8-0"></span>4.4 Does more energy and CO2 lead to better model performance?

A final perspective from which we analyze the carbon emissions of ML models is by comparing the amount of carbon emitted by models to their performance on benchmark tasks such as image classification, machine translation and question answering. We compare the emissions of the models from our sample and their performance on four tasks: image recognition on ImageNet [\[14\]](#page-13-12) (35 models), machine translation for English-French and English-German on the 2014 WMT Translation tasks [\[10\]](#page-13-13) (30 models), question answering on the SQuAD 1.1 dataset [\[39\]](#page-14-19) (10 models), and named entity recognition on the CoNLL 2003 dataset [\[41\]](#page-14-18) (11 models) [4](#page-8-1) . Our goal with this analysis is to validate whether, generally speaking, the more carbon-intensive models from our sample achieved better performance on common benchmarks compared to the models with less incurred emissions.

Figure [4](#page-9-1) shows the performance of the models in these four tasks and the associated carbon emissions; we also represent the theoretical Pareto front given the data, which corresponds to the set of Pareto-efficient solutions based on our data. We can think of the Pareto front of our metrics, the black line in the figures, as the curve connecting the models that achieved the best accuracy for a given amount of CO2eq emissions. In other words, all the data points under the Pareto lines correspond to models that obtained lower accuracy than other models in the sample despite producing the same or more carbon emissions.

Based on the comparison between carbon emissions and performance, we can observe that the only task in which better performance accuracy has systematically yielded more CO2 is image classification on ImageNet, seen on the top right subplot of Figure [4.](#page-9-1) Still, the relationship is far from being highly correlated (especially given that that the x-axis in on a logarithmic scale). For example, out of the 35 models analyzed, the top two models in terms of performance are also the most carbon-emitting. However, the third most carbon-intensive model is on the lower end of the performance (achieving 76 % accuracy), and we also see low-emitting models on the higher end of performance.

For other tasks, the trend is even less clear – for instance, for the 30 models evaluated on the WMT translation task (top left plot of Figure [4\)](#page-9-1), there is no clear link between CO2 emissions and BLEU score, for neither English-French or English-German – although the WMT English-French task seems to incur more carbon emissions than the English-German one, which can be explained in part by the fact that the WMT English-French data set is almost 4 times larger than the English-German one, which can require a longer training time and thus a higher energy consumption. For the final two NLP tasks, question answering and named entity recognition, we have less data points (10 for the former and 11 for the latter), and the connection between carbon emissions and accuracy is very unclear. For both tasks, many models from both the high and low ends of the range of CO2 emissions achieve comparable performance on the SQuAD dataset (bottom-left plot) as well as the CoNLL dataset (bottom-right plot).

<span id="page-8-1"></span><sup>4</sup>We also had data from a fifth task, object detection, which is represented in Table [1](#page-2-1) and Figure [3,](#page-7-1) but we did not have enough distinct data points to enable a meaningful comparison.

<span id="page-9-1"></span>![](_page_9_Figure_2.jpeg)

# <span id="page-9-0"></span>5 DISCUSSION AND FUTURE WORK

In the current section, we discuss the significance and the context of our analysis, its limitations, as well as promising directions for future work to improve the transparency of carbon emissions reporting in our field.

<span id="page-9-2"></span><sup>5</sup>We find a similar pattern between accuracy and energy consumption, which can be seen in Figure [5](#page-17-0) in the Supplementary Materials. Manuscript pending review

### 5.1 Discussion of Results

While the total carbon footprint of the field of ML is unclear due its distributed nature and the lack of systematic reporting of emissions in different settings, in the face of the climate crisis, it is important for the ML community to acquire a better understanding of its environmental footprint and how to reduce it [\[28,](#page-14-17) [38\]](#page-14-6). Our study is the first analysis of the carbon emissions of a multitude of ML models from different perspectives ranging from energy source to performance. While our sample is only a small portion of the entire Machine Learning field, the carbon emissions associated to the models in our data set is significant: the total carbon emissions of the models analyzed in our study is about 253 tons of CO2eq, which is equivalent to about 100 flights from London to San Francisco or from Nairobi to Beijing. While this may not seem like a large amount, the increase in emissions in recent years – from an average of 487 tons of CO2eq for models from 2015-2016 to an average of 2020 tons for models trained in 2020-2022 – as well as other trends that we observed in Section [4.3,](#page-7-0) indicate that the overall emissions due to ML model are rising.

In Section [4,](#page-4-0) we have discussed that the main sources of variance in the amount of emissions associated to training machine learning models is due to the carbon intensity of the primary energy source and the training time, with the power consumption of the hardware having a smaller influence. In terms of training time, the models in our sample range from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours, pointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170 days with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be over 3.5 million hours (14.8 days with 10,000 GPUs) [\[38\]](#page-14-6). Obviously, such long training times result in large amounts of carbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest training time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the lowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking, we can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more layers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters), whereas simpler and shallower models such as convolutional neural networks tend to be on the lower end of the emissions spectrum. Given that Transformer architectures are increasing in popularity – especially in NLP but also for several Computer Vision tasks – having a better idea of their energy consumption, carbon emissions, and the factors that influence them is also crucial part of analyzing the current and future state of our field.

An important observation from our analysis is that better performance is not generally achieved by using more energy. In other words, good performance can be achieved with limited carbon emissions because the progress in recent years has brought the possibility to train machine learning models efficiently. Image Classification is the task in our sample in which we observed the strongest correlation between performance and emissions. However, even in this task we also observed that small increments in carbon emissions lead to large increments in top-1 accuracy (see the left-hand-side of Figure [4\)](#page-9-1). This highlights the availability of efficient approaches and architectures.

### <span id="page-10-0"></span>5.2 Limitations

The analyses that we have carried out and the insights that they have provided us are useful towards a better understanding of the overall carbon emissions of ML model training. We are also aware of the limitations of our study: for one, we recognize that our sample is not fully representative of the field as a whole, given the diversity of models and architectures that exist and the speed at which our field is evolving. As we discussed in Section [3,](#page-2-0) despite our best efforts and several reminders, only 15% of authors from our initial sample of 500 were willing to share relevant

information with us. We also recognize that there are several factors that we are missing in order to be more precise in our estimation the carbon footprint of ML models: for instance, we do not have the necessary information regarding the Power Usage Effectiveness (PUE) of the data centers used for model training (i.e. the overhead used for heating, cooling, Internet etc.), as well as the real-time energy consumption of the hardware used for training. We also do not account for carbon offsets and power purchase agreements, which intend to bring computing centers closer to carbon neutrality and which are often taken into account by providers of cloud compute in their carbon accounting [\[18\]](#page-13-16). Despite this, the apples-to-apples carbon analysis that we carried out in the current study provides useful insights about the current state of carbon emissions in our field, as well as how this has evolved over time in the last 9 years.

Furthermore, while this study and much of the related work in this field has focused on estimating the carbon emissions of model training, there are many pieces of other overall carbon footprint of our field which are still missing: for instance, the carbon emissions of tasks such as data processing, data transfer, and data storage [\[28\]](#page-14-17), as well as the carbon footprint of manufacturing and maintaining the hardware used for training ML models [\[19\]](#page-13-11), We are also lacking information regarding the carbon impact of model development and inference – given that a model that is trained a single time can be deployed on-demand for millions of queries, this can ultimately add up to more emissions than those produced by the initial model training [\[31\]](#page-14-4). These are all directions for future research, which we discuss in more detail below.

### <span id="page-11-0"></span>5.3 Future Work

There is much interesting and exciting work to be done that would help us better understand the carbon emissions and broader environmental implications of ML. This includes:

Additional empirical studies. There is still a lot of uncertainty around, for instance, the relative contribution of added parameters of ML to their energy consumption and carbon footprint, as well as the proportion of energy used for pre-training versus fine-tuning ML models for different tasks and architectures. Furthering this research can benefit the field both from the perspective of sustainability and overall efficiency.

Widening the scope of ML life-cycle emissions. The overwhelming majority of work in carbon accounting for ML models has been limited to model training. However, both the upstream emissions (i.e. those incurred by manufacturing and transporting the required computing equipment) as well as the downstream ones (i.e. the emissions of model deployment) warrant further exploration and better understanding.

Increased standardization and transparency in carbon emissions reporting. As stated in Section [5.2,](#page-10-0) we put in significant efforts in contacting authors and gathering data to carry out our study, and were still lacking much of the necessary information that we would have liked to have. While certain conferences such as NeurIPS are starting to include compute information in submissions in submission checklists, there is still a lot of variability in carbon reporting, and figures can vary widely depending on what factors are included. Having a more standardized approach, such as ISO standards, to reporting the carbon emissions of ML can help better understand their evolution.

Considering the trade-off between sustainability and fairness. The environmental impacts of ML also come with consequences in terms of fairness, given the interplay between fairness and sustainability, most recently discussed by Hessenthaler et al. [\[21\]](#page-13-22). This includes, for instance, the consideration of the environmental impacts of ML approaches when benchmarking models [\[58\]](#page-15-3), but also, conversely, considering the impact on robustness and bias of model distillation techniques that improve model efficiency [\[23,](#page-13-23) [55\]](#page-15-5). Generally speaking, given that many advances in ML from last Manuscript pending review

years can be attributed to training increasingly deep and computationally expensive models, especially in fields such as natural language processing, it is important to be cognizant of the broader societal impacts of these models, be it from the perspective of their energy consumption [\[8,](#page-13-6) [15\]](#page-13-24), the attribution of computing resources [\[2,](#page-13-19) [3\]](#page-13-20) or the influence of corporate interests on research directions [\[1,](#page-13-25) [9\]](#page-13-21).

While discussions regarding the carbon footprint of our daily lives has started to become more common in many communities, alongside increased awareness of how our lifestyle choices (such as the way we travel and the food we eat) contribute to carbon emissions, we are lacking much of the necessary information necessary to regarding the impacts of the models we train. We hope that our work encourages better practices and more transparency in reporting the computational needs of the models and details of the energy used, and that our study will be a meaningful contribution towards a better understanding of our impact as ML researchers and practitioners.

### REFERENCES

- <span id="page-13-25"></span>[1] Mohamed Abdalla and Moustafa Abdalla. 2021. The Grey Hoodie Project: Big tobacco, big tech, and the threat on academic integrity. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 287–297.
- <span id="page-13-19"></span>[2] Orevaoghene Ahia, Julia Kreutzer, and Sara Hooker. 2021. The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation. arXiv preprint arXiv:2110.03036 (2021).
- <span id="page-13-20"></span>[3] Nur Ahmed and Muntasir Wahed. 2020. The de-democratization of AI: Deep learning and the compute divide in Artificial Intelligence research. arXiv preprint arXiv:2010.15581 (2020).
- <span id="page-13-15"></span>[4] Amazon Web Services. 2021. Delivering Progress Every Day : Amazon's 2021 Sustainability Report. [https://sustainability.aboutamazon.com/2021](https://sustainability.aboutamazon.com/2021-sustainability-report.pdf) [sustainability-report.pdf](https://sustainability.aboutamazon.com/2021-sustainability-report.pdf)
- <span id="page-13-2"></span>[5] Lasse F. Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. 2020. Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models. arXiv[:2007.03051](https://arxiv.org/abs/2007.03051) [cs.CY]
- <span id="page-13-17"></span>[6] Nicholas Apergis, James E Payne, Kojo Menyah, and Yemane Wolde-Rufael. 2010. On the causal dynamics between emissions, nuclear energy, renewable energy, and economic growth. Ecological Economics 69, 11 (2010), 2255–2260.
- <span id="page-13-5"></span>[7] Nesrine Bannour, Sahar Ghannay, Aurélie Névéol, and Anne-Laure Ligozat. 2021. Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools. In EMNLP, Workshop SustaiNLP.
- <span id="page-13-6"></span>[8] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? . In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 610–623.
- <span id="page-13-21"></span>[9] Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2021. The Values Encoded in Machine Learning Research. arXiv[:2106.15590](https://arxiv.org/abs/2106.15590) [cs.LG]
- <span id="page-13-13"></span>[10] Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation. 12–58.
- <span id="page-13-7"></span>[11] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).
- <span id="page-13-8"></span>[12] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).
- <span id="page-13-10"></span>[13] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. 2019. Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, 2 (2019), 292–308.
- <span id="page-13-12"></span>[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248–255.
- <span id="page-13-24"></span>[15] Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A Smith, Nicole DeCario, and Will Buchanan. 2022. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1877–1894.
- <span id="page-13-18"></span><span id="page-13-14"></span>[16] Facebook. 2020. Facebook 2020 Sustainability Report. [https://sustainability.fb.com/wp-content/uploads/2021/06/2020\\_FB\\_Sustainability-Report.pdf](https://sustainability.fb.com/wp-content/uploads/2021/06/2020_FB_Sustainability-Report.pdf)
- [17] Johannes Friedrich, Mengpin Ge, and Andrew Pickens. 2020. This interactive chart shows changes in the world's top 10 emitters. (2020).
- <span id="page-13-16"></span>[18] Google. 2022. Carbon free energy for Google Cloud regions.<https://cloud.google.com/sustainability/region-carbon>
- <span id="page-13-11"></span>[19] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon: The Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 854–867.
- <span id="page-13-3"></span>[20] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. Towards the systematic reporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research 21, 248 (2020), 1–43.
- <span id="page-13-22"></span>[21] Marius Hessenthaler, Emma Strubell, Dirk Hovy, and Anne Lauscher. 2022. Bridging Fairness and Environmental Sustainability in Natural Language Processing. arXiv preprint arXiv:2211.04256 (2022).
- <span id="page-13-9"></span>[22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-Optimal Large Language Models. <https://doi.org/10.48550/ARXIV.2203.15556>
- <span id="page-13-23"></span>[23] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. arXiv preprint arXiv:2010.03058 (2020).
- <span id="page-13-0"></span>[24] IEA. 2019. Global Energy & CO2 Status Report 2019. IEA (International Energy Agency): Paris, France (2019). [https://www.iea.org/reports/global](https://www.iea.org/reports/global-energy-co2-status-report-2019)[energy-co2-status-report-2019](https://www.iea.org/reports/global-energy-co2-status-report-2019)
- <span id="page-13-1"></span>[25] International Telecommunication Union. 2020. Greenhouse gas emissions trajectories for the information and communication technology sector compatible with the UNFCCC Paris agreement: L. 1470.<http://handle.itu.int/11.1002/1000/14084>
- <span id="page-13-4"></span>[26] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700 (2019).

- <span id="page-14-8"></span>[27] Loïc Lannelongue, Jason Grealey, and Michael Inouye. 2021. Green algorithms: Quantifying the carbon footprint of computation. Advanced Science (2021), 2100707.
- <span id="page-14-17"></span>[28] Anne-Laure Ligozat, Julien Lefèvre, Aurélie Bugeau, and Jacques Combaz. 2021. Unraveling the hidden environmental impacts of AI solutions for environment. arXiv preprint arXiv:2110.11822 (2021).
- <span id="page-14-20"></span>[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In European conference on computer vision. Springer, 740–755.
- <span id="page-14-9"></span>[30] Kadan Lottick, Silvia Susai, Sorelle A Friedler, and Jonathan P Wilson. 2019. Energy Usage Reports: Environmental awareness as part of algorithmic accountability. arXiv preprint arXiv:1911.08354 (2019).
- <span id="page-14-4"></span>[31] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. arXiv preprint arXiv:2211.02001 (2022).
- <span id="page-14-1"></span>[32] Jens Malmodin and Dag Lundén. 2018. The energy and carbon footprint of the global ICT and E&M sectors 2010–2015. Sustainability 10, 9 (2018), 3027.
- <span id="page-14-24"></span>[33] Aaditya Mattoo and Arvind Subramanian. 2012. Equity in climate change: an analytical review. World Development 40, 6 (2012), 1083–1097.
- <span id="page-14-25"></span>[34] Jennifer Morgan and David Waskow. 2014. A new look at climate equity in the UNFCCC. Climate Policy 14, 1 (2014), 17–22.
- <span id="page-14-7"></span>[35] Rakshit Naidu, Harshita Diddee, Ajinkya Mulay, Aleti Vardhan, Krithika Ramesh, and Ahmed Zamzam. 2021. Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning. arXiv preprint arXiv:2107.06946 (2021).
- <span id="page-14-2"></span>[36] Copenhagen Centre on Energy Efficiency. 2020. Greenhouse gas emissions in the ICT sector: Trends and methodologies [Internet]. [https:](https://c2e2.unepdtu.org/wp-content/uploads/sites/3/2020/03/greenhouse-gas-emissions-in-the-ict-sector.pdf) [//c2e2.unepdtu.org/wp-content/uploads/sites/3/2020/03/greenhouse-gas-emissions-in-the-ict-sector.pdf](https://c2e2.unepdtu.org/wp-content/uploads/sites/3/2020/03/greenhouse-gas-emissions-in-the-ict-sector.pdf)
- <span id="page-14-5"></span>[37] David Patterson, Joseph Gonzalez, Urs Hölzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. arXiv preprint arXiv:2204.05149 (2022).
- <span id="page-14-6"></span>[38] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).
- <span id="page-14-19"></span>[39] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016).
- <span id="page-14-0"></span>[40] Henning Rodhe. 1990. A comparison of the contribution of various gases to the greenhouse effect. Science 248, 4960 (1990), 1217–1219.
- <span id="page-14-18"></span>[41] Erik F Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. arXiv preprint cs/0306050 (2003).
- <span id="page-14-15"></span>[42] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).
- <span id="page-14-14"></span>[43] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2021. Multitask Prompted Training Enables Zero-Shot Task Generalization. <https://doi.org/10.48550/ARXIV.2110.08207>
- <span id="page-14-21"></span>[44] Steffen Schlömer, Thomas Bruckner, Lew Fulton, Edgar Hertwich, Alan McKinnon, Daniel Perczyk, Joyashree Roy, Roberto Schaeffer, Ralph Sims, Pete Smith, et al. 2014. Annex III: Technology-specific cost and performance parameters. In Climate Change 2014: Mitigation of Climate Change: Contribution of Working Group III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change. Cambridge University Press, 1329–1356.
- <span id="page-14-10"></span>[45] Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson, Sorelle Friedler, and Sasha Luccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing.
- <span id="page-14-13"></span>[46] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (2020), 54–63.
- <span id="page-14-16"></span>[47] Matthew Skiles, Euijin Yang, Orad Reshef, Diego Robalino Muñoz, Diana Cintron, Mary Laura Lind, Alexander Rush, Patricia Perez Calleja, Robert Nerenberg, Andrea Armani, Kasey M. Faust, and Manish Kumar. 2021. Conference demographics and footprint changed by virtual platforms. Nature Sustainability 2398-9629 (2021).
- <span id="page-14-3"></span>[48] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).
- <span id="page-14-23"></span>[49] Siddharth Suman. 2018. Hybrid nuclear-renewable energy systems: A review. Journal of Cleaner Production 181 (2018), 166–177.
- <span id="page-14-12"></span>[50] Neil C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F Manso. 2020. The computational limits of deep learning. arXiv preprint arXiv:2007.05558 (2020).
- <span id="page-14-11"></span>[51] Tristan Trébaol. 2020. CUMULATOR — a tool to quantify and report the carbon footprint of machine learning computations and communication in academia and healthcare. Technical Report.
- <span id="page-14-26"></span><span id="page-14-22"></span>[52] United States Energy Information Administration. 2012-2021. Detailed State Data.<https://www.eia.gov/electricity/data/state/>
- [53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998–6008.

- <span id="page-15-1"></span>[54] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. 2021. Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv preprint arXiv:2111.00364 (2021).
- <span id="page-15-5"></span>[55] Guangxuan Xu and Qingyuan Hu. 2022. Can model compression improve NLP fairness. arXiv preprint arXiv:2201.08542 (2022).
- <span id="page-15-0"></span>[56] Mirza Yusuf, Praatibh Surana, Gauri Gupta, and Krithika Ramesh. 2021. Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. arXiv preprint arXiv:2109.12584 (2021).
- <span id="page-15-2"></span>[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models.<https://doi.org/10.48550/ARXIV.2205.01068>
- <span id="page-15-3"></span>[58] Xiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and William Yang Wang. 2020. Hulk: An energy efficiency benchmark platform for responsible natural language processing. arXiv preprint arXiv:2002.05829 (2020).
- <span id="page-15-4"></span>[59] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 (2016).

### A SUPPLEMENTARY MATERIALS

# <span id="page-16-0"></span>A.1 Emails sent to authors

**Subject:** Information Request: Computing Infrastructure Used in your Paper Hello,

My name is XXXX and I am a researcher working on the environmental impact of Machine Learning. I am trying to gather data regarding the carbon footprint of recent state-of-the-art research papers. This will help the ML community get a better idea of how much CO2 we are emitting when training models.

In order to help me on my mission, I was hoping you could give me more information about your paper entitled YYYY.

More specifically, could you tell me:

- Where it was trained? If it was on a local computing cluster, could you tell me the location of the cluster? And if it was trained on the cloud, could you indicate the provider and server region (e.g. "Microsoft Azure, us-east1")?
- What hardware you used
- The total training time of your models?

Thank you very much for this information, XXXX

# <span id="page-16-1"></span>A.2 Information regarding training hardware

<span id="page-16-2"></span>Table 3. The top 5 GPUs/TPUs used, the number of models that used them for training, the range of quantities that were used, and their Thermal Design Power (TDP).

| Model       | Number of models | TDP   | Quantity used |
|-------------|------------------|-------|---------------|
| Tesla V100  | 30               | 300 W | 1-128         |
| TPU v3      | 9                | 450 W | 1-1024        |
| RTX 2080 Ti | 8                | 250 W | 4-16          |
| Tesla M40   | 5                | 250 W | 8             |
| GTX 1080    | 4                | 180 W | 1-8           |

In Table [3,](#page-16-2) we represent the 5 most popular GPU and TPU models used in the papers we analysed, accompanied by the number of papers that used them, the range of quantities used, and their TDP. The Tesla V100 was by far the most popular piece of hardware, representing almost a third of the papers, followed by the TPU v3. The TDP of the hardware used in our paper sample also varies significantly, from 180W for models such as the GTX 1080 to 450W for the TPU v3 model, meaning that TPUs, on average, consume more energy during usage.Looking at the number of GPUs and TPUs used for ML training in the papers that we surveyed, we can see that there is a large range in the quantity of GPUs/TPUs used for model training, with some models leveraging up to 1024 TPU v3s for training, while others utilize a single GTX 1080 GPU for varying amounts of time, which makes the total energy consumption vary significantly. We analyze the connection between energy usage and performance on different ML tasks in § [4.4,](#page-8-0) in order to determine whether higher energy consumption helps achieve better performance in different ML tasks.

### A.3 Energy Consumption by Task

In Figure [5](#page-17-0) below, we plot the same four tasks as in Figure [3,](#page-7-1) representing the energy consumed instead of the CO2 emitted. We find largely similar trends as the ones we describe in Section [4.4,](#page-8-0) with better performance on tasks like machine translation and image classification not necessarily being contingent on higher energy consumption.

<span id="page-17-0"></span>![](_page_17_Figure_4.jpeg)

### A.4 Carbon intensity over time

In Figure [6,](#page-18-0) we plot the evolution over the years of the carbon intensity of the energy grid for each model, as well as the number of models trained with each energy source. We observe that, despite the need to address the climate crisis by using cleaner energy sources, there has not been a decrease in neither the average carbon intensity nor the number of models trained with cleaner energy. On the contrary, we do observe a stark increase of models trained with coal.

<span id="page-18-0"></span>![](_page_18_Figure_4.jpeg)

![](_page_18_Figure_6.jpeg)
