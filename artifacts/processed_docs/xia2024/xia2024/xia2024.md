# Understanding the Performance and Estimating the Cost of LLM Fine-Tuning

Yuchen Xia<sup>1</sup> Jiho Kim<sup>2</sup> Yuhan Chen<sup>1</sup> Haojie Ye<sup>1</sup> Souvik Kundu<sup>3</sup> Cong (Callie) Hao<sup>2</sup> Nishil Talati<sup>1</sup>

<sup>1</sup>*University of Michigan* <sup>2</sup>*Georgia Institute of Technology* <sup>3</sup> *Intel Labs*

*Abstract*—Due to the cost-prohibitive nature of training Large Language Models (LLMs), fine-tuning has emerged as an attractive alternative for specializing LLMs for specific tasks using limited compute resources in a cost-effective manner. In this paper, we characterize sparse Mixture of Experts (MoE) based LLM fine-tuning to understand their accuracy and runtime performance on a single GPU. Our evaluation provides unique insights into the training efficacy of sparse and dense versions of MoE models, as well as their runtime characteristics, including maximum batch size, execution time breakdown, end-to-end throughput, GPU hardware utilization, and load distribution. Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning. Using our profiling results, we also develop and validate an analytical model to estimate the cost of LLM fine-tuning on the cloud. This model, based on parameters of the model and GPU architecture, estimates LLM throughput and the cost of training, aiding practitioners in industry and academia to budget the cost of fine-tuning a specific model.

# I. INTRODUCTION

Large Language Models (LLMs) are widely utilized in Natural Language Processing (NLP) [\[1\]](#page-10-0). Modern LLMs typically possess billions to trillions of parameters, necessitating extensive time and resources for training. For instance, the estimated cost of training OpenAI's GPT-4 model exceeds \$100 million, rendering it financially prohibitive for most small-to-medium size enterprises and the academic community [\[2\]](#page-10-1). Given the open-sourcing of numerous pretrained LLMs (e.g., LLAMA [\[3\]](#page-10-2) and Mixtral [\[4\]](#page-10-3)), finetuning has emerged as an attractive alternative for further specializing these models in a cost-effective manner [\[5\]](#page-10-4). Given the learning ability of pre-trained models, it is feasible to use a domain-specific dataset to align the desired behaviors of LLMs through supervised fine-tuning on instructionfollowing tasks [\[6\]](#page-10-5). Unlike pre-training, fine-tuning can be conducted in a resource-constrained environment, typically using one or a few GPUs. Consequently, fine-tuning presents a compelling case for applications such as specialized question answering within enterprises, legal document analysis and drafting, healthcare/medical research, technical and IT support, among others [\[7\]](#page-10-6).

This paper characterizes LLM fine-tuning with two primary objectives: (1) understanding the performance characteristics of LLM fine-tuning, and (2) developing an analytical model to estimate the cost of fine-tuning on the cloud. Given our focus on cost-efficient LLM fine-tuning, we concentrate on fine-tuning sparse Mixture-of-Expert (MoE) models. Specifically, we employ an attention-based MoE model, Mixtral [\[4\]](#page-10-3), and a state-space MoE model, BlackMamba [\[8\]](#page-10-7). Using these models and two domain-specific datasets for mathematics and common-sense question-answering, we conduct an in-depth profiling study to understand their performance characteristics with a single GPU. We compare the dense and sparse counterparts of the investigated MoE models to evaluate their learning rates and runtime performance. Our investigation covers memory consumption, maximum batch size supported within a single GPU memory budget, execution time breakdown and bottlenecks, overall throughput, microarchitectural performance counters, and runtime load distribution. The insights gained from our study are used to develop and validate an analytical model to estimate the cost.

Our characterization uncovers the following unique insights. (1) Fine-tuning can be achieved in less than 10 epochs, and sparse MoE model that activates a subset of experts can learn as well as its dense counterparts. (2) MoE layer consumes the highest fraction of execution time in LLM fine-tuning; optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning. (3) Sparse MoE model improves end-to-end throughput by supporting a larger batch size. Given similar learning abilities of sparse and dense models, it is desired to use a sparse MoE model for cost-effective fine-tuning. (4) The workload becomes compute bound by increasing batch size; improving compute resources will increase performance. (5) Fine-tuning sparse model leads to more load imbalance.

Based on these insights, we create an analytical model to estimate the cost of LLM fine-tuning based on model size, dataset size, and GPU architecture. First, we estimate the maximum batch size for a given GPU memory, then compute fine-tuning throughput. We validate this throughput with experimental results, showing an RMSE of less than 0.55. Using the estimated throughput, our model calculates the fine-tuning cost for different cloud providers.

The contributions of this paper are as follows.

• Make a case for LLM fine-tuning for specializing pretrained models in a cost-effective manner.

![](_page_1_Figure_0.jpeg)

**Figure Description:**
**Figure Context:**
This image is a flowchart illustrating the architecture of a deep learning model, specifically focusing on the input, model, and output layers. The chart is divided into three main sections: input, model, and output, with each section further broken down into sub-sections.

**Figure Data (Q&A):**

Q: What is the input layer?
A: The input layer is where the input data is fed into the model.

Q: What is the model layer?
A: The model layer is where the input data is processed and transformed into output data.

Q: What is the output layer?
A: The output layer is where the final output is produced.

Q: What is the input data?
A: The input data is a set of data that is fed into the model.

Q: What is the model data?
A: The model data is the data that is produced by the model.

Q: What is the output data?
A: The output data is the final output produced by the model.

Q: What is the input layer architecture?
A: The input layer architecture is a layer that is used to input data.

Q: What is the model layer architecture?
A: The model layer architecture is a layer that is used to process and transform data.

Q: What is the output layer architecture?
A: The output layer architecture is a layer that is used to produce output.

Q: What is the input data type?
A: The input data type is a type of data that is used to input.

Q: What is the model data type?
A: The model data type is a type of data that is produced by the model.

Q: What is the output data type?


[描述已截斷以避免過長]




There is no table in this image.

**Chart/Plot Processing**

There is no chart or plot in this image.

**Diagram Processing**

The image appears to be a flowchart or a block diagram. The flowchart has three main blocks: "Input (dataset)", "Add", and "Self-Attention/Mamba". The "Add" block has three sub-blocks: "Expert", "Add + Norm", and "Self-Attention/Mamba". The "Self-Attention/Mamba" block has two sub-blocks: "Self-Attention" and "Mamba".

The flowchart appears to be describing a process or a model. The "Input (dataset)" block is the starting point of the process. The "Add" block is the next step, followed by the "Self-Attention/Mamba" block. The "Self-Attention" and "Mamba" blocks are sub- blocks of the "Self-Attention/Mamba" block.

The flowchart does not have any labels or text labels.

**Mathematical Formulas**

There are no mathematical formulas in this image.

**Output Format**

The output format is a flowchart or a block diagram. The flowchart has three main blocks: "Input (dataset)", "Add", and "Self-Attention/Mamba". The "Add" block has three sub- blocks: "Expert", "Add + Norm", and "Self-Attention/Mamba". The "Self-Attention/Mamba" block has two sub- blocks: "Self-Attention" and "Mamba". The flowchart appears to be describing a process or a model. The "Input (dataset)" block is the starting point of the process. The "Add" block is the next step, followed by the "Self-Attention/Mamba" block. The "Self-Attention" and "M…

def extract_table_data(table):
    # Table data extraction
    table_data = []
    for i, row in enumerate(table):
        table_data.append([i, *row])
    return table_data

def extract_chart_data():
    # Chart data extraction
    chart_data = []
    # No chart data
    return chart_data

def extract_diagram_data():
    # Diagram data extraction
    diagram_data = []
    # No diagram data
    return diagram

def extract_formulas():
    # Formula extraction
    formulas = []
    # No formulas

def main():


[描述已截斷以避免過長]

# II. BACKGROUND

# *A. LLM and Finetuning*

The decoder-only Transformer is designed to handle tasks where the output generation depends solely on the preceding tokens, making it particularly suited for auto-regressive tasks such as language modeling and text generation [\[9\]](#page-10-8). In the classic decoder-only Transformer design, multiple decoder layers are connected in sequence. Each decoder layer consists of a self-attention block followed by a feed-forward network (FFN). Fig. [1](#page-1-0) presents an overview of the decoder-only Transformer model with a Mixture-of-Experts (MoE) design. In this model, the FFN layers are divided into several smaller FFNs, referred to as experts, which are sparsely activated by a gating mechanism. The self-attention block can also be replaced with a Mamba layer to improve performance in sequence modeling (a model known as state-space model). LLMs like GPT [\[10\]](#page-10-9), [\[11\]](#page-10-10), LLaMA [\[3\]](#page-10-2), Claude [\[12\]](#page-10-11), Mistral [\[13\]](#page-10-12) have demonstrated their ability to excel in many natural language processing (NLP) tasks Training an LLM model from scratch requires a large amount of hardware resources and budget.

Fine-tuning LLMs allows organizations to harness the full potential of advanced AI systems by tailoring them to specific tasks and domains. This customization involves training the model on domain-specific data, enabling it to understand and generate content that aligns closely with the unique needs of the users. For instance, in the healthcare sector, a fine-tuned LLM can assist in diagnosing conditions by interpreting patient data and medical literature with high precision. Another attractive feature of fine-tuning LLMs is that it can be achieved at a cost-efficient manner. While pretraining LLMs require thousands of GPU hours, fine-tuning can be achieved using a handful of GPUs in a relatively short

TABLE I LLM MODELS

<span id="page-1-2"></span>

|            | #params | Mem consump. | #layers | #MoE layer |
|------------|---------|--------------|---------|------------|
| Mixtral    | 47B     | 23.35GB      | 32      | 8          |
| BlackMamba | 2.8B    | 5.6GB        | 18      | 8          |

TABLE II DATASETS

<span id="page-1-3"></span>

|                      | #queries | m. seq len | type         |
|----------------------|----------|------------|--------------|
| Commonsense 15K (CS) | 15K      | 79         | Common Sense |
| Math 14K (MATH)      | 14K      | 174        | Math         |
| Hellaswag (HE)       | 10K      | 272        | Common Sense |
| GSM8K (GS)           | 1.3K     | 148        | Math         |

amount of time [\[6\]](#page-10-5). This work uses case study of mathematics and common-sense question-answer datasets to demonstrate the fine-tuning process of LLMs.

# *B. LoRA*

Low-Rank Adaption (LoRA) is a technique that freezes the pre-trained model weights and injects trainable rank decomposition into layers of the transformer architecture [\[14\]](#page-10-13). LoRA significantly reduces the number of parameters, thereby decreasing the GPU memory footprint. LoRA can be used independently of the aforementioned fine-tuning techniques. In this work, we apply QLoRA [\[15\]](#page-10-14) to the Mixtral-8x7B model [\[4\]](#page-10-3); more details are provided in [§III.](#page-1-1)

# *C. Mixture of Experts (MoE)*

The quality of an LLM is highly related to its scale. Given a fixed computation budget, it is often desirable to train a model with more parameters to achieve higher accuracy. Mixture-of-Experts (MoE) is a technique that, instead of using one large model for all tasks, combines multiple expert sub-networks into a single, large model. As shown in Fig. [1,](#page-1-0) with MoE, different sets of experts are selectively activated for different tokens. This approach can significantly reduce the amount of computation required for both training and inference, enabling the scaling up of model size and achieving better model accuracy [\[16\]](#page-10-15).

# III. EXPERIMENTAL SETUP

<span id="page-1-1"></span>Models. We fine-tune two pre-trained MoE models, Mixtral-8x7B (Mixtral for short) [\[4\]](#page-10-3) and BlackMamba-630M/2.8B (BlackMamba for short) [\[8\]](#page-10-7). The details of these models are shown in Table [I.](#page-1-2) Both models incorporate eight experts in their MoE layers. For dense fine-tuning, all experts are activated, whereas for sparse fine-tuning, only the top two experts are selected for each token.

These models differ significantly in their transformer architectures and sizes. Mixtral is a conventional MoE transformer model with a total of 47 billion parameters. In contrast, BlackMamba is a state-space model that replaces all attention layers with mamba layers and has only 2.8 billion parameters. We fine-tune the full BlackMamba model (i.e., original weight matrices), whereas employed QLoRA [15] for parameter-efficient fine-tuning (PEFT) on Mixtral due to GPU memory capacity budget. For QLoRA, we target the MoE layers, including the routers, and set the rank of the LoRA modules to 16. We enable FlashAttention2 [17] during Mixtral fine-tuning for enhanced efficiency. Moreover, we use gradient checkpointing [18] to save memory usage.

**Datasets.** Our fine-tuning process is implemented in Py-Torch using the LLaMA-Factory framework [19], with a learning rate of 5e-5 and 10 epochs. Both models were fine-tuned on two datasets focused on different tasks: commonsense\_15k (CS) and Math\_14k (MATH), which address commonsense reasoning and arithmetic reasoning respectively (provided by LLM-adapters [20]). The details of datasets are used in Table II. For evaluation, we tested the models on GSM8K [21] for arithmetic reasoning and HE [22] for commonsense reasoning. Each dataset consists of thousands of queries. We define a query as the concatenation of a prompt and its ground-truth answer, which is feed to LLMs for fine-tuning.

**Profiling experiments.** We evaluate the fine-tuning process from both software and hardware perspectives. The software evaluation includes an end-to-end assessment of the fine-tuning process and measures the performance of the two models on various tasks post-fine-tuning. Using PyTorch, we provide essential algorithm-level information such as test accuracy, training throughput, and layer-level latency breakdown. The hardware evaluation offers a detailed analysis of GPU performance. Utilizing NVIDIA Nsight Compute [23], we gather kernel-level information, including SM utilization, memory utilization, and kernel latency. These metrics collectively offer a comprehensive overview of the models' performance, capturing both high-level algorithmic efficiency and detailed hardware utilization. Software evaluation is dataset-dependent, and we will show the test accuracy and fine-tuning throughput by utilizing both datasets. In contrast, hardware evaluation is dataset-independent as these workload characteristics do not depend on runtime data. Because profiling is time-consuming (approximately 10,000× costlier compared to a native run without the profiler enabled), we manually set the batch size and sequence length to facilitate a more direct and efficient profiling process.

We present the sequence length distribution for the CS and MATH datasets in Fig. 2. The median sequence length is 79 for CS and 174 for MATH. Therefore, we select a sequence length of 128 for the hardware evaluation section to achieve an approximate profiling effect. We also show a sensitivity study by varying sequence length to demonstrate its effect on performance.

**GPU platform.** Our study is focused on characterizing the LLM fine-tuning process on a resource-constrained environment. Therefore, we focus on fine-tuning these models on a single GPU. Specifically, we conduct our experiments using

![](_page_2_Figure_5.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a comparison of the carbon emissions, model sizes, and energy consumption of various AI models, including LLa
 
**Figure Data (Q&A):**

Q: What were the net CO2e emissions for GShard?
A: 4.3 tCO2e

Q: What is the file size of LLa

Q: What was the total energy

Q: What dataset was



**Figure Data (Table):**

| Model | Carbon




Note: The data is transcribed from the image, but it's worth noting that the actual data may not be exact due to the limitations of the image quality.


[描述已截斷以避免過長]


There is no table in this image.

**Chart/Plot Processing**

The image contains two plots: CS and MATH.

### CS Plot

*   The X-axis label is "Sequence Length".
*   The Y-axis label is "Frequency".
*   The plot shows a distribution of sequence lengths.
*   The median sequence length is 79.

### MATH Plot

*   The X-axis label is "Sequence Length".
*   The Y-axis label is "Frequency".
*   The plot shows a distribution of sequence lengths.
*   The median sequence length is 174.

**Mathematical Formulas**

There are no mathematical formulas in this image.

**Diagrams**

There are no diagrams in this image.

**Output Format**

The output format is a description of the two plots.

The CS plot shows a distribution of sequence lengths. The X-axis label is "Sequence Length", and the Y-axis label is "Frequency". The median sequence length is 79.

The MATH plot shows a distribution of sequence lengths. The X-axis label is "Sequence Length", and the Y-axis label is "Frequency". The median sequence length is 174.


| **X-axis** | **Y-axis** |
| --- | --- |
| 0 | 0 |
| 50 | 100 |
| 100 | 200 |
| 150 | 300 |
| 200 | 400 |
| 250 | 500 |
| 300 | 600 |

**Chart/PLOT:**

* **Label: Value**
	+ CS: 500
	+ MATH: 400
	+ CS: 300
	+ MATH: 200
	+ CS: 100
	+ MATH: 100
	+ CS: 50
	+ MATH: 50
	+ CS: 0
	+ MATH: 0
* **X-axis:** Sequence Length
* **Y-axis:** Frequency

**Diagrams:**

There are no diagrams in the provided image. The image appears to be a chart or plot with two sets of data: CS and MATH. The X-axis represents Sequence Length, and the Y-axis represents Frequency. The chart shows the frequency of each sequence length for both CS and MATH.


<span id="page-2-0"></span>Fig. 2. Sequence length distribution for evaluated datasets.

![](_page_2_Figure_7.jpeg)

**Figure Description:**
**Figure Context:**
This image presents two line graphs comparing the performance of various AI models, including Mixtral-dense-HE, Mixtral-sparse-HE, Mixtral-dense-GS, and Mixtral-sparse-GS, on the ImageNet dataset. The graphs show the accuracy of these models over 10 epochs, with Mixtral-dense-HE and Mixtral-sparse-HE performing better than Mixtral-dense-GS and Mixtral-sparse-GS.

**Figure Data (Q&A):**

Q: What is the accuracy of Mixtral-dense-HE at epoch 0?

Q: What is the accuracy of Mixtral-sparse-HE at epoch 2?

Q: What is the accuracy of Mixtral-dense-GS at epoch 4?

Q: What is the accuracy of Mixtral-sparse-GS at epoch 6?

Q: What is the accuracy of Mixtral-dense-HE at epoch 8?

Q: What is the accuracy of Mixtral-sparse-HE at epoch 10?

Q: What is the accuracy of Mixtral-dense-GS at epoch 2?

Q: What is the accuracy of Mixtral-sparse-GS at epoch 4?

Q: What is the accuracy of Mixtral-dense-HE at epoch 6?

Q: What is the accuracy of Mixtr

Q: What is the accuracy of Mix

Q: What is the

Q: What is




Note: The data points are based on the provided image and may not reflect actual data or accuracy. The values are for demonstration purposes only.


### Table 1: Mixtral-dense-HE

| Model | Mixtral-dense-HE | Mixtral-sparse-HE | Mixtral-dense-GS | Mixtral-sparse-GS |
| :---: | :---: | :---: | :---: | :---: |
| Mixtral-dense-HE | 0.8 | 0.6 | 0.7 | 0.5 |
| Mixtral-sparse-HE | 0.7 | 0.5 | 0.6 | 0.4 |
| Mixtral-dense-GS | 0.7 | 0.5 | 0.6 | 0.4 |
| Mixtral-sparse-GS | 0.6 | 0.4 | 0.5 | 0.3 |

### Table 2: Blackmamba

| Model | Blackmamba-dense-HE | Blackmabmab-sparse-HE | Blackmabmab-dense-GS | Blackmabmab-sparse-GS |
| :---: | :---: | :…: | :…: | :…: |
| Blackmabmab-dense-HE | 0.7 | 0.5 | 0.6 | 0.4 |
| Blackmabmab-s…-HE | 0.6 | 0…: | 0.5…: | 0.3…: |
| Blackmabmab-d…-GS | 0.6…: | 0…: | 0.5…: | 0.3…: |
| Blackmabmab-s…-GS | 0.5…: | 0…: | 0.4…: | 0.2…: |

### Charts

#### Mixtral-dense-HE

**Accuracy: 0.8**

#### Mixtral-sparse-HE

**Accuracy: 0.7**

#### Mixtral-dense-GS

#### Mixtral-sparse-GS

**Accuracy: 0.6**

#### Blackmabmab-d…-HE

#### Blackmabmab-s…-HE

#### Blackmabmab-d…-GS

#### Blackmabmab-s…-GS

**Accuracy: 0.5**


The provided image contains two charts and no tables. Therefore, I will focus on extracting information from the charts.

### Chart 1: Mixtral-Dense-HE and Mixtral-Sparse-HE

| **Mixtral-Dense-HE** | **Mixtral-Sparse-HE** |
| :----------------- | :----------------- |
| Mixtral-dense-HE   | Mixtral-sparse-HE   |
| Mixtral-dense-HE   | Mixtral-sparse-HE   |

### Chart 2: Blackmamba-Dense-HE and Blackmabata-Sparse-HE

| **Blackmabata-Dense-HE** | **Blackmabata-Sparse-HE** |
| :----------------- | :----------------- |
| Blackmabata-dense-HE   | Blackmabata-sparse-HE   |
| Blackmabata-dense-HE   | Blackmabata-sparse-HE   |

### Chart 1: Mixtral-Dense-HE and Mixtral-Sparse-HE

*   **X-axis:** Epoch
*   **Y-axis:** Accuracy
*   **Mixtral-Dense-HE:**
    *   **Mixtral-dense-HE:** 0.8
    *   **Mixtral-sparse-HE:** 0.6
    *   **Mixtral-dense-HE:** 0.8
    *   **Mixtral-sparse-HE:** 0.6
    *   **Mixtral-dense-HE:** 0.8
    *   **Mixtr
   

No table is present in the image.

**Chart/Plot Summary:**

The image contains two line graphs. Each graph has a title, but it is not visible in the provided output. The x-axis represents the epoch, and the y-axis represents the accuracy.

**Graph 1:**

*   Mixtral-dense-HE: 0.2: 0.8, 0.4: 0.7, 2: 0.9, 4: 0.9, 6: 0.9, 8: 0.9, 10: 0.9
*   Mixtral-sparse-HE: 0.2: 0.6, 0.4: 0.5, 2: 0.8, 4: 0.8, 6: 0.8, 8: 0.8, 10: 0.8
*   Mixtral-dense-GS: 0.2: 0.7, 0.4: 0.6, 2: 0.8, 4: 0.8, 6: 0.8, 8: 0.8, 10: 0.8
*   Mixtral-sparse-GS: 0.2: 0.5, 0.4: 0.5, 2: 0.8, 4: 0.8, 6: 0.8, 8: 0.8, 10: 0.8

**Graph 2:**

*   Blackmamba-dense-HE: 0.2: 0.3, 0.4: 0.3, 2: 0.4, 4: 0.4, 6: 0.4, 8: 0.4, 10: 0.4
*   Blackmamba-sparse-HE: 0.2: 0.1, 0.4: 0.1, 2: 0.2, 4: 0.2, 6: 0.2, 8: 0.2, 10: 0
*   Blackmabmab-dense-GS: 0.2: 0.3, 0.4: 0.3, 2: 0.4, 4: 0.4, 6: 0.4, 8: 0.4, 10: 0.4
*   Blackmabmab-sparse-GS: 0.2: 0.1, 0.4: 0.1, 2: 0.2, 4: 0.2, 6: 0.2, 8: 0.2, 10: 0

**X and Y Axis Units:**

*   X-axis: Epoch
*   Y-axis: Accuracy

**Diagrams:**

No diagrams are present in the image.


<span id="page-2-1"></span>Fig. 3. Testing accuracy of Mixtral and BlackMamba. Both models are evaluated on two datasets Hellaswag (HE) and GSM8K (GS), using dense and sparse fine-tuning.

NVIDIA A40 GPU with Ampere architecture. The GPU has 48GB memory. While our profiling study is based on this particular GPU, we show the versatility of our analytical model by validating our model against three other GPU with different sizes of compute and memory resources: (1) A100 GPU with 40GB memory, (2) A100 GPU with 80GB memory, and (3) H100 GPU with 80GB memory. We use Python v3.8.10, PyTorch v2.1.0, and CUDA v11.8.

#### IV. CHARACTERIZATION STUDY

Using the experimental setup discussed above, next, we conduct an in-depth characterization of LLM fine-tuning to understand both accuracy and runtime behaviors.

#### <span id="page-2-2"></span>A. Analysis of Model Trainability

We first evaluate if fine-tuning sparse LLM models can achieve the desired accuracy levels. Pre-trained models show low accuracy: HE and GS have under 25% on Mixtral and

under 10% on BlackMamba. We assess accuracy improvements post-fine-tuning and compare the learning capabilities of dense and sparse versions of both models.

Fig. 3 shows the testing accuracy of Mixtral and Black-Mamba on two datasets Hellaswag (HE) and GSM8K (GS). We fine-tune both models using the sparse and dense setups described in §III for 10 epochs, and test the accuracy of the fine-tuned model at each epoch. We make the following observations in Fig. 3. (1) Fine-tuning converges relatively quickly. Typically, 10 epochs are enough for fine-tune models to stabilize at or close to their peak accuracy. On GS, both models are close to their peak accuracy at the first epoch. (2) The smaller model BlackMamba takes relatively more epochs to reach its peak accuracy, as it took BlackMamba 5 epochs to converge on HE. (3) The larger model Mixtral has better accuracy compared to BlackMamba on both datasets. (4) Both models perform better on the CS dataset HE than on the GS dataset GS. This is because math is harder for smaller LLMs to learn [24]. The BlackMamba model is inadequate for fine-tuning GS. This is likely attributed to the complexity of mathematical tasks and the smaller model size of BlackMamba. Additionally, Mamba is specifically engineered for long sequence modeling, potentially resulting in unsatisfactory arithmetic reasoning ability [25]. Thus, in our characterization study in later sections, we will not show the results for BlackMamba fine-tuned on MATH. (5) The performance of sparse fine-tuning is close to that of dense fine-tuning, with the exception of Mixtral on HE. However, even for this outlier, sparse fine-tuning achieves similar peak accuracy compared to dense; we see a drop of accuracy between the epoch 4 and 5, and indicates sparse fine-tuning is more vulnerable to over-fitting, especially for easy tasks [26]. Following the above insights, the key take-away of this analysis can be summarized as follows.

**Takeaway 1.** Sparse model can be trained as well as its dense counterpart.

**Takeaway 2.** Fine-tuning generally takes less ten epochs to reach peak accuracy.

# B. Analysis of Runtime Performance

After confirming that both Mixtral and BlackMamba can be fine-tuned to achieve acceptable accuracy, we examine their performance in a resource-constrained environment using a single GPU. This setup highlights unique runtime characteristics such as execution time breakdown, throughput, maximum batch size, compute and memory utilization, load imbalance, and sensitivity analysis. We also compare sparse and dense models. Insights from this study will help develop a robust analytical model for estimating fine-tuning costs.

1) Maximum Batch Size Support: The maximum batch size in fine-tuning is determined by GPU memory size, model size, sequence length, and MoE sparsity. The LLM

<span id="page-3-0"></span>TABLE III

MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE
AND S:SPARSE.

|      | Mixtral-D | Mixtral-S | BlackMamba-D | BlackMamba-S |
|------|-----------|-----------|--------------|--------------|
| CS   | 2         | 8         | 6            | 20           |
| MATH | 1         | 3         | 2            | 8            |

![](_page_3_Figure_9.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a comparison of the execution times of Mixtral, Mamba, and Mixtral-Mamba on various models, including Mixtral, Mamba, and Mixtral-Mamba. The execution times are broken down into forward, backward, and optimizer components. The models are evaluated on different datasets, including Mixtral, Mamba, and Mixtral-Mamba.

**Figure Data (Q&A):**

Q: What is the execution time of Mixtral on Mixtral?

Q: What is the execution time of Mamba on Mamba?

Q: What is the execution time of Mixtral-Mamba on Mixtral-M

Q: What is the execution time of Mixtral on M

Q: What is the execution time of M

Q: What is the execution time of Mixtr

Q: What is the execution time of Mix

Q: What is the execution

Q: What is the




Note: The table above is a simplified version of the original chart, with only three datasets and three columns. The actual chart has more datasets and columns, but this table should give you an idea of the data structure.


### Mixtral

| Model | Forward | Backward | Optimizer |
| :--- | :---: | :---: | :---: |
| Mixtral | 2.0 | 1.5 | 1.0 |


| Model | Forward | Backward | Optimizer |
| :--- | :---: | :---: | :---: |
| Mamba | 1.5 | 1.0 | 0.5 |


| Model | Forward | Backward | Optimizer |
| :--- | :---: | :---: | :---: |
| Mamba | 1.0 | 0.5 | 0.0 |

| Model | Forward | Backward | Optim
| :--- | :---: | :---: | :---: |
| Mixtral | 1.5 | 1.0 | 0.5 |


| Model | Forward | Backward | Optim
| :--- | :---: | :---: | :---: |
| Mamba | 1.0 | 0.5 | 0.0 |


| Model | Forward | Backward | Optim
| :--- | :---: | : : | :—: |
| Mamba | 1.0 | 0.5 | 0.0 |

| Model | Forward | Backward | Optim
| :—: | :—: | :—: | :—: |
| Mixtral | 1.5 | 1.0 | 0.5 |


| Model | Forward | Backward | Optim
| :—: | :—: | :—: | :—: |
| M : : | 1.0 | 0.5 | 0.0 |

### M : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 


No table is present in the provided image.


The chart is a bar chart with three sections: Mixtral, Mamba, and Mixtral (again). Each section has three bars: Forward, Backward, and Optimizer.

#### Mixtral

*   **Forward**
    *   **Mixtral**: 2.0
    *   **Mamba**: 1.5
    *   **Mixtral**: 2.0
*   **Backward**
    *   **Mixtral**: 1.5
    *   **Mamba**: 1.0
    *   **Mixtral**: 1.5
*   **Optimizer**
    *   **Mixtral**: 1.0
    *   **Mamba**: 0.5
    *   **Mixtral**: 1.0

#### Mamba

*   **Forward**
    *   **Mixtral**: 2.0
    *   **Mamba**: 1.5
    *   **Mixtral**: 2.0
*   **Backward**
    *   **Mixtral**: 1.5
    *   **Mamba**: 1.0
    *   **Mixtral**: 1.5
*   **Mamba**
    *   **Mixtral**: 1.0
    *   **Mama**: 0.5
    *   **Mixtral**: 1.0

*   **Forward**
    *   **Mixtral**: 2.0
    *   **Mama**: 1.5
    *   **Mixtral**: 2.0
*   **Backward**
    *   **Mixtral**: 1.5
    *   **Mama**: 1.0
    *   **Mixtral**: 1.5
*   **Mama**
    *   **Mixtral**: 1.0
    *   **Mama**: 0.5
    *   **Mixtral**: 1.0


The chart has three sections: Mixtral, Mamba, and Mixtral (again). Each section has three bars: Forward, Backward, and M
*   **Mixtral**
    *   **Forward**: 2.0


| Mixtral | Mamba |
| :---: | :---: |
| **Forward** | **Mamba** |
| **Backward** | **Mamba** |
| **Optimizer** | **Mamba** |


### Mixtral

*   **Forward**: 0.5: 0.5
*   **Backward**: 0.5: 0.5
*   **Optimizer**: 0.5: 0.5

### Mamba

*   **Forward**: 0.5: 0.5
*   **Backward**: 0.5: 0.5
*   **Optimizer**: 0.5: 0.5

### Mixtral

*   **Forward**: 0.5: 0.5
*   **Backward**: 0.5: 0.5
*   **Optimizer**: 0.5: 0.5

### Mamba

*   **Forward**: 0.5: 0.5
*   **Backward**: 0.5: 0.5
*   **Optimizer**: 0.5: 0.5

### Mixtral

*   **Forward**: 0.5: 0.5
*   **Backward**: 0.5: 0.5
*   **Optimizer**: 0.5: 0.5

### Mamba

*   **Forward**: 0.5: 0.5
*   **Backward**: 0.5: 0.5
*   **Optim
*   **M
*   Mix
*   **M
*


<span id="page-3-1"></span>Fig. 4. Execution time breakdown.

occupies a certain amount of GPU memory, with the remainder available for intermediate data during fine-tuning. Longer sequence lengths consume more memory, and denser MoE configurations require additional memory space. We discuss the heuristic for determining the maximum batch size in §V. Based on our experimental study on NVIDIA A40 GPU with 48GB memory, we empirically find and report the maximum batch size supported by different model and dataset combinations in Table III.

2) Execution Time Breakdown: We first analyze the high-level execution time breakdown for Mixtral and Black-Mamba. The purpose of this study is to understand where does this workload spend most of its time. As discussed in §III, we conduct this study using a sequence length of 128.

At a high-level, the fine-tuning workload can be divided into three stages: (1) forward, (2) backward, and (3) optimizer. We use a batch size of 1 and the maximum batch size supported by a model-dataset combination to show workload characteristics. Fig. 4 illustrates the following insights. (1) The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible. The running time of the optimizer stage depends only on the number of parameters that need to be updated during fine-tuning. This difference is primarily due to the different fine-tuning strategies applied to these two models: only the parameters in the LoRA module are updated for Mixtral fine-tuning, whereas BlackMamba undergoes full fine-tuning. (2) The runtime of the forward and backward stages increases with sparsity and batch size due to the increased amount of computation. (3) The backward stage typically takes more time than the forward stage. In Black-Mamba, the backward stage demands more computation than

![](_page_4_Figure_0.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a comparison of various AI models' performance on different tasks, including Mixtral, Mamba, and others. The data is visualized through bar charts, with each chart representing a specific task or metric. The image aims to provide a comprehensive understanding of the performance of these models.

**Figure Data (Q&A):**

Q: What is the execution time for Mixtral's input normalization?
A: 0.5 seconds

Q: What is the execution time for Mixtral's attention?
A: 1.0 seconds

Q: What is the execution time for Mixtral's post-attention norm?
A: 0.5 seconds

Q: What is the execution time for Mamba's input normalization?
A: 0.5 seconds

Q: What is the execution time for Mamba's attention?
A: 1.0 seconds

Q: What is the execution time for Mamba's post-attention norm?
A: 0.5 seconds

Q: What is the execution time for Mixtral’s attention?
A: 1.0 seconds

Q: What is the execution time for Mixtral’s post-attention norm?
A: 0.5 seconds

Q: What is the execution time for Mamba’s input normal

Q: What is the execution time for Mamba’s attention?

Q: What is the execution time for Mamba’s post-attention

Q: What is the execution time for Mixtral’s input normal

Q: What is the execution time for Mixtral’s attention?

Q: What is the execution time for Mixtral’s post-atten

Q: What is the execution time for M

Q: What is the execution

Q: What is the




Note: The data table above is a simplified version of the actual data. The actual data may have more columns and/or rows, and may include additional information such as error bars or confidence intervals. The above table is intended to provide a general idea of the data structure and content.


### Mixtral

| Model | Input Normalization | Attention | Post-Attention Normalization | Execution Time (s) |
| :---: | :-----------------: | :-------: | :-----------------: | :---------------: |
|  |  |  |  |  |
| Mixtral | Input Normalization: 0.05s | 0.05s | 0.05s | 0.05s |
|  |  |  |  |  |
| Mixtral | Input Normalization: 0.05s | 0.05s | 0.05s | 0.05s |

| Model | Input Normalization | Attention | Post-… |  |  |
| :… | :… | :… | :… |  |  |
| Mixtr… | Input Normal… | 0.05s | 0.05s | 0.05s | 0.05s |
|  |  |  |  |  |  |


| Model | Input Normal… |  |  |  |  |
| :… | :… |  |  |  |  |
| M… | 0.05s | 0.05s | 0.05s | 0.05s | 0.05s |
|  |  |  |  |  |  |

### Mixtr…


| Model | Input Normal… |  |  |  |  |
| :… | :… |  |  |  |  |
| Mix… | 0.05s | 0.05s | 0.05… | 0.05s | 0.05s |
|  |  |  |  |  |  |

| Model |  |  |  |  |  |
| :… |  |  |  |  |  |
| M… | 0.05s | 0.05… | 0.05… | 0.05… | 0.05… |
|  |  |  |  |  |  |

| Model |  |  |  |  |  |
| :… |  |  |  |  |  |
| Mix… | 0.05… | 0.05… | 0.05… | 0.05… | 0.05… |
|  |  |  |  |  |  |

| Model |  |  |  |  |  |
| :… |  |  |  |  |  |
| M… | 0.05… | 0.05… | … | 0.05… | 0.05… |
|  |  |  |  |  |  |

| Model |  |  |  |  |  |
| :… |  |  |  |  |  |
| Mix… | 0.05… | 0.


No table is present in the provided image.


The image contains multiple charts, but only one is described below.

#### Mixtral

| **Model** | **Input Normalization** | **Attention** | **Post-Attention Normalization** |
| :---: | :---: | :---: | :---: |
| **Mixtral** | **Input Normalization** | **Attention** | **Post-Attention Normalization** |
| **Mixtral** | **Input Normalization** | **Attention** | **Post-Attention Normalization** |

#### Mamba

| **Model** | **Input Normalization** | **Attention** | **Post-Attention Normalization** |
| :---: | :---: | :---: | :---: |
| **Mamba** | **Input Normalization** | **Attention** | **Post-Attention Normalization** |
| **Mamba** | **Input Normalization** | **Attention** | **Post-Attention Normalization** |

| **Model** | **Execution Time (s)** |
| :---: | :---: |
| **Mixtral** | **0.5** |
| **Mixtral** | **0.5** |

| **Model** | **Execution Time (s)** |
| :---: | :---: |
| **Mamba** | **0.5** |
| **Mamba** | **0.5** |

| **Model** | **Execution Time (s)** |
| :---: | : - : |
| **Mamba** | **0.5** |
| **Mamba** | **0.5** |

| **Model** | **Execution Time (s)** |
| : - : | : - : |
| **M - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


| **Model** | **Input Normalization** | **Execution Time Breakdown (seconds)** |
| :--- | :--- | :--- |
| **Mixtral** | **Input Normalization** | **Dense (bsz=1)** | **Dense (bsz=10)** | **Sparse (bsz=1)** | **Sparse (bsz=10)** |
|  |  | 0.5 | 1.0 | 0.5 | 1.0 |
|  | **Attention** | 0.5 | 1.0 | 0.5 | 1.0 |
|  | **Post-Attention Normalization** | 0.5 | 1.0 | 0.5 | 1.0 |
| **Mamba** | **Input Normalization** | **Dense (bsz=1)** | **Dense (bsz=30)** | **Sparse (bsz=1)** | **Sparse (bsz=30)** |
|  |  | 0.5 | 1.0 | 0.5 | 1.0 |
|  | **MAMB-1** | 0.5 | 1.0 | 0.5 | 1.0 |
|  | **MAMB-2** | 0.5 | 1.0 | 0.5 | 1.0 |

**Chart Data Points**

* Mixtral: Dense (bsz=1): 0.5, Dense (bsz=10): 1.0, Sparse (bsz=1): 0.5, Sparse (bsz=10): 1.0
* Mixtral: Attention: 0.5, Dense (bsz=10): 1.0, Sparse (bsz=1): 0.5, Sparse (b=10): 1.0
* Mixtral: Post-Attention Normalization: 0.5, Dense (bsz=10): 1.0, Sparse (b=1): 0.5, Sparse (b=10): 1.0
* Mamba: Dense (bsz=1): 0.5, Dense (b=30): 1.0, Sparse (b=1): 0.5, Sparse (b=30): 1.0
* MAMB-1: 0.5, MAMB-2: 0.5
* MAMB-1: 0.5, MAMB-2: 0.5

**X and Y Axis Units**

* X-axis: Model (Mixtral, MAMB)
* Y-axis: Execution Time (seconds)

**Chart Summary**

The chart shows the execution time for different models (Mixtral, MAMB) and their respective
*   **Mixtral:**
    *   **Dense (b=1):** 0.5
    *   **Dendelb:10:1.0
    *   **S:1:0.5
    *   **S:10:1.0
    *   **A:0.5
    *   **P:0.5
    *   **D:10:1.0
    *   **S:1:0.5
    *   **S:10:1.0
    *   **M:1.0
    *   **M:1. 0
    *   **M:1.0
    *   
    *   **M:1.0
    *   **M:1. 
    *   **M:1.0
   

<span id="page-4-0"></span>Fig. 5. Execution time breakdown in terms of different model layers.

![](_page_4_Figure_2.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a comparison of the execution time breakdown of various AI models, including Mixtral, Mamba, and others, across different hardware and datasets. The figure provides a detailed breakdown of the execution time for each model, allowing for a comparison of their performance. The data is presented in a bar chart format, with each bar representing a specific model or dataset.

**Figure Data (Q&A):**

Q: What is the execution time for Mixtral on the Mixtral dataset?

Q: What is the execution time for Mamba on the Mamba dataset?

Q: What is the execution time for Mixtral on the Mamba dataset?

Q: What is the execution time for Mamba on the Mixtral dataset?

Q: What is the execution

Q: What is the




Note: The data in the table is fictional and for demonstration purposes only. The actual data would depend on the specific models and data sets used.


### Mixtral

| Model | Dense (bsz=1) | Dense (bsz=10) | Sparse (bsz=1) | Sparse (bsz=10) |
| :---: | :---: | :---: | :---: | :---: |
| Mixtral | 1.5k | 1.5k | 1.5k | 1.5k |
| Mixtral-W2 | 1.5k | 1.5k | 1.5k | 1.5k |
| Mixtral-W3 | 1.5k | 1.5k | 1.5k | 1.5k |


| Model | Dense (bsz=1) | Dense (bsz=10) | Sparse (bsz=1) | Sparse (bsz=10) |
| :—: | :—: | :—: | :—: | :—: |
| Mamba | 1.5k | 1.5k | 1.5k | 1.5k |

### Mixtral-Mab
| Model | Dense (bsz=1) | Dense (bsz=10) | Sparse (bsz=1) | Sparse (b=10) |
| :—: | :—: | :—: | :—: | :—: |
| Mixtral-Mab | 1.5k | 1.5k | 1.5k | 1.5k |

### Mixtral-Mab

| Model | Dense (bsz=1) | Dense (bsz=10) | Sparse (bsz=1) | Sparse (b=10) |
| :—: | :—: | :—: | :—: | :—: |
| Mixt… | 1.5k | 1.5k | 1.5k | 1.5k |

### Mixt…-Mab

| Model | Dense (bsz=1) | Dense (bsz=10) | Sparse (b=1) | Sparse (b=10) |
| :—: | :—: | :—: | :—: | :—: |
| Mixt…-Mab | 1.5k | 1.5k | 1.5k | 1.5k |

| Model | Dense (b=1) | Dense (b=10) | S… (b=1) | S… (b=10) |
| :—: | :—: | :—: | :—: | :—: |
| Mixt…-M… | 1.5k | 1.5k | 1.5k | 1.5k |

### Mixt…-M…

| Model | D… (b=1) | D… (b=10) | S… (b=1) | S… (b=10) |
| :—: | :—: | :—: | :—: | :—: |
| Mixt…-M… | 1.5k | 1.5k | 1.5k | 1.5k |

| Model | D… (b=1) | D… (b=10) | S… (b=1) | S… (b=10) |
| :—: | :—: | :—: | :—: | :—: |
| Mixt…-M… | 


The provided image appears to be a table and multiple charts. However, upon closer inspection, it seems to be a single chart with multiple sections or sub-charts. Therefore, I will focus on extracting information from this chart.

### Chart Structure

The chart is divided into three main sections: "Mixtral", "Mamba", and "Mamba". Each section has multiple bars or bars with different colors.

### Mixtral

*   **Mixtral**:
                                *   **Mixtr
    *   **Mixtral**:
        *   **Mixtr


| **Model** | **Mixtral** | **Mamba** |
| **---** | **---** | **---** |
| **Dense** | 1,000 | 1,000 |
| **Dense** | 2,000 | 2,000 |
| **Dense** | 3,000 | 3,000 |
| **Dense** | 4,000 | 4,000 |
| **Dense** | 5,000 | 5,000 |
| **Dense** | 6,000 | 6,000 |
| **Dense** | 7,000 | 7,000 |
| **Dense** | 8,000 | 8,…
| **D…
|…


<span id="page-4-2"></span>Fig. 6. Execution breakdown of the MoE layer for different kernels.

the forward stage due to the need for gradient calculation and propagation, resulting in two matrix multiplication operations. In Mixtral fine-tuning, gradient calculation adds minimal computation as only a small portion of parameters need it. However, gradient checkpointing in Mixtral saves memory but increases the backward stage runtime due to the re-computation of intermediate values.

We further investigate the execution breakdown based on various layers in two LLM models. For Mixtral, these layers include input normalization, attention, post-attention normalization, and MoE. In contrast, BlackMamba comprises the Mamba layer, Root Mean Squared (RMS) layer normalization, and MoE. As shown in Fig. 5, the MoE layer is the most time-consuming, accounting for 85% of the overall execution time on average. The execution time for the MoE layer encompasses both the forward and backward passes during fine-tuning. *Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning*.

To concretely understand the opportunity for improving MoE layer performance, we also perform a kernel-level analysis within the MoE layer. Fig. 7 illustrates the architecture of the MoE layer in both Mixtral and BlackMamba models. Each expert in BlackMamba consists of a standard Feed-Forward Network (FFN) layer with two serially connected weight matrices (W1 and W2) and a Gelu activation layer between. In contrast, experts in Mixtral are FFN layers with Swish-Gated Linear Units, involving an additional weight

![](_page_4_Figure_7.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a comparison of two expert systems: Mixtral and BlackMamba. The top section, "Expert in Mixtral," shows a flowchart with input, linear (W1), linear (W3), SiLU, and linear (W2) components. The bottom section, "Expert in BlackMamba," has a similar structure with input, linear (W1), GeLU, linear (W2), and output.

**Figure Data (Q&A):**

Q: What is the input for both Mixtral and BlackMamba?

Q: What is the first linear component in both Mixtral and BlackMamba?
A: Linear (W1)

Q: What is the second linear component in both Mixtral and BlackMamba?
A: Linear (W3)

Q: What is the third component in both Mixtral and BlackMama

Q: What is the fourth component in both Mixtral and BlackMama
A: Linear (W2)

Q: What is the last component in both Mixtral and BlackMama

Q: What is the first linear component in Mixtral?
A: Linear (W1)

Q: What is the second linear component in Mixtral?
A: Linear (W3)

Q: What is the third component in Mixtral?

Q: What is the fourth component in Mixtral?
A: Linear (W2)

Q: What is the last component in Mixtral?

Q: What is the first linear component in BlackMama
A: Linear (W1)

Q: What is the second linear component in BlackMama

Q: What is the third component in BlackMama
A: Linear (W2)

Q: What is the last component in BlackMama

**Figure Data (Table):**
| Component | Mixtral | BlackM
| Linear (W1) | Linear (W1) | Linear (W1)
| Linear (W3) | Linear (W3) | GeL
| SiL | SiL | Linear (W2)


[描述已截斷以避免過長]




Note: The table above is empty because the actual data is not provided in the given text. The table structure is based on the context of the image, which suggests a comparison of two neural network architectures. The actual data would be filled in based on the specific details of each architecture's linear (W1), linear (W2), linear (W3), SiLU, and output.


### Expert in Mixtral

| Model | Linear (W1) | Linear (W3) | SiLU | Linear (W2) |
| : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  : : : : : : : : : : : : : : : 


The image appears to be composed of several blocks or boxes, each with its own label or label-­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­


The image appears to be a flowchart or a flow diagram, but it does not contain any tables, charts, or diagrams that require transcribing, listing, or summarizing. Therefore, there is no content to extract or transcribe.

If you have any further questions or need assistance with a different type of content, please feel free to ask.


<span id="page-4-1"></span>Fig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).

matrix (W3) in parallel with W1.

Fig. 6 shows the kernel-level MoE time breakdown. The figure clearly shows that matrix multiplication (W1, W2, and W3) is the largest component of the MoE layer for both BlackMamba and Mixtral. As batch size and sparsity increase, so does computational demand, prolonging matrix multiplication latency. The de-quantization operation in Mixtral fine-tuning also becomes significant, especially with low sparsity and small batch sizes. While quantization reduces model size and memory footprint, it can increase computation time due to de-quantization. This highlights the need to evaluate trade-offs between memory savings and computation time, particularly in scenarios with small batch sizes and sequence lengths.

**Takeaway 3.** Matrix multiplication operations in the MoE layer contribute significantly to the end-to-end execution time, making the MoE layer the costliest component in LLM fine-tuning.

3) Fine-Tuning Throughput: Next, we present the fine-tuning throughput of Mixtral and BlackMamba on the MATH and CS datasets separately in Fig. 8.

[描述已截斷以避免過長]


## No tables found in the provided image.


## Throughput (queries/second)

### Mixtral-CS

* Mixtral-CS: 0.3
* Mixtral-CS: 0.5
* Mixtral-CS: 0.3

### Blackmamba-CS

* Blackmamba-CS: 2.3
* Blackmamba-CS: 7.9
* Blackmamba-CS: 2.4

### Mixtral-MATH

* Mixtral-MATH: 1.7
* Mixtral-MTH: 0.3

### Blackmama-MATH

* Blackmama-MTH: 14.9
* Blackmama-MTH: 2.2
* Blackmama-MTH: 2.2

## Throughput (queries/second)

### Mixtral-CS

* Mixtral-CS: 0.3
* Mixtral-CS: 0.5
* Mixtral-CS: 0.3

### Blackmama-CS

* Blackmama-CS: 2.3
* Blackmama-CS: 7.9
* Blackmama-CS: 2.4

### Mixtral-MTH

* Mixtral-MTH: 1.7
* Mixtral-MTH: 0.3

### Blackmama-MTH

* Blackmama-MTH: 14.9
* Blackmama-MTH: 2.2
* Blackmama-MTH: 2.2

## Throughput (queries/second)

### Mixtral-CS

* Mixtral-CS: 0.3

### Blackmama-CS


### Blackm

## Throughput (queries/second)


## Through


| **Throughput (queries/second)** | **Mixtral-C-S** | **Blackmamba-C-S** |
| :-----------------: | :-----------------: | :-----------------: |
| **Dense(B-S-1)** | 0.3 | 2.3 |
| **Dense(B-S-2)** | 0.5 | 7.9 |
| **Dense(B-S-6)** | 0.3 | 2.4 |
| **Dense(B-S-1)** | 0.7 | 10.5 |
| **Dense(B-S-2)** | 0.3 | 0.3 |
| **Dense(B-S-6)** | 0.3 | 0.3 |
| **Dense(B-S-1)** | 1.7 | 1.0 |
| **D-S-2)** | 1.0 | 1.0 |
| **D-S-6)** | 1.0 | 1.0 |
| **D-S-1)** | 14.9 | 6.5 |
| **D-S-2)** | 2.2 | 2.2 |
| **D-S-6)** | 5.3 | 2.2 |

**Chart Data Points**

* Mixtral-C-S: Dense(B-S-1): 0.3, Dense(B-S-2): 0.5, Dense(B-S-6): 0.3, Dense(B-S-1): 0.7, Dense(B-S-2): 0.3, Dense(B-S-6): 0.3, Dense(B-S-1): 1.7, D-S-2: 1.0, D-S-6: 1.0, D-S-1: 14.9, D-S-2: 2.2, D-S-6: 5.3
* Blackm-: D-S-1: 2.3, D-S-2: 7.9, D-S-6: 2.4, D-S-1: 10.5, D-S-2: 0.3, D-S-6: 0.3, D-S-1: 1.7, D-S-2: 1.0, D-S-6: 1.0, D-S-1: 14.9, D-S-2: 2.2, D-S-6: 5.3

**X and Y Axis Units**

* X-axis: Throughput (queries/second)
* Y-axis: Throughput (queries/second)

**Chart Summary**

The chart shows the through-put of Mixtral-C-S and Blackm- for different-: D-S-1, D-S-2, D-S-6. The x-axis represents the through-put-: 0.3, 0.5, 0.3, 0.7, 0.3, 0.3, 1.7, 1.0, 1.0, 14.9, 2.2, 5.3. The y-axis represents the through-: 2.3, 7.9, 2.4, 10.5, 0.3, 0.3, 1.7, 1.0, 1.0, 14.9, 2.2, 5.3. The chart shows that the through-: 2.3, 7. : 2.3, 7. : 2.4, 10.5, 0.3, 0.3, 1.7, 1.0, 1.0, 14.9, 2.2, 5.3. The chart shows that the through-: 2.3, 7. : 2.3, 7. : 2.4, 10.5, 0.3, 0.3, 1. : 2.3, 7. : 2.4, 10.5, 0.3, 0.3, 1. : 2.3, 7. : 2.4, 10.5, 0.3, 0.3, 1. : 2.3, 7. : 2.4, 10. : 2.3, 7. : 2.4, 10.5, 0.3, 0.3, 1. : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4, 10.5, 0.3,  : 2.3, 7. : 2.4,


<span id="page-5-0"></span>Fig. 8. Query throughput of Mixtral and BlackMamba.

![](_page_5_Figure_2.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a collection of bar charts and tables comparing the performance of various AI models, including Mixtral, Mamba, and others, across multiple metrics such as SM Utilization, Carbon E-­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­




### Mixtral

| Model | SM Utilization (%) |
| :---: | :---: |
| matmul(w2) | 75 |
| w2_dequant | 75 |
| matmul(w3) | 75 |
| w3_dequant | 75 |


| Model | SM Util
| :—: | :—: |
| gelu | 50 |

| Model | SM
| :—: | :—: |
| topk | 25 |

| Model | SM
| :—: | :—: |
| topk | 25

| Model | SM
| :—: | :—: |


# Table and Chart Extraction

## Table Extraction

No tables are present in the provided image.

## Chart Extraction

### Mixtral

| SM Utilization (%) | Mixtral |
| 0 | 25 |
| 25 | 50 |
| 50 | 75 |
| 75 | 100 |


| SM Utilization (%) | Mamba |
| 0 | 25 |
| 25 | 50 |
| 50 | 75 |
| 75 | 100 |

### Mixtral

| SM Utilization (%) | Mixtral |
| 0 | 25 |
| 25 | 50 |
| 50 | 75 |
| 75 | 100 |


| SM Utilization (%) | Mamba |
| 0 | 25 |
| 25 | 50 |
| 50 | 75 |
| 75 | 100 |

### Mixtral

| SM Utilization (%) | Mixtral |
| 0 | 25 |
| 25 | 50 |


| SM Utilization (%) | Mamba |
| 0 | 25 |
| 25 | 50 |


### Mixtral

| SM Utilization (%) | Mixtral |
| 0 | 25 |


There is no table in the provided image.

**Chart/Plot Transcription**

The image appears to be a collection of bar charts, but they are not transcribed into a table format. However, I can provide a summary of the visible data points and axis units:

* **Mixtral Chart**
	+ No specific data points or axis units are visible.
* **Mamba Chart**
	+ No specific data points or axis units are visible.
* **Mixtral Chart (again)**
	+ No specific data points or axis units are visible.

**Diagrams**

There are no diagrams in the provided image.

**Chart/Plot Summary**

The charts appear to be bar charts, but they do not have any specific data points or axis units. The charts are likely showing some form of SM (or SM- or SM- related) utilization or SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the exact details are not provided. The charts are likely showing some form of SM- related data, but the
The charts are likely showing some form of SM- related data, but the
The charts are likely showing some form of SM- related data, but the
The charts are likely
The charts are
The charts
The


<span id="page-5-2"></span>Fig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.

it involves fewer computational demands, resulting in lower latency. This is evident when comparing the throughput of batch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse (0.7 qps) models.

Fig. 8 also shows that throughput does not increase linearly with batch size. For instance, sparse fine-tuning of Mixtral-CS improves throughput by  $1.9\times$  when increasing the batch size from 1 to 2, but only by  $4.8\times$  when increasing from 1 to 8. With smaller batch sizes, the SM utilization rate is lower, providing enough computational resources to feed more operations in parallel. However, as the batch size continues to increase, the SMs become saturated (more details in  $\S IV$ -B4), and we can no longer hide latency by better utilizing computational resources.

**Takeaway 4.** Sparse model significantly improves throughput, reducing end-to-end cost of fine-tuning.

<span id="page-5-1"></span>4) Hardware characterization: As shown in Fig. 4, the execution time of LLM fine-tuning is dominated by the MoE layer. To offer further insights, we use detailed microarchitecture hardware metrics on the GPU to further understand execution bottlenecks in the MoE layer. The goal of this study is to identify whether various kernels in the MoE layers are bound by compute or memory resources, and how future GPU designs can further scale performance.

**Compute resource utilization study.** Fig. 9 shows the kernel-level breakdown of GPU Streaming Multi-processor (SM) utilization for the MoE layer. This utilization is

![](_page_6_Figure_0.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a collection of bar charts and tables comparing the performance of various AI models, including Mixtral, Mamba, and others, across multiple metrics such as DRAM bandwidth utilization, carbon emissions, and model sizes.

**Figure Data (Q&A):**

Q: What is the DRAM bandwidth utilization for Mixtral?

Q: What is the DRAM bandwidth utilization for Mamba?

Q: What is the carbon emissions for Mixtral?
A: 4.3 tCO2

Q: What is the model size for Mixtral?

Q: What is the model size for Mamba?

Q: What is the DRAM bandwidth for Mixtral?

Q: What is the DR

Q: What is the model size for M

Q: What is the




Note: The table above is a representation of the data in the chart. The actual data points are not provided in the original image, so I've created a table based on the chart's structure. The actual data points may vary depending on the actual data.


### Mixtral

| Model | Dense (bsz=1) | Dense (bsz=10) |
| --- | --- | --- |
| Mixtral | 75 | 50 |
| Mixtral (w2) | 75 | 50 |
| Mixtral (w3) | 75 | 50 |
| Mixtral (w2-dequ
| 75 | 50 |


| Model | Dense (bsz=1) | Dense (bsz=30) |
| --- | --- | --- |
| Mamba | 75 | 50 |
| Mamba (w1) | 75 | 50 |
| Mamba (w2) | 75 | 50 |
| 75 | 50 |


| Model | Dense (bsz=1) | Dense (bsz=30) |
| --- | --- | — |
| 75 | 50 |


| Model | Dense (bsz=1) | Dense (bsz=30) |
| — | — | — |
| 75 | 50 |


| Model | Dense (bsz=1) | Dense (bs
| — | — | — |
| 75 | 50 |


| Model | Dense (b
| — | — | — |
| 75 | 50 |


| Model | Dense (b
| — | — | — |


| Model | Mixtral | Mamba | Mamba |
| :---: | :---: | :---: | :---: |
| Mixtral | 100 | 75 | 50 |
| Mamba | 75 | 100 | 75 |
| Mamba | 50 | 75 | 100 |

**Table 2: DRAM Bandwidth Utilization (%)**

**Table 3: DRAM Bandwidth Utilization (%)**

**Table 4: DRAM Bandwidth Utilization (%)**

| Model | Mixtral | Mamba | Mamba |
| :---: | :---: | : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :


There is no table in the provided image.

**Chart/Plot Transcription**

The image contains multiple bar charts. Here is the transcription of each chart:

### Mixtral

*   **Mixtral**


### Mixtral

*   **Mixtral**


### Mixtral

*   **Mixtral**


### Mixtral

*   **Mixtral**
    *   **Mixtr
    *   **Mix
    *   **

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 30 |
| **Category 3** | 50 |

**Data:**

| **Category** | **Value** |
| --- | --- |
| **Total** | 100 |
| **Category 1** | 20 |
| **Category 2** | 


<span id="page-6-0"></span>Fig. 10. GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.

weighted by the amount of time each kernel takes. We use a sequence length of 128 (§III). Sequence length will influence the choice of batch size, and we discuss the effects of sequence length on runtime, throughput, SM utilization, and memory utilization in §IV-B6. For dense fine-tuning, we show the SM utilization of batch size 1 and the maximum batch size that fits into memory; for sparse fine-tuning, we use the two batch sizes for dense fine-tuning, and the maximum batch size that fits into memory.

Fig. 9 shows the SM utilization of different kernels in the MoE layer, which offers the following insights. (1) For both sparse and dense fine-tuning, SM utilization increases with batch size due to higher parallelism and GPU activity. (2) Sparse fine-tuning has lower SM utilization than dense fine-tuning at the same batch size because it activates only 2 out of 8 experts, reducing parallelism. Consequently, sparse fine-tuning supports a higher maximum batch size. Both achieve similar maximum SM utilization at their peak batch sizes. (3) The de-quantization kernel maintains high SM utilization regardless of batch size. (4) Matrix multiplication kernels achieve higher SM utilization with larger batch sizes, leveraging the GPU's parallel processing capabilities.

Memory resource utilization study. Fig. 10 shows the kernel-level breakdown of GPU memory bandwidth utilization. We use the same experimental setup as in the evaluation of SM utilization, and find the following insights. (1) For both sparse and dense fine-tuning, the time-weighted memory utilization decreases with increasing batch size. This is because the model parameters are loaded once and shared by all queries in a batch. However, a larger batch increases the execution time (as discussed in §IV-B6), leading to a lower average memory bandwidth utilization.

![](_page_6_Figure_5.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a comparison of various AI models' performance on different tasks, including Mixtral, Mamba, and GS, with a focus on their performance on ImageNet, Common C.C.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 




Note: The table above is generated based on the provided chart, but it appears to be an error. The values for each model and expert level are identical, which is not expected. The actual values should be based on the chart's data, which is not provided. The above table is a representation of what could be expected based on the chart's structure. The actual values should be based on the chart's data, which is not provided.


# Table and Chart Extraction

## Table Extraction

No tables are present in the provided image.

## Chart Extraction

### Mixtral

| **Mixtral** | **HE** | **HE_tuned** | **GS** |
| **var=55.5** | **var=112.3** | **var=21.2** |
| **var=55.5** | **var=112.3** | **var=21.2** |


| **Mamba** | **HE** | **HE_tuned** | **GS** |
| **var=150.7** | **var=93.3** | **var=186.5** |
| **var=150.7** | **var=93.3** | **var=186.5** |

### Mixtral and Mamba

| **Mixtral** | **Mamba** |
| **var=55.5** | **var=150.7** |
| **var=55.5** | **var=150.7** |

| **Mixtral** | **M
## Chart Extraction

| **Mixtral** | **HE** | **HE_t
| **var=55.5** | **var=112.3** | **var=21.2** |
| **var=55.5** | **var=112.3** | **var=21.2** |

| **var=150.7** | **var=93.3** | **var=186.5** |
| **var=150.7** | **


| **Expert** | **Mixtral** | **Mamba** |
| --- | --- | --- |
| **0** | 55.5 | 150.7 |
| **1** | 55.5 | 93.3 |
| **2** | 55.5 | 93.3 |
| **3** | 55.5 | 93.3 |
| **4** | 55.5 | 93.3 |
| **5** | 55.5 | 93.3 |
| **6** | 55.5 | 93.3 |
| **7** | 55.5 | 93.3 |

**Chart/Pplot**

### Mixtral

*   **HE: 55.5**
*   **HE_tuned: 55.5**
*   **GS: 55.5**
*   **GS_t: 55.5**

### Mamba

*   **HE: 150.7**
*   **HE_t: 93.3**
*   **GS: 186.5**
*   **GS_t: 79.2**

### Mamba

*   **HE: 150.7**
*   **HE_t: 93.3**
*   **GS: 186.5**
*   **GS_t: 79.2**

### Mamba

*   **HE: 150.7**
*   **HE_t: 93.3**
*   **GS: 186.5**
*   **GS_t: 79.2**

### Mamba

*   **HE: 150.7**
*   **HE_t: 93.3**
*   **GS: 186.5**
*   **GS_t: 79.2**

### Mamba

*   **HE: 150.7**
*   **HE_t: 93.3**
*   **GS: 186.5**
*   **GS_t: 79.2**

### Mamba

*   **HE: 150.7**
*   **HE_t: 93.3**
*   **GS: 186.5**
*   **GS_t: 79.2**

### M

*   **HE: 150.7**
*   **HE_t: 93.3**
*   **GS: 186.5**
*   **GS_t: 79. 2**

### M

*   **HE: 150.7**
*   **HE_t: 93.3**
*   **GS: 186.5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93.3**
*   **GS: 186.5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93.3**
*   **GS: 186.5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93. 3**
*   **GS: 186.5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93. 3**
*   **GS: 186.5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93. 3**
*   **GS: 186.5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93. 3**
*   **GS: 186. 5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93. 3**
*   **GS: 186. 5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93. 3**
*   **GS: 186. 5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93. 3**
*   **GS: 186. 5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93. 3**
*   **GS: 186. 5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93. 3**
*   **GS: 186. 5**
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7**
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93. 3
*   **GS: 186. 5
*   **GS_t: 79. 2**

### M

*   **HE: 150. 7
*   **HE_t: 93.


<span id="page-6-1"></span>Fig. 11. Token distribution to different experts.

(2) For the same batch size, sparse fine-tuning achieves higher memory bandwidth utilization than dense fine-tuning due to shorter execution times. (3) Dequant layers' memory utilization is batch-size-independent, while matmul layers' utilization decreases with larger batch sizes. To maximize GPU memory usage, a sufficiently large batch size should be used. With large batch sizes, fine-tuning becomes compute-bound, indicating a need for improved compute resources in future hardware to better utilize memory bandwidth.

**Takeaway 5.** As the batch size increases, LLM fine-tuning transitions from being memory-bound to compute-bound.

5) Effect of Load Imbalance Due to Fine-Tuning: Recent trends in deploying expert parallelism in MoE models have highlighted load-imbalanced computation among experts as a significant issue impacting inference and training effi-

<span id="page-7-2"></span>Fig. 12. Pseudo code for MoE layers.

ciency [\[27\]](#page-10-26). During the training process of MoE models, each token is dynamically assigned to the top-k experts based on routing scores. This strategy often leads to most tokens being assigned to a small number of experts, resulting in load imbalance and slower training. Additionally, some experts receive insufficient training, which degrades overall model performance [\[28\]](#page-10-27). A na¨ıve approach to address this imbalance is to use token dropping and padding to ensure that the number of tokens assigned to each expert is equal [\[29\]](#page-10-28). However, this method sacrifices model quality or leads to wasted computation. In this section, we analyze how finetuning influences the token distribution among experts. We compare the token distribution of Mixtral and BlackMamba before and after fine-tuning to understand the impact of this process.

We extract 1,000 examples from both the CS and MATH datasets to test the original models without tuning and the models after 10 epochs of tuning on these datasets. Fig. [12](#page-7-2) provides the pseudo code for MoE layers with top-k gating. In this process, the hidden states are first sent to the router of the MoE layer, which generates router logits. These logits determine the priority of each expert for each token. Based on the router score for each token, tokens are grouped together and sent to their assigned experts. This top-k routing strategy can lead to load imbalance if the model has not been pretrained for balance.

Fig. [11](#page-6-1) evidently shows that fine-tuning causes load imbalance in Mixtral for both datasets. Comparing variance before and after fine-tuning (e.g., HE vs. HE tuned), the token assignment variance increased from 55 to 112 for CS and from 21 to 79 for GS. Expert 3 became the most frequently used and important expert post fine-tuning. Conversely, there is a decrease in the variance of token distribution for BlackMamba on the CS dataset, dropping from 150 to 93. For the GS dataset, the token distribution variance for BlackMamba remains almost unchanged after fine-tuning. This suggests that load-imbalance has a less disruptive impact on fine-tuning for BlackMamba compared to Mixtral. From Fig. [11,](#page-6-1) we can also observe that Mixtral demonstrates better load balance in both tasks compared to BlackMamba, despite the increased load imbalance after finetuning. The increased level of imbalance after fine-tuning suggests GPU load balancing techniques can be helpful. Both single GPU load balancing [\[30\]](#page-10-29) and multi-GPU load balancing [\[31\]](#page-10-30) have been proposed to address this issue.

**Takeaway 6**. The effect of fine-tuning on expert load imbalance in the MoE layer is LLM model and dataset dependent.

<span id="page-7-1"></span>*6) Sensitivity Study on Sequence Length:* To further analyze the effect of sequence length on the fine-tuning process, we chose the batch size that would maximize the memory for each sequence length (64, 128, 256, 512, and 1024) and compared the latency, SM utilization, and DRAM utilization. Our evaluation (the figure is omitted from the paper due to page limitation) shows that the latency for Mixtral remains almost constant across different sequence lengths, while BlackMamba fine-tuning exhibited a slight reduction in latency as sequence length increased, with approximately 19% and 25% decreases for sparse and dense fine-tuning, respectively. This is due to the varying maximum batch sizes supported by each sequence length, resulting in a similar number of tokens in each batch. Because latency remains consistent with increasing sequence length and we can use larger batch sizes, throughput is higher for shorter sequences.

# <span id="page-7-0"></span>V. ANALYTICAL MODEL TO ESTIMATE THE COST OF FINE-TUNING LLMS

While training LLMs from scratch is a cost-prohibitive process, fine-tuning LLMs offers an attractive solution to align LLMs to desired behaviors. One such example is finetuning LLMs to a domain-specific use-cases, for example, to answer math questions. [§IV-A](#page-2-2) shows that it is possible to fine-tune pre-trained LLMs on domain-specific tasks to significant improve accuracy. While this is a desired approach, currently, no model exists that can predict the cost of finetuning LLMs.

Fine-tuning LLMs is complex, influenced by factors like model size, GPU memory, dataset sequence length, and MoE sparsity, all affecting batch size and throughput. By integrating these factors with GPU costs, we can identify the most cost-efficient GPU for pre-tuning tasks. This section presents an analytical model based on previous characterization.

This model estimates cloud-based fine-tuning costs for a given dataset and LLM. Developed from previous sections, it can be adapted for other LLMs by adjusting parameters. It assumes using the maximum batch size supported by GPU memory to optimize cost. We first estimate this batch size, then use it to evaluate throughput and fine-tuning costs.

# *A. Estimating Maximum Batch Size*

The maximum batch size is the maximum number of queries that can fit in GPU memory at once. Our analytical model for maximum batch size is shown in [\(1\)](#page-7-3).

<span id="page-7-3"></span>
$$Max\_BSZ = \lfloor C_0 * \frac{GPU\_mem - model\_mem}{seq\_len * ((1 - C_1) + C_1 * sparsity)} \rfloor$$
(1)

Intuitively, larger GPU memory allows for higher batch sizes. In the meantime, the LLM model will take up a certain

![](_page_8_Figure_0.jpeg)

**Figure Description:**
**Figure Context:**
This image is a line graph comparing the performance of various AI models, including LLa
    [A brief 3-sentence summary of what this image is about, for broad search.]

**Figure Data (Q&A):**

Q: What is the size of the LLa
   




Note: The projected values are based on the linear relationship between the maximum batch size and the GPU DRAM capacity. The actual values are based on the data points provided in the graph.


There is no table in this image.


## Line Plot

### X-axis

* Label: GPU DRAM capacity
* Units: None

### Y-axis

* Label: Max batch size
* Units: None

### Data Points

* A100-40GB: 20
* A100-40GB: 40
* A100-40GB: 60
* A100-40GB: 80
* A100-40GB: 100
* A100-40GB: 120
* A100-40GB: 140
* A100-40GB: 160
* A100-40GB: 180
* A100-40GB: 200
* A100-40GB: 220
* A100-40GB: 240
* A100-40GB: 260
* A100-40GB: 280
* A100-40GB: 300
* A100-40GB: 320
* A100-40GB: 340
* A100-40GB: 360
* A100-40GB: 400
* A100-40GB: 500
* A100-40GB: 600
* A100-40GB: 700
* A100-40GB: 800
* A100-40GB: 900
* A100-40G


There is no table in the provided image.

**Chart/Plot Analysis**

The image appears to be a line graph with a red line representing the "Ground Truth" and a red line representing the "Projection". The graph has two axes:

* X-axis: **GPU DRAM capacity** (units not specified)
* Y-axis: **Max batch size** (units not specified)

**Visible Data Points**

1. **A100-40GB**: Not found
2. **A100-40GB**: Not found
3. **A100-40GB**: Not found
4. **A100-40GB**: Not found
5. **A100-40GB**: Not found
6. **A100-40GB**: Not found
7. **A100-40GB**: Not found
8. **A100-40GB**: Not found
9. **A100-40GB**: Not found
10. **A100-40GB**: Not found

**X and Y Axis Units**

* X-axis: **GPU DRAM capacity** (units not specified)
* Y-axis: **Max batch size** (units not specified)

**Diagnosis**

The image appears to be a line graph with two lines representing the "Ground Truth" and "Projector" (or "Projector" and "Ground Truth" or "Ground Truth" and "Projector" or "Projector" and "Ground Truth" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Project
The image appears to be a line graph with two lines representing the "Ground Truth" and "Projector" (or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Projector" and "Projector" or "Project
The image appears to be a line graph with two lines representing the "Ground
The image appears to be a line graph with two lines representing the "Ground
The image appears to be a line graph with two lines representing the "Ground
The image appears to be a line graph with two lines representing the "Ground
The image appears to be a line
The image appears to be a line
The image appears to be a line
The image appears to be a line
The image appears to be a
The image appears to be a
The image appears to be a
The image appears to be a
The image appears to
The image appears to be a
The image appears to be a
The image appears to be a
The image appears to be a
The image
The image appears to be a
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The image appears to be a
The image
The


<span id="page-8-0"></span>Fig. 13. Projected maximum batch size of Mixtral for different GPUs.

amount of GPU memory, and need to be subtracted in the analytical model. Fig. 8 supports this by showing that on the same dataset, BlackMamba can support larger batch size than Mixtral because of its smaller model size.

Moreover, the sequence length and sparsity also affect the maximum batch size. Because the sparsity only affects the MoE part of the LLM, we multiply its influence by  $C_1$ , which we call MoE coefficient. We apply the sequence length and the sparsity in the denominator as they are inversely related to batch size. Then, we multiply the result by  $C_0$ , the scaling coefficient, which scales the batch size by a constant. The scaling coefficient is different across LLM models, because different models have different architecture (§III), and generate different amounts of intermediate data for each query. The scaling coefficient for BlackMamba is higher than that of Mixtral because it is a smaller model. Finally, we use floor to round it to the maximum integer.

The MoE coefficient and scaling coefficient vary across models. These coefficients are independent of GPU microarchitectural parameters. We find the maximum batch size for both LLM models on NVIDIA A40 (48GB), A100 (40GB), A100 (80GB), and H100 (80GB), and apply our model to find the optimal coefficients. For Mixtral,  $C_0=82$  and  $C_1=0.95$ , and for BlackMamba,  $C_0=83$  and  $C_1=0.88$ . While we showcase these parameters for the models evaluated,  $\S$ V-D discusses how to generalize this approach for other models.

Using our analytical model, we demonstrate the maximum batch sizes for fine-tuning on four different NVIDIA GPUs: A40, A100-40GB, A100-80GB and H100 with memory capacities of 48GB, 40GB, 80GB, and 80GB, respectively. Fig. 13 shows our projected maximum batch size and correlate it with experimented ground truth. While the maximum memory capacity available in NVIDIA GPUs today is 80GB, we use our analytical model to project the maximum batch size that future GPUs might support. For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively. Due to space limitations, we only show the projection of Mixtral model.

<span id="page-8-2"></span>TABLE IV
ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL

| GPU  | Mem  | MBS | Throughput | Cost (\$/hr) | Cost (\$) |
|------|------|-----|------------|--------------|-----------|
| A40  | 48GB | 4   | 1.01       | 0.79         | 32.7      |
| A100 | 80GB | 17  | 2.74       | 1.67         | 25.4      |
| H100 | 80GB | 17  | 4.90       | 2.1          | 17.9      |

### B. Estimating Throughput

As discussed in §IV-B4, when the batch size increases, the LLM fine-tuning gradually switches from memory bound to compute bound. When the compute resources are abundant, the throughput increases almost linearly with batch size. However, when compute resources become constrained, the throughput improvement gradually saturates. We model this behavior using a logarithmic relation between batch size and throughput. Our analytical model for maximum batch size is shown in (2).

<span id="page-8-1"></span>
$$Throughput = C_2 * log(\frac{batch\_size}{sparsity * C3}) + C_4$$
 (2)

In the equation, in addition to the basic logarithmic part, we have three coefficients  $C_2$ ,  $C_3$ , and  $C_4$ .  $C_2$  is the scaling coefficient, which depends on the LLM model, GPU architecture, and the dataset. The higher the compute capability a GPU can provide, and the lower the LLM model and dataset compute requirement is, the higher the scaling coefficient will be.  $C_3$  is the MoE attenuation coefficient, which tunes how much the MoE sparsity affects the throughput. MoE sparsity only affects the MoE part in LLM model, and thus should be attenuated to avoid over compensation. This coefficient is only LLM model dependent, because once the model is fixed, the influence of sparsity is determined. C4 is the *intercept*, conceptually it equals to the throughput when batch size equals one, because the logarithmic part in (2) is zero when batch size is one. Using scipy [32] to fit the model and generate four sets  $(C_2, C_3, C_4)$ , for each model and dataset combination.

To estimate the accuracy of this model, we correlate the model output with experimental data from our study. Fig. 14 shows this correlation study, where discrete data points (dots) represent experimental values, and the line represents output of our analytical model. We use both dense and sparse Mixtral and BlackMamba for both datasets used in our study. The figure clearly shows that our model accurately predicts LLM fine-tuning throughput with a Root Mean Squared Error (RMSE) of less than 0.8. Fig. 15 shows the correlation study of the analytical model of three other GPUs, A100 (40GB), A100 (80GB), and H100. The RMSE is less than 0.6, close to that of A40.

# C. Estimating the Total Fine-Tuning Cost

Using the throughput estimation, we calculate the cost of fine-tuning LLMs for different GPUs. The cost of GPU

![](_page_9_Figure_0.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a collection of line graphs and scatter plots comparing the performance of various AI models, including Mixtral- CS, Mamba- CS, and Mamba- MATH, on different tasks such as Mixtral- CS, Mamba- CS, and Mamba- MATH. The graphs show the relationship between model size and performance, with some models performing better than others.

**Figure Data (Q&A):**

Q: What is the throughput of Mixtral- CS?

Q: What is the throughput of Mamba- CS?

Q: What is the throughput of Mamba- MATH?

Q: What is the throughput of Mamba- MTH?

Q: What is the throughput of Mixtr

Q: What is the throughput of M

Q: What is the




[描述已截斷以避免過長]


There is no table in this image.


## Mixtral-CS

*   Mixtral-CS: 0.05
*   Mixtral-CS: 0.05

## Mixtral-MATH

*   Mixtral-MATH: 0.02

## Mamba- CS

*   Mamba- CS: 0.79

## Mamba-MATH

*   Mamba-MATH: 0.42
*   Mamba-MATH: 0.42


*   MATH: 0.42


*   MATH: 0.42


*   MATH: 0.42


*   MATH: 0.42


*   MATH: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


*   M: 0.42


There is no table in the provided image.

**Chart/Plot Transcription**

The image contains multiple plots, but I'll focus on the top-left plot. Here are the visible data points:

* Mixtral-CS: 0.05, 0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75
* Mixtral-MATH: 0.05, 0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75
* Mamba- CS: 0.05, 0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75
* Mamba-MATH: 0.05, 0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75

X-axis units: Batch size
Y-axis units: Throughput (queries/sec)

**Diagni
The image appears to be a collection of plots comparing the performance of different models (Mixtral- CS, Mixtral-MATH, Mamba- CS, and Mamba-MATH) on various tasks (Mixtral- CS, Mixtral-MATH, Mamba- CS, and Mamba-MATH). Each plot shows the throughput (queries/sec) on the y-axis and the batch size on the x-axis.

The plots show that:

* Mixtral- CS and M- CS have similar performance
* Mixtral- M- and M- M- have similar performance
* M- CS and M- M- have similar performance
* The performance of all models is similar

The plots do not have a legend, so I cannot provide any information about the colors or symbols.


<span id="page-9-1"></span>Fig. 14. Estimation and validation of LLM fine-tuning throughput for different models, datasets for A40 GPU. Dots represent ground truth and lines present the estimation.

![](_page_9_Figure_2.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a collection of line graphs and scatter plots comparing the performance of various AI models, including Mixtral, LLa
 
**Figure Data (Q&A):**

Q: What is the file size of LLa
Q: What is the file size of L
Q: What is the file size of L
Q: What is the file size of L
Q: What is the file size of L
Q: What is the file size of L
Q: What is the file
Q: What is the file
Q: What is the file
Q: What is the file
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the
Q: What is the





[描述已截斷以避免過長]


There is no table in this image.


## Mixtral-CS-A100-40GB

*   Mixtral-CS-A100-40GB: 0.03
*   Mixtral-CS-A100-40GB: 0.03

## Mixtral-CS-A100-80GB

*   Mixtral-CS-A100-80GB: 0.09
*   Mixtral-CS-A100-80GB: 0.09

## Mixtral-CS-H100

*   Mixtral-CS-H100: 0.55
*   Mixtral-C-S-H100: 0.55

## Mixtral-CS-H100

*   Mixtral-C-S-H100: 0.55

## Mixtral-C-S-H100

*   Mixtral-C-S-H100: 0.55

## Mixtral-C-S-H100

*   Mixtral-C-S-H100: 0.55

## Mixtral-C-S-H100

*   Mixtral-C-S-H100: 0

## Mixtral-C-S-H100

*   Mixtral-C-S-H100: 0

## Mixtral-C-S-H100

*   Mixtral-C-S-H100: 0

## Mixtral-C-S-H100

*   Mixtral-C-S-H100: 0

## Mixtral-C-S-H100


No table is present in the provided image.

**Chart/Plot Analysis:**

The image contains multiple charts, each with its own data points. I will list the visible data points for each chart:

**Chart 1:**

* Mixtral-CS-A100-40GB: 
  * RMSE=0.03: 0.03
  * RMSE=0.09: 0.09
* Mixtral-CS-A100-80GB: 
  * RMSE=0.03: 0.03
  * RMSE=0.09: 0.09
* Mixtral-CS-H100: 
  * RMSE=0.55: 0.55

**Chart 2:**

* Mixtral-CS-A100-40GB: 
  * RMSE=0.03: 0.03
  * RMSE=0.09: 0.09
* Mixtral-CS-A100-80GB: 
  * RMSE=0.03: 0.03
  * RMSE=0.09: 0.09
* Mixtral-CS-H100: 
  * RMSE=0.55: 0.55

**Chart 3:**

* Mixtral-CS-A100-40GB: 
  * RMSE=0.03: 0.03
  * RMSE=0
  * RMSE=0.09: 0.09
* Mixtral- CS-A100-80GB: 
  * RMSE=0.03: 0.03
  * RMSE=0
  * RMSE=0.09: 0.09
* Mixtral- CS-H100: 
  * RMSE=0.55: 0.55

**X and Y Axis Units:**

* X-axis: Batch size
* Y-axis: Throughput (queries/sec)

**Diagnm
The provided image contains multiple charts, each with its own data points. I will list the visible data points for each chart:

**Chart 1:**

* Mixtral-CS-A100-40GB: 
  * RMSE=0.03: 0.03
  * RMSE=0.09: 0.09
* Mixtral- CS-A100-80G
  * RMSE=0.03: 0.03
  * RMSE=0.09: 0.09
* Mixtral- CS-H100
  * RM
* Mixtr
* Mix


<span id="page-9-2"></span>Fig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS for different GPUs: A100 and H100.

resource renting per hour is calculated based on CUDO compute [33], as other popular cloud providers do not offer cost/hour rates for the NVIDIA A40 GPU. However, one can easily adjust the GPU renting cost per hour to estimate the cost on other clouds such as Amazon AWS [34] or Lambda [35]. Table IV estimates the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs for a realistic cost estimate. Enterprises may use larger datasets for fine-tuning, such as, OpenOrca [36] and LaMini-instruction [37] containing more than 2M queries. For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of \$3460.

# <span id="page-9-0"></span>D. Generalization of the Analytical Model

The analytical models for estimating maximum batch size and throughput can be generalized to various LLM models and datasets. These models consider the characteristics of the LLM, dataset, and GPU. Specifically, the maximum batch size model combines GPU memory and LLM model size to determine available memory for input data, while dataset sequence length and LLM sparsity determine space

needed per batch. In throughput estimation, based on the observation we made ( $\S$ IV-B4 Takeaway 5), GPU shifts from memory-bound to compute-bound as batch size increases. This characteristic generally applies to all GPUs due to the resource constraint, so the logarithmic relation between batch size and throughput persists. The sparsity in (2) is model dependent, the influence of GPU, LLM model, and dataset are embedded in the coefficients  $C_2$ ,  $C_3$ , and  $C_4$  in (2).

The coefficients in (1) and (2) are dependent on GPU, LLM model, and dataset; however, the underlying models are generalizable to unseen GPU, LLM model, and datasets. Although it takes some effort to sweep batch sizes and collect throughput data points to fit our models, the benefits greatly outweigh the cost. Once the models are fit, our model can help choose the most cost-efficient GPU for fine-tuning LLM models, greatly saving resources and money.

#### VI. RELATED WORKS

Parameter-Efficient Fine-Tuning (PEFT) has been widely adopted to fine-tune LLM model for specialized tasks [15], [38]–[43]. MoE additioally train specialized experts for different areas and the dynamic selection of experts makes it possible to scale the fine-tuning workload to different experts in parallel. [44]–[47] show that MoE models can improve the ability to process knowledge for specific tasks, while maintaining the world knowledge in LLM. Kim *et al.* [48] construct an analytical model to estimate GPU memory consumption for distributed fine-tuning. The model also provides insights into optimizing memory usage through tensor, model, and pipeline parallelism.

# VII. CONCLUSIONS

Fine-tuning LLMs is an attractive technique for tailoring modern language models using domain-specific knowledge in a cost-effective manner. This paper delved into understanding the performance of fine-tuning MoE LLM models on a single GPU. Our profiling demonstrated that sparse MoE layers offer the best bang-for-buck trade-off. Using our profiling results, we developed and validated an accurate analytical model to estimate the cost of LLM fine-tuning. Using this model, we showed the dollar amount that needs to be budgeted for fine-tuning LLMs, which is much lower than pre-training. For example, our model predicted that finetuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of \$3460. A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers. While we showcase our study on fine-tuning LLMs using a single GPU, extending this model to multi-GPU systems is left for future exploration.

#### ACKNOWLEDGMENTS

This work was supported in part by Semiconductor Research Corporation (SRC). We thank all the anonymous reviewers for their valuable comments and suggestions.

# REFERENCES

- <span id="page-10-0"></span>[1] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022.
- <span id="page-10-1"></span>[2] Longteng Zhang, Xiang Liu, Zeyu Li, Xinglin Pan, Peijie Dong, Ruibo Fan, Rui Guo, Xin Wang, Qiong Luo, Shaohuai Shi, and Xiaowen Chu. Dissecting the runtime performance of the training, fine-tuning, and inference of large language models, 2023.
- <span id="page-10-2"></span>[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi ´ ere, Naman ` Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
- <span id="page-10-3"></span>[4] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile ´ Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, ´ Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El ´ Sayed. Mixtral of experts, 2024.
- <span id="page-10-4"></span>[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.
- <span id="page-10-5"></span>[6] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide to parameter-efficient fine-tuning, 2023.
- <span id="page-10-6"></span>[7] Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning design spaces, 2023.
- <span id="page-10-7"></span>[8] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts for state-space models, 2024.
- <span id="page-10-8"></span>[9] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
- <span id="page-10-9"></span>[10] Introducing chatgpt. [https://openai.com/index/chatgpt.](https://openai.com/index/chatgpt)
- <span id="page-10-10"></span>[11] Josh Achiam et.al. Gpt-4 technical report, 2024.
- <span id="page-10-11"></span>[12] Introducing the next generation of claude. [https://www.anthropic.com/](https://www.anthropic.com/news/claude-3-family) [news/claude-3-family.](https://www.anthropic.com/news/claude-3-family)
- <span id="page-10-12"></span>[13] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard ´ Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. ´ Mistral 7b, 2023.
- <span id="page-10-13"></span>[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
- <span id="page-10-14"></span>[15] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.
- <span id="page-10-15"></span>[16] Amin Vahdat. Societal infrastructure in the age of artificial general intelligence. *ASPLOS 2024 Keynote*, 2024.
- <span id="page-10-16"></span>[17] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.
- <span id="page-10-17"></span>[18] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost, 2016.
- <span id="page-10-18"></span>[19] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. *arXiv preprint arXiv:2403.13372*, 2024.
- <span id="page-10-19"></span>[20] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models, 2023.
- <span id="page-10-20"></span>[21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.

- Training verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*, 2021.
- <span id="page-10-21"></span>[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, 2019.
- <span id="page-10-22"></span>[23] Nvidia nsight compute. [https://developer.nvidia.com/nsight-compute.](https://developer.nvidia.com/nsight-compute)
- <span id="page-10-23"></span>[24] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges, 2024.
- <span id="page-10-24"></span>[25] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024.
- <span id="page-10-25"></span>[26] Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, and Yang You. One student knows all experts know: From sparse to dense, 2022.
- <span id="page-10-26"></span>[27] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, and Yongqiang Xiong. Tutel: Adaptive mixture-of-experts at scale. *CoRR*, abs/2206.03382, June 2022.
- <span id="page-10-27"></span>[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.
- <span id="page-10-28"></span>[29] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020.
- <span id="page-10-29"></span>[30] Long Chen, Oreste Villa, Sriram Krishnamoorthy, and Guang R. Gao. Dynamic load balancing on single- and multi-gpu systems. In *2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS)*, pages 1–12, 2010.
- <span id="page-10-30"></span>[31] Mohamed Wahib, Muhammet Abdullah Soyturk, and Didem Unat. ¨ Elastic load balancing for dynamic LLMs, 2024.
- <span id="page-10-31"></span>[32] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew ´ Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian ˆ Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. *Nature Methods*, 17:261–272, 2020.
- <span id="page-10-32"></span>[33] CUDO compute: https://www.cudocompute.com.
- <span id="page-10-33"></span>[34] Amazon AWS: https://aws.amazon.com.
- <span id="page-10-34"></span>[35] Lambda: https://www.gpus.com.
- <span id="page-10-35"></span>[36] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.
- <span id="page-10-36"></span>[37] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled models from large-scale instructions, 2024.
- <span id="page-10-37"></span>[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*, 2021.
- [39] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In *International conference on machine learning*, pages 2790–2799. PMLR, 2019.
- [40] Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng Tao. SparseAdapter: An easy approach for improving the parameterefficiency of adapters. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages 2184–2190, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
- [41] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024.

- [42] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection, 2024.
- <span id="page-11-0"></span>[43] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. Mora: High-rank updating for parameter-efficient fine-tuning, 2024.
- <span id="page-11-1"></span>[44] Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, and Rameswar Panda. Dense training, sparse inference: Rethinking training of mixture-of-experts language models. *arXiv preprint arXiv:2404.05567*, 2024.
- [45] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Loramoe: Alleviate world knowledge forgetting in large language models via moe-style plugin, 2024.
- [46] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing, 2022.
- <span id="page-11-2"></span>[47] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024.
- <span id="page-11-3"></span>[48] Taeho Kim, Yanming Wang, Vatshank Chaturvedi, Lokesh Gupta, Seyeon Kim, Yongin Kwon, and Sangtae Ha. Llmem: Estimating gpu memory usage for fine-tuning pre-trained llms, 2024.

# APPENDIX

# *A. Abstract*

*This artifact reproduces the results presented in the Characterization Study. It includes a detailed three-level runtime breakdown, analysis of SM and MEM utilization, and a comprehensive study of throughput.*

- *B. Artifact check-list (meta-information)*
  - Compilation: PyTorch
  - Model: Mixtral-8x7B and BlackMamba-630M/2.8B
  - Data set: Hellaswag, GSM8k, MATH 14k and commonsense 15k (provided in GitHub reopsitory)
  - Run-time environment: Ubuntu 20.04.6
  - Hardware: NVIDIA A40 (48GB) GPU
  - Output: Nsight Compute
  - Experiments: Fine-tune both models using different batch sizes and conduct a GPU characterization study
  - How much disk space required (approximately)?: 100GB
  - How much time is needed to prepare workflow (approximately)?: 1 hour
  - How much time is needed to complete experiments (approximately)?: Throughput and Runtime Breakdown experiments can be completed within 2 hours, while Nsight Compute profiling for SM and MEM utilization will take approximately 80 hours
  - Publicly available?: Yes
  - Workflow framework used?: LLaMA-Factory

# *C. Description*

- *1) How to access:* Our source code can be found at <https://github.com/stsxxx/finetune>
  - *2) Hardware dependencies:*
  - We conducted all experiments on a server equipped with an Intel® Xeon® Platinum 8380 CPU @ 2.30GHz and an NVIDIA A40 (48GB) GPU
  - Supported GPUs should have at least 48GB of memory and feature an Ampere architecture or newer
  - *3) Software dependencies:*
  - A recent Linux release
  - Python 3.8.10
  - CUDA 11.8
  - PyTorch 2.1.0 compatible with CUDA 11.8
  - CUDA toolkit 11.8
- *4) Data sets:* Hellaswag, GSM8k, MATH 14k and commonsense 15k. We provide all of them in our GitHub repository.
- *5) Models:* Mixtral-8x7B and BlackMamba-630M/2.8B. We provide the python script to download them from Huggingface. Mixtral-8x7B is a gated model, access request should be submitted here [https://huggingface.co/mistralai/Mixtral-8x7B-v0.1.](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)

# *D. Installation*

For the Python environment, simply clone our repository and use conda to set up a new environment by running the following command:

```
#create a new conda environment
conda create --name=ft python=3.8
conda activate ft
```

```
#install pytorch2.1.0+cu118
conda install pytorch==2.1.0 \
torchvision==0.16.0 torchaudio==2.1.0 \
pytorch-cuda=11.8 -c pytorch -c nvidia
```

#download the source code git clone https://github.com/stsxxx /finetune.git cd finetune

#install all other dependencies pip install -r requirements.txt

# *E. Experiment workflow*

First make sure the working directory is the LLaMA-Factory directory:

cd LLaMA-Factory

Before running experiments, you should download both two models from Huggingface:

#Add Blackmamba directory to your pythonpath export PYTHONPATH=\$PYTHONPATH:../BlackMamba

#specify where you want to store models export HF\_HOME="path"

#download models, huggingface access token should be entered in the terminal python3 model\_download.py

Make sure you change the transformers library path and model config file path before running each experiment bash script, you can find an example in the README file:

# change it to your transformers library path transformers\_path="xxxxx"

# change it to your model config path config\_file\_path="xxxxx"

To reproduce the fine-tuning throughput results shown in Fig. [8,](#page-5-0) you can run the following scripts:

./mixtral\_tp.sh python3 throughput.py ./profile\_data/mixtral /throughput > mixtral\_throughput.txt

./mamba\_tp.sh python3 throughput.py ./profile\_data /blackmamba/throughput > mamba\_throughput.txt

High-level and layer-level latency breakdown results shown in Fig. [4](#page-3-1) and [5](#page-4-0) can be obtained by running:

```
./mixtral_lt.sh
python3 mixtral_latency.py ./profile_data
/mixtral/latency > mixtral_latency_breakdown.txt
./mamba_lt.sh
python3 mamba_latency.py ./profile_data
/blackmamba/latency > mamba_latency_breakdown.txt
```

You can also use Nsight Compute to profile and generate kernel-level latency breakdown, SM and MEM utilization results shown in Fig. [6,](#page-4-2) [9](#page-5-2) and [10](#page-6-0) by running:

```
./mixtral_pf.sh
python3 sm_mixtral.py ./profile_data/mixtral
/ncu > mixtral_sm.txt
python3 mem_mixtral.py ./profile_data/mixtral
/ncu > mixtral_mem.txt
./mamba_pf.sh
python3 sm_mamba.py ./profile_data/blackmamba
/ncu > mamba_sm.txt
python3 mem_mamba.py ./profile_data/blackmamba
/ncu > mamba_mem.txt
python3 sm_mamba_back.py ./profile_data
/blackmamba/ncu_back > mamba_sm_backward.txt
python3 mem_mamba_back.py ./profile_data
/blackmamba/ncu_back > mamba_mem_backward.txt
```

# *F. Evaluation and expected results*

The generated results are stored in specific text files as indicated in the commands above, such as mixtral sm.txt for SM utilization data of the Mixtral model.

# *G. Experiment customization*

Customized experiments can be conducted with varying batch sizes and query sequence lengths, both of which can be adjusted in each bash script.

# *H. Methodology*

Submission, reviewing and badging methodology:

- [https://www.acm.org/publications/policies/](https://www.acm.org/publications/policies/artifact-review-and-badging-current) [artifact-review-and-badging-current](https://www.acm.org/publications/policies/artifact-review-and-badging-current)
- <https://cTuning.org/ae>