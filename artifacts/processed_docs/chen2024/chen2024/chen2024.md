## Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation

Shaoyuan Chen<sup>1</sup> Wencong Xiao<sup>2</sup> Yutong Lin<sup>1</sup> Mingxing Zhang<sup>1</sup> Yingdi Shan<sup>1</sup> Jinlei Jiang<sup>1</sup> Kang Chen<sup>1</sup> Yongwei Wu<sup>1</sup>

> <sup>1</sup>Tsinghua University <sup>2</sup>ByteDance

## Abstract

Transformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.

To enhance the efficiency of LLM decoding, we introduce model-attention disaggregation. This approach leverages a collection of cheap, memory-optimized devices for the attention operator while still utilizing high-end accelerators for other parts of the model. This heterogeneous setup ensures that each component is tailored to its specific workload, maximizing overall performance and cost efficiency. Our comprehensive analysis and experiments confirm the viability of splitting the attention computation over multiple devices. Also, the communication bandwidth required between heterogeneous devices proves to be manageable with prevalent networking technologies. To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. Experimental results indicate that Lamina can provide 16.1 ∼ 90.1% higher estimated throughput than existing solutions with similar costs.

## 1 Introduction

## 1.1 Motivation

Disaggregated serving architectures for large language models (LLMs) [\[40,](#page-14-0) [41,](#page-14-1) [59\]](#page-15-0) have recently emerged as efficient frameworks for handling generative inference requests. The core concept of disaggregation involves allocating separate

resources for different tasks to improve resource utilization. This approach aligns perfectly with LLM processing, which can be divided into two distinct phases. The first phase, known as the prefill phase, processes all input tokens from the prompt in parallel and is computation-bound. The second phase, i.e., the decode phase, generates the output tokens one after another, and is typically memory-bound.

Splitting the two phases of inference reduces interference between different requests and allows for more flexible parallel configurations for the two phases. To better leverage the differing characteristics of each phase, several methods propose using heterogeneous hardware to reduce the cost of disaggregated serving [\[12,](#page-12-0) [59\]](#page-15-0). Specifically, flagship all-rounder GPUs like NVIDIA H100 integrate high-performance computational units and high-bandwidth memory (HBM) within a single package, delivering good performance for LLM inference. However, as shown in [Table 1,](#page-1-0) specialized accelerators optimized for either computation or bandwidth can be significantly cheaper than the H100 in terms of TFLOPS per dollar/watt (e.g., TPU v6e) or bandwidth per dollar/watt (e.g., NVIDIA H20), but not both. This cost disparity arises because all-rounder GPUs combine powerful computation units, HBM controllers, and high-bandwidth internal buses within a single chip. Such integration leads to larger die sizes and increased transistor counts, posing additional challenges for chip designing, packaging, and thermal management [\[21,](#page-12-1) [25,](#page-13-0) [55\]](#page-15-1), all of which drive up the design and manufacturing cost.

According to our analyses and experiments, while the separation of resources works well for the prefill nodes, we identified significant inefficiencies in the decoding phase. For instance, as analyzed in [section 2,](#page-2-0) the computation resource utilization is often below 20% when serving the LLaMA3- 70B model with H100. This is primarily due to the limited GPU memory size, which cannot accommodate the large aggregated KV cache for large batches, as well as the low arithmetic intensity of the attention operators.

A detailed examination reveals that the decoding phase mainly comprises two types of operators, each facing distinct resource bottlenecks. Linear transformations, includ-

<span id="page-1-0"></span>Table 1: H100, H20, and TPU v6e specifications.

|                      | H100       | H20         | TPU v6e [7] |
|----------------------|------------|-------------|-------------|
| BF16 TFLOPs          | 989        | 148         | 918         |
| Memory capacity      | 80 GB      | 96 GB       | 32 GB       |
| Memory bandwidth     | 3.35 TB/s  | 4.0 TB/s    | 1.64 TB/s   |
| Power rating         | 700 W      | 400 W       | unlisted    |
| Inter-chip bandwidth | 450 GB/s   | 450 GB/s    | 448 GB/s    |
| Network bandwidth    | 400 Gbps   | 400 Gbps    | 200 Gbps    |
| Price per chip [2]   | \$11.06/hr | \$4.63/hr * | \$2.70/hr   |

<sup>\*:</sup> As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.

ing QKVO projections and feedforward networks, are implemented with *generalized matrix-matrix multiplications* (GEMMs). Since all requests multiply with the same parameter matrices in these operators, processing multiple requests in batch can avoid repeated parameter loads from memory, making these operators primarily computation-bound. In contrast, the self-attention operator is memory-bound. This pivotal operator requires each request to read its own, distinct KV cache, resulting in a *batched generalized matrix-vector multiplication* (BGEMV) pattern. Increasing batch sizes does not improve the computation resource utilization but places additional pressure on the already limited memory capacity.

## 1.2 Our Contributions

In light of the above findings, we propose an innovative concept called model-attention disaggregation, as illustrated in Figure [1.](#page-1-1) This approach involves further disaggregating the decoding phase by creating two pools of heterogeneous accelerators: one optimized for computational power and the other for memory resources. We use the memory-optimized accelerators to store the KV caches and process self-attention operators, while the computation-optimized devices handle all other operators. By choosing the most suitable devices for each kind of operators, this architecture further increases hardware utilization and leads to better overall performance. Moreover, different LLMs and workloads present varying computation and memory resource requirements. Homogeneous accelerator solutions, however, can only provide a fixed ratio of computation and memory resources, which can result in resource wastage. For instance, as context lengths increase, the memory capacity needed to store the KV cache expands accordingly; with a fixed resource ratio, a substantial portion of computational resources remains underutilized when processing requests with long contexts. By pooling heterogeneous accelerators, we can adjust the number of each kind of accelerators to better match the LLM and workload and hence improve resource utilization.

The primary challenge associated with attention offloading arises from the substantial communication demands be-

<span id="page-1-1"></span>![](_page_1_Figure_7.jpeg)

**Figure Description:**
I'll do my best to extract every single piece of data from the image and format it as Question-Answer pairs.

**Figure Context:**
This image is a high-level overview of a distributed AI system, showing the flow of data through various components, including data sources, models, and training, and the use of various tools and technologies to manage and optimize the system.

**Figure Data (Q&A):**

Q: What is the name of the system being described?
A: Distributed AI System

Q: What is the purpose of the "Pretor-1" model?
A: To perform a high-temperature, high-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and-weather- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- to- and- 

[描述已截斷以避免過長]




### Model/Attention Disaggregation

#### 1. Model/Attention Disaggregation

| **Model/Attention Disaggregation** | **Model/Attention Disagagre**
| **Model/Attention Disagagre** | **Model/Attention Disagagre** |

#### 2. Model/Attention Disagagre

| **Model/Attention Disagagre** | **Model/Attention Disagagre** |
| **Model/Attention Disagagre** | **Model/Attention Disagagre** |

#### 3. Model/Attention Disagagre

#### 4. Model/Attention Disagagre

#### 5. Model/Attention Disagagre

#### 6. Model/Attention Disagagre

#### 7. Model/Attention Disagagre

#### 8. Model/Attention Disagagre

#### 9. Model/Attention Disagagre

| **Model/Attention Disagagre** | **Model/Attention Disagagre** |
| **Model/                               


However, I don't see the image in your request. Could you please provide the image or describe the content you'd like me to extract information from? I'll follow the instructions to extract and describe the content in the required format.

Figure 1: The disaggregated architecture of LLM serving.

tween heterogeneous accelerators when sending and receiving the inputs and outputs of self-attention operators. Unlike the original prefill-decode disaggregation, where the KV cache is transferred only once between the prefill nodes and the decode nodes, our model-attention disaggregation architecture requires inter-GPU communication for every layer of the model. Even worse, communication between heterogeneous GPUs must rely on *data center networks* (DCNs), such as Ethernet and InfiniBand, which provide only ~10% of the bandwidth of *inter-chip interconnects* (ICIs) like NVLink between homogeneous GPUs. If not handled properly, this frequent communication would introduce high network roundtrip times (RTTs) to the token generation latency, worsening the user experience.

To assess the practicality of our novel disaggregated architecture, we first conduct a detailed quantitative study indicating that these concerns are manageable in the context of LLM inference. In [subsection 3.1,](#page-3-0) we provide profiling and analysis to determine the minimum bandwidth threshold between different accelerator pools. Our findings reveal that 200/400Gbps DCNs, widely deployed in current AI-oriented data centers, suffice for attention offloading. However, this can only be achieved if the inter-GPU communication is carefully implemented and optimized, which is not possible for off-the-shelf communication libraries such as NCCL or Gloo.


[描述已截斷以避免過長]

# <span id="page-2-0"></span>2 Background: The Underutilization of GPUs in LLM Decoding

To comprehensively understand the challenges and limitations present in current LLM decoding implementation with homogeneous hardware, this section will provide a detailed performance analysis of LLM decoding with LLaMA3-70B model as a representative LLM. The specific notations used in this analysis are explained in Table 2.

<span id="page-2-1"></span>Table 2: Notations used in the performance analysis. The values for LLaMA3-70B are also presented.

| Parameter | Description                  | Typical Value    |
|-----------|------------------------------|------------------|
| N         | Number of parameters in LLM. | 70 billion       |
| d         | Hidden dimension.            | 8192             |
| L         | Layers of the LLM.           | 80               |
| G         | GQA group size.              | 8                |
| e         | Bytes per element.           | 2                |
| В         | Batch size.                  | 1 ~ 1024         |
| l         | Sequence length.             | $128 \sim 32768$ |

#### 2.1 Preliminaries

Modern large language models (LLMs) primarily rely on the transformer architecture [49]. In a transformer-based LLM, each input token is first mapped to a word embedding of dimension d. These embeddings then pass through a series of transformer blocks. The final output embeddings are multiplied by a sampling matrix to generate the predicted likelihoods for the next token.

Within each transformer block, the input embeddings are projected into three distinct vectors: query  $(q_i)$ , key  $(k_i)$ , and value  $(v_i)$ , all of which have the same dimension d as hidden states. These vectors are processed through an *attention operator* to compute attention scores. The attention scores are then weighted by a matrix  $W_{\text{out}}$  to produce the output embeddings  $y_i$  of the attention layer.

$$q_i = W_q x_i, \quad k_i = W_k x_i, \quad v_i = W_v x_i,$$
 $a_i = \sum_{j=1}^n \operatorname{softmax} \left(\frac{q_i^\top k_j}{\sqrt{d}}\right) v_j, \quad \star$ 
 $v_i = W_{\text{out}} a_i.$ 

The output  $y_i$  is then passed through a feedforward network that scales it into an intermediate vector space, followed by another matrix multiplication to scale it back:

$$x'_{i} = W_{\text{proj}} \cdot f_{\text{act}} (W_{\text{fc}} \cdot y_{i}).$$

Although the transformer block involves various transformations, there are actually only two kind of computationally expensive operations, which are the attention operator (denoted by \* in the equations) and the other matrix projection steps. Thus, in the following of this section, we will conduct a quantitative analysis based on the roofline model [50] and experimental measurements to evaluate these two kinds of operators. This analysis will highlight the differing characteristics of attention and non-attention operators during the decoding phase, which explains why current LLM decoding implementations with homogeneous hardware often lead to underutilization of GPUs, thus motivating the need for heterogeneous architectures.

#### 2.2 Hardware Underutilization

#### 2.2.1 The Underutilization in Non-Attention Operators

To improve GPU utilization in LLM decoding, continuous batching is widely adopted [16, 20, 46]. By processing multiple inputs concurrently, the model parameters in GPU memory can be reused, making the workload more computation-intensive. For a batch of B requests, the non-attention operator requires approximately 2NB floating-point operations. Additionally, these operators involve loading model parameters eN and reading/writing a total of 2eBd input and output data from

<span id="page-3-1"></span>![](_page_3_Figure_0.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a collection of line graphs and charts comparing the performance of various models, including L- 2, 4, 8, and 16, and their respective L- 2, 4, 8, and 16 models. The charts show the time it takes for each model to complete a task, with the x-axis representing the batch size and the y-axis representing the time in seconds.

**Figure Data (Q&A):**

Q: What is the time it takes for the 2 x NVIDIA H100 model to complete a task?
A: 0.1 seconds

Q: What is the time it takes for the 4 x NVIDIA H100 model to complete a task?
A: 0.2 seconds

Q: What is the time it takes for the 8 x NVIDIA H100 model to complete a task?
A: 0.3 seconds

Q: What is the time it takes for the 16 x NVIDIA H100 model to complete a task?
A: 0.4 seconds

Q: What is the time it takes for the 2 x NVIDIA H100 model to complete a task with a batch size of 1000?
A: 0.5 seconds

Q: What is the time it takes for the 4 x NVIDIA H100 model to complete a task with a batch size of 1000?
A: 0.6 seconds

Q: What is the time it takes for the 8 x NVIDIA H100 model to complete a task with a batch size of 1000?
A: 0.7 seconds

Q: What is the time it takes for the 16 x NVIDIA H100 model to complete a 1000 batch size?
A: 0.8 seconds

Q: What is the time it takes for the 2 x NVIDIA H100 model to complete a 1000 batch size?

Q: What is the time it takes for the 4 x NVIDIA H100 model to 1000?

Q: What is the time it takes for the 8 x NVIDIA H100 model to 1000?



[描述已截斷以避免過長]




[描述已截斷以避免過長]


[描述已截斷以避免過長]

#### 2.2.2 The Underutilization in Attention Operators

Different from the weight matrix projection operators, the attention operator, when processing a batch of requestss still performs a batched matrix-vector multiplication, where each query accesses and processes its own KV cache. As a result, the arithmetic intensity of the attention operator remains constant, irrespective of the batch size. This behavior makes attention operations memory-bound, and increasing the batch size does not improve resource utilization. More recent models have adopt *grouped-query attention* (GQA), which splits  $q_i$  into a group of G independent queries and reduce the size of  $k_i$  and  $v_i$  by a factor of G. Each query goes through the attention computation with the same  $k_i$  and  $v_i$  and the outputs are simply concatenated. With GQA, the arithmetic intensity of attention operators is increased G times, but is still quite low compared with other operators.

As shown in Figure 3, the bandwidth utilization of attention operators remains above 70% even for small batch sizes, such as 20. This holds true even on memory-specialized accelerators like H20, which delivers only 15% of the TFLOPs of the H100. However, the batch size achievable for attention operations is constrained by GPU memory capacity, particularly due to the high memory demand of KV caches for longer

<span id="page-3-2"></span>![](_page_3_Figure_7.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a collection of charts and tables comparing the performance of various AI models, including L-  , L-  , and L-  , with L-  , L-  , and L-  . The charts show the performance of these models in various tasks, including  ,  , and  .

**Figure Data (Q&A):**

Q: What is the batch size for L-  ?

Q: What is the batch size for L-




Unfortunately, the image does not contain a table. If it did, I would follow the instructions to convert it to a Markdown table format.

**Chart/PLOT PROCESSING:**

The image contains multiple plots, but I'll describe the content of each plot:

1. **Plot 1:**
   - X-axis: Batch size
   - Y-axis: Time (s)
   - Data points:
     - Label: 4096 @ NVIDIA H100: 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.8, 0.9, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30, 0.31, 0.32, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 4. 5. 6. 7. 8. 9. 10. 11. 12. 

Figure 3: Measured time consumption and model bandwidth utilization (MBU) of attention operators in LLaMA3-70B during one decode iteration. Results with different sequence lengths and hardware configurations are presented.

context lengths. For example, with a context length of 8192, the full memory of an H100 can only hold KV caches for about 30 requests, with the actual number being lower due to memory used by model weights.

[描述已截斷以避免過長]

#### 3 Model-Attention Disaggregation

#### <span id="page-3-0"></span>3.1 Overview

Current LLM serving systems often employ the same hardware for both attention and non-attention operators during the decode phase. However, our analysis reveals that this homogeneous approach leads to suboptimal resource utilization for both types of operators, due to the following reasons:

Attention operators demonstrate low arithmetic intensity, as each value retrieved from the KV cache participates in only a limited number of computations. Given

the disparity between memory bandwidth and computing power in modern high-performance accelerators, which favor high arithmetic intensity for efficient resource utilization, these operators tend to underutilize the computation resources of advanced GPUs.

• For non-attention operators, while increasing the batch size could potentially enhance hardware utilization, this also results in a corresponding increase in the KV cache, which may exceed the available memory capacity. Consequently, to prevent memory overflow, the batch size is often kept small, which also leads to inefficient hardware utilization because of low arithmetic intensity.

To address the above limitations of homogeneous decoding solutions, we propose the model-attention disaggregation architecture, which uses memory-specialized accelerators to store KV caches and compute the attention operators; the nonattention operators are still executed on original accelerators. A model-attention disaggregation system can use multiple devices of each kind to provide different *degrees of parallelism* (DOPs). If we use *a* GPUs for non-attention operators and *b* memory-optimized GPUs for attention operators, we denote the DOP as (*a*,*b*).

By leveraging the cheaper memory-optimized devices, we can make larger batch sizes due to the extended memory capacities to store the KV caches, hence increasing the arithmetic intensity and promoting the hardware utilization of non-attention operators. Moreover, as the attention computation are moved to memory-optimized devices, we avoid wasting precious computation resources of high-end GPUs.

One potential obstacle in implementing attention offloading lies in the necessity of data transmission between heterogeneous accelerators for each layer of the model, which could encounter the communication wall problem and increase the end-to-end decoding latency. We conduct a quantitative analysis to determine the required interconnect bandwidth for such transfers. Say we run one iteration with batch size *B*, and we can afford α× more latency for the networking overhead, the minimum interconnect bandwidth required can thus be calculated as

$$\begin{aligned} \text{minimum bandwidth} &= \frac{\text{size of data to transmit}}{\alpha \cdot \text{computation time}} \\ &= \frac{(2 + 2/G)edBL}{\alpha[\text{MTIME}(B) + \text{ATIME}(B, l)]} \end{aligned}$$

where MTIME(*B*) and ATIME(*B*,*l*) is running time of nonattention and attention operators at batch size *B* and sequence length *l*, respectively, and they can be measured experimentally. The estimated minimum bandwidths required for different batch sizes, when α = 0.2, are calculated and presented in [Figure 4.](#page-4-0)

As evident from the data presented, the required interconnect bandwidth does not exceed 30 GB/s, even when dealing

<span id="page-4-0"></span>![](_page_4_Figure_8.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a comparison of the performance of two models, L- 2.5B and L- 2.5B- 2.5B, on various tasks, including image- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2. 5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2. 5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 2.5B- 




[描述已截斷以避免過長]


| **Batch size** | **Bandwidth (GB/s)** |
| --- | --- |
| 1 | 1.5 |
| 2 | 3.0 |
| 4 | 6.0 |
| 8 | 12.0 |
| 16 | 24.0 |

**Table 2:**

**Figure 1:**

*   **X-axis:** Batch size
*   **Y-axis:** Bandwidth (GB/s)
*   **Data points:**
    *   (1, 1.5)
    *   (2, 3.0)
    *   (4, 6.0)
    *   (8, 12.0)
    *   (16, 24.0)

**Figure 2:**

**Figure 3:**

*   **X-axis:** Batch size
*   **Y-axis:** Band
*   **Data points:**
    *   (1, 1.5)
    *   (2, 3.0)
    *   (4, 6.0)
    *   (8, 12.0)
    *   (16, 24.0)

**Figure 4:**

*   **X-axis:** Batch
*   **Y-axis:** Band
*   **Data points:**
    *   (1, 1.5)
    *   (2, 3.0)
   
*   **X-axis:** Batch
*   **Y-axis:** Band
*   **Data points:**
    *   (1, 1.5)
    *   (2, 3.0)
    *   (4, 6.0)
    *   (8, 12. 0)
    *   (16, 24. 0)

**Figure 5:**

*   **X-axis:** Batch
*   **Y-axis:** Band
*   **Data points:**
    *   (1, 1.5)
    *   (2, 3. 0)
    *   (4, 6. 0)
    *   (8, 12. 0)
    *   (16, 24. 0)

**Figure 6:**

*   **X-axis:** Batch
*   **Y-axis:** Band
*   **Data points:**
    *   (1, 1. 5)
    *   (2, 3. 0)
    *   (4. 0)
    *   (8. 0)
    *   (16. 0)

**Figure 7:**

*   **X-axis:** Batch
*   **Y-axis:** Band
*   **Data points:**
    *   (1. 5)
    *   (2. 0)

Figure 4: The required network bandwidth for decoding LLaMA3-70B using attention offloading with H100 and H20, with at most 20% latency slow-down for network overhead.

with batch sizes as high as 300. This bandwidth demand can be easily met by networking technologies like 400Gbps Ethernet. Indeed, contemporary data centers already fulfill this requirement, where each GPU is typically equipped with an exclusive 400Gbps NIC to provide sufficient networking bandwidth for LLM training.


[描述已截斷以避免過長]

## 3.2 Practical Challenges

While model-attention disaggregation promises potential benefits in improving LLM decoding efficiency, it also introduces a set of formidable practical challenges. We discuss some of these challenges below.

Frequent network communications. By separating the attention operator from computation-optimized devices to memory-optimized devices, we introduce cross-machine data communications within each model layer. Even though the interconnect bandwidth in existing data centers is sufficient for attention offloading, we found that networking latency might still be a problem for efficient LLM decoding. With attention offloading, we have layer-wise data transfer between GPUs on different nodes, which may be up to thousands round-trips per second. These frequent network transfers might significantly increase the decoding time due to the accumulated network latencies. Hence, we need a refurnished, latency-optimized, GPU-aware networking stack for optimal performance of model-attention disaggregation.

Software engineering challenges. With model-attention disaggregation, we are moving the execution of *attention operator*, an intermediate operation of the transformer block, to other devices. This requires complicated and destructive

modifications to the existing LLM codebase. Specifically, we have to dissect the models into separate slices that do not align with the modular structure of the transformer-based LLMs. This process is not only labor-intensive and error-prone but also significantly increases maintenance complexity. Hence, automated tools to help slice the models and perform relevant optimizations are highly desirable.

Difficult execution overlapping. In a heterogeneous disaggregated system, various devices such as compute-optimized GPUs, memory-optimized GPUs, and NICs can be utilized simultaneously. Hence, we might achieve significant execution time reduction if the execution of operations occupying different devices could be overlapped. However, in the transformers architectures of current LLMs, attention operators and model operators are tightly interleaved in a sequential manner, with the output of one operator being transmitted over the network for the input of the other. Consequently, operations that depend on distinct hardware resources cannot be effectively overlapped in time, leading to considerable resource underutilization. Therefore, careful orchestration of operations on various devices and efficient design of task pipelines are required to promote execution overlapping and increase resource utilization.

## 4 System Design

We build Lamina, a distributed heterogeneous LLM decoding system that implements model-attention disaggregation and solves the related challenges. Lamina employs two kinds of acceleration devices: memory devices are used for storing KV cache and computing the attention operator, and computation devices are used for storing model parameters and computing other parts of the model. These two kinds of devices are interconnected with high-speed DCN, e.g., Infiniband or Ethernet.

## 4.1 Fully Host-Bypassed Network Stack

The communication between GPUs across different nodes, often utilizing RDMA technologies, is a complex process that requires the coordination of multiple system agents, including the CPU, GPU, and NIC. To reduce GPU-aware networking overhead, GPUDirect RDMA (GDR) [\[3\]](#page-12-6) is developed to allow the RDMA-capable NIC (RNIC) to directly access GPU memory. This eliminates the need for host memory as an intermediate buffer, thereby enhancing both network latency and bandwidth. However, the control path still requires CPU involvement and includes several steps, all of which lie on the critical path and contribute to network latency. Specifically, when transferring data using GPUDirect RDMA, the following steps are performed:

<span id="page-5-0"></span>1. The local CPU waits for all prior GPU kernels to complete, ensuring the data to be transmitted is ready.

- 2. The local CPU submits a send *work request* (WR) to the RNIC.
- 3. The local RNIC processes the send WR, fetching the data from GPU memory and transmitting it over the physical network link.
- 4. The remote RNIC receives the data and writes it to the GPU memory.
- <span id="page-5-1"></span>5. The remote CPU waits for the RDMA receive operation to complete.
- 6. The remote CPU launches the subsequent GPU kernels.

Based on our experimental results, steps [1](#page-5-0) through [5](#page-5-1) may incur a latency of 60–70 µs. Furthermore, because we have to launch the kernel after the received data is ready, the GPU kernel launch overhead, which might be up to 20 µs, is also added to end-to-end latency. All these additional latencies pose a significant overhead for model-attention disaggregation, which must rely on frequent network communications.

To reduce such networking overhead, we develop a *fully host-bypassed network* (FHBN) stack, which completely eliminates host CPU involvement in both control and data paths of GPU-aware networking. We describe how FHBN performs send and recv operations below.

FHBN recv. To implement the FHBN recv function, we employ the *device-side polling* technique to await the completion of the recv operation. Specifically, we allocate a seqno variable on the receiver's GPU memory. The sender increments the remote seqno with RDMA write after each send operation. The data send and seqno increment operations are batched in a single WR post and hence would not increase the end-to-end latency. When the receiver GPU is ready to receive and process the incoming data, it actively polls the value of seqno with a specialized GPU kernel. This approach not only eliminates the need for CPU involvement during the recv process, but also allows asynchronous launch of the polling kernel and subsequent computation kernels to the GPU stream. Therefore, the GPU kernel launch overhead is also removed from the critical path.

FHBN send. The implementation of FHBN send, illustrated in [Figure 5,](#page-6-0) is more involved as it necessitates the GPU to directly submit RDMA commands to RNIC. When the CPU submits a new RDMA WR to RNIC, it first enqueues the WR to the *work queue* (WQ) in the host memory. Then, it tells the RNIC that there is outstanding work by ringing the *doorbell* (DB), a special register in the *user access region* (UAR) of the RNIC. The UAR is part of the RNIC's mmio region and is mapped to the address space of unprivileged applications to allow kernel-bypass RDMA operations. All above steps are implemented in the RDMA userspace library (libibverbs).

To enable direct RDMA command submission on GPUs, we have to allow GPUs to directly access the UAR via PCIe P2P. Specifically, we use the cudaHostRegisterIoMemory API to map the UAR into the GPU's address space. Then,

<span id="page-6-0"></span>![](_page_6_Picture_0.jpeg)

**Figure Description:**
I'll do my best to extract every single piece of data from the image and format it as Question-Answer pairs.

**Figure Context:**
This image is a high-level overview of a system architecture, showing the flow of data and communication between various components.

**Figure Data (Q&A):**

Q: What is the name of the system architecture?
A: System Architecture

Q: What is the role of the Host CPU?
A: It is the central processing unit of the system

Q: What is the role of the Host DRAM?
A: It is the memory of the system

Q: What is the role of the User Access Region?
A: It is the user-   

Q: What is the role of the GPU?
A: It is the   

Q: What is the role of the Host CPU?
A: It is the   

Q: What is the role of the Host DRAM?
A: It is the   

Q: What is the role of the User Access Region?
A: It is the   

Q: What is the role of the User   

Q: What is the role of the   

Q: What is the   



### CPU Core
* **CPU Core**: `ibv_post_send (libibv)
* **CPU Core**: `cud

**Host DRAM**
```markdown
### Host DRAM
| **Host DRAM** | **Value** |
| **-----------** | **--------**
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0
| **WQ**        | 0
| **CQ**        | 0


Figure 5: Diagram of WR submission with FHBN send and conventional GPU-aware send implementations.

we reimplement the RDMA command submission logic in CUDA device code. To further decrease latency, we leverage the BlueFlame mechanism, a hardware feature provided by Mellanox RNICs [4]. This approach allows the WR to be directly submitted to the RNIC with mmio write to UAR, eliminating the need for the RNIC to fetch the WR from host memory via an expensive PCIe DMA read. Note that the WR should still be enqueued into the WQ, as the hardware may occasionally miss the BlueFlame WR and fall back to the regular workflow, particularly under heavy loads.

#### 4.2 Automated Model Converter

#### 4.2.1 Model Splitting

In the attention offloading architecture, different operators of the LLM might be executed on different hardware; hence, we need to partition the model into slices, which is achieved by cutting at the attention operators. It often involves significant modifications to the existing codebase, primarily because the desired cutting points do not align with the LLM's inherent modular structure. This misalignment complicates the partitioning process and increases the risk of errors and inconsistencies within the heterogeneous system.

To facilitate model partitioning, we develop an automated model splitter capable of transforming the LLM into individually invokable slices, illustrated in Figure 6. Given the LLM source code, the splitter uses symbolic execution to generate a weighted computation graph. The weight of each edge denotes the size of the data passed between the operators, which is derived from the model's shape specification.

Due to the presence of residual connections and other intricate model constructs, directly removing the attention operator does not always result in a disconnected computation graph. Therefore, we compute the *minimum weighted cut* of

<span id="page-6-1"></span>![](_page_6_Figure_8.jpeg)

**Figure Description:**
**Figure Context:**
This image is a flowchart illustrating the architecture of a deep learning model, specifically the GSh
**Figure Data (Q&A):**

Q: What is the input size of the model?

Q: What is the size of the L

Q: What is the




Figure 6: The partitioned computation graph of an LLM.

the remaining graph, from the input to the output of the attention operator. The edges in this minimum cut, representing the context that must be saved between slice invocations, are removed from the computation graphs. This process is iteratively applied to each attention operator, ultimately yielding n+1 model slices, where n denotes the original number of the attention operators.

#### <span id="page-6-2"></span>4.2.2 Resource Utilization Overlapping

While the attention operators and other operators in a transformer block are executed sequentially, a closer examination of the attention computation reveals the potential for achieving partial overlapping of resource utilization. Given an attention query q and the set of token indices I, the attention computation can be carried out in a divide-and-conquer manner. Assume that I can be written as the disjoint union of two subsets  $I_1$  and  $I_2$ , and let

$$\begin{split} A_q(I) &= \sum_{i \in I} \operatorname{softmax} \left( \frac{q^\top k_i}{\sqrt{d}} \right) v_i, \\ S_q(I) &= \sum_{i \in I} \exp \left( \frac{q^\top k_i}{\sqrt{d}} \right), \end{split}$$

where  $A_q(I)$  is the attention output and  $S_q(I)$  is the denominator of softmax, then  $A_q(I)$  can be easily obtained by combining the partial attention results on  $I_1$  and  $I_2$ , i.e.,  $[A_q(I_1), S_q(I_1)]$  and  $[A_q(I_2), S_q(I_2)]$ :

$$A_q(I) = \frac{A_q(I_1)S_q(I_1) + A_q(I_2)S_q(I_2)}{S_q(I_1) + S_q(I_2)}.$$

During LLM decoding, we may divide the current token set into two partitions during attention computation: all previous tokens (prev) and the newly generated token (new). Note that  $[A_q(\text{prev}), S_q(\text{prev})]$  can be computed as soon as  $q_n$  is ready; therefore, we may eagerly execute Q-Proj and transfer  $q_n$ , and then execute K-Proj, V-Proj and transfer  $k_n, v_n$  to the attention

workers. As illustrated in Figure 7, this does not only improve the GPU utilization on both kinds of workers, but also reduces the end-to-end latency by hiding the communication behind the computation.

<span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)

**Figure Description:**
I'll do my best to extract the data from the image and format it as Question-Answer pairs.

**Figure Context:**
This image is a flowchart illustrating the architecture of a deep learning model, specifically the L- 3- 2- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 




![](_page_7_Figure_3.jpeg)

**Figure Description:**
I'll do my best to extract every single piece of data from the image and format it as Question-Answer pairs.

**Figure Context:**
This image is about the architecture and performance of a large-scale AI model, specifically L- 3- 2- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1-  1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 1- 




[描述已截斷以避免過長]


Figure 7: Illustration of resource utilization overlapping by splitting the attention computation.

The above attention splitting optimization is integrated in our automated model converter. After dissecting the original model, the converter will generate a serial program of each model slice by computing a topological order of its computation graph. During this topological sort, we always put the Q-Proj operator and all its dependencies as early as possible. Then, we insert the "send Q" instruction immediately after the Q-Proj operator and "send KV" at the end of this slice.

#### 4.3 Execution Pipelining

Due to the serial nature of transformer-based models, if there is only one batch under processing, the memory device is idle when the computation device is working, and vice versa. To address this resource underutilization problem and increase system throughput, we may run multiple batches concurrently in a pipelined fashion. With properly designed pipelining, better hardware utilization can be achieved without sacrificing latency. We propose the rotational staggered pipelining to solve this problem.

<span id="page-7-1"></span>![](_page_7_Figure_9.jpeg)

**Figure Description:**
I'll do my best to extract the data from the image and format it as Question-Answer pairs.

**Figure Context:**
This image is a comparison of three language models: LLa-1, LLa-2, and LLa-3. It shows the model sizes, datasets, and performance, and provides a comparison of the three models.

**Figure Data (Q&A):**

Q: What is the model size of LLa-1?

Q: How many parameters does L-1 have?

Q: What is the model size of L-2?

Q: What is the model size of L-3?

Q: What is the model size of L-4?

Q: What is the model size of L-5?

Q: What is the model size of L-6?

Q: What is the model size of L-7?

Q: What is the model size of L-8?

Q: What is the model size of L-9?

Q: What is the model size of L-10?

Q: What is the model size of L-11

Q: What is the model size of L-12

Q: What is the model size of L-13

Q: What is the model size of L-4

Q: What is the model size of L-5

Q: What is the model size of L-6

Q: What is the model size of L-7

Q: What is the model size of L-8

Q: What is the model size of L-9

Q: What is the model size of L-10

Q: What is the model

Q: What is the




[描述已截斷以避免過長]


Figure 8: Illustration of rotational staggered pipelining.

Assume that we execute n batches concurrently. Let  $t_m, t_a$  represent the time required for executing one model slice and one attention operator, respectively. As illustrated in Figure 8, we deploy n-1 model replicas, with each replica starting its

tasks at a time of  $\frac{t_m}{n-1}$  later than the previous one. All batches share a common set of memory devices to maximize aggregated memory bandwidth and improve memory utilization. For every batch, the KV cache is evenly partitioned across these devices. All memory devices jointly compute the attention operator for a single batch. The number of memory devices is selected to make  $t_a = \frac{t_m}{n-1}$ . After the attention operator, each batch transitions to the next model replica according to a rotational schedule; that is, the kth model slice of the jth batch is executed on replica (j+k) mod (n-1)+1.

This rotational task scheduling, combined with the staggered execution intervals, guarantees seamless task transitions for each batch and ensures a conflict- and bubble-free workflow on each device. Furthermore, by increasing the number of concurrent batches, the overall inference latency can be reduced due to the decreased attention computation time. However, the rotational scheduling requires migrating batch execution contexts between computation devices. Note that when n = 2, the context migration is unnecessary because both batches are executed within a single model replica.

#### 5 Implementation

Lamina is implemented with ~6000 lines of Python and C/C++ code, in addition to a few lines of CUDA code implementing custom kernels. The fully host-bypassed network stack is built on top of a modified version of rdma-core [6]. Lamina uses Ray [5] to facilitate task scheduling and worker placement in distributed heterogeneous environments.

**Fault tolerance.** With attention-offloading, we have two different types of accelerators. Lamina addresses faults in these two types of accelerators with different approaches. Note that all request states, i.e., the KV caches, are only stored in the attention devices. Consequently, should any model worker experience a failure, we can seamlessly replace that worker with a functioning one, without losing any progresses. In case of an attention worker failure, we reconstruct the KV cache by using the prompt texts and already generated tokens, which are stored in the LLM service front-end.

Handling the prefill-decode transition. During the prefill phase, the generated KV cache shall be transmitted to the attention workers for decoding. For each request, the global scheduler picks a set of model workers and attention workers to handle the decode phase. Like previous works [40,59], the KV cache is asynchronously transferred in a layer-by-layer fashion to hide the communication latency behind computation. Moreover, the data transfer is controlled by the attention workers: the attention workers only reads the KV cache from prefill workers during the free periods between receiving QKV tensors from model workers. This approach minimizes interference with ongoing decoding tasks.

**Attention parallelism.** Given the limited capability of a single device, we may use multiple memory devices to jointly

<span id="page-8-0"></span>![](_page_8_Figure_0.jpeg)

**Figure Description:**
I'll do my best to extract the data from the image and format it as Question-Answer pairs.

**Figure Context:**
This image is a comparison of the performance of different models on various tasks, including request-level and head-level partitioning.

**Figure Data (Q&A):**

Q: What is the request-level partitioning model?

Q: What is the request-...




Note: The table only shows the resources for each request, but the actual data is not provided. The table is based on the text of the image, which is not provided.


### Request- and Head-Level Partitioning

| Request | Request- and Head-Level Partitioning |
| 1 | Request 1: 1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:1:


However, I don't see the image. Could you please provide the image or describe the content you'd like me to extract information from? I'll follow the instructions to extract and describe the content in the specified format.

Figure 9: Work partition methods of the attention operator.

store the KV caches and compute the attention operators. As depicted in [Figure 9,](#page-8-0) the attention operators can be parallelized among memory devices in various ways. One method is to distribute different requests across different devices; an alternative strategy is to partition and distribute the attention heads, which can also be computed independently, to different devices. The head-level partitioning approach ensures a balanced workload distribution, whereas the request-level partitioning may result in load imbalance due to the differences in sequence lengths and therefore the KV cache sizes among requests. However, head-level partitioning has limited flexibility, as it requires the number of memory devices to be divisible by the number of attention heads. We opt for head-level partitioning in Lamina, which offers optimal load balancing.

## 6 Evaluation

Testbed. We deploy Lamina on a real heterogeneous cluster with two kinds of GPU nodes. Each node consists of either eight H100 or H20 GPUs, and each GPU is paired with a dedicated ConnectX-7 NIC via PCIe switch. The GPU nodes are interconnected with 400 Gbps RoCE network. We use H100 as compute-optimized GPUs and H20 as memory-optimized GPUs for Lamina.

Models. Lamina supports a wide variety of LLM architectures, including OPT [\[58\]](#page-15-2), LLaMA [\[48\]](#page-14-5), and LLaMA3 [\[9\]](#page-12-10). All these architectures have similar outlines and workload characteristics and only have minor differences irrelevant to system designs. Hence, as listed in [Table 3,](#page-8-1) we choose LLaMA-33B, LLaMA-65B, and LLaMA3-70B for evaluations. All model parameters and KV caches are stored in FP16 format.

Workloads To mirror the real-world LLM use cases, we use four request traces collected from the production systems of two LLM service providers, Azure [\[1,](#page-12-11) [40\]](#page-14-0) and Kimi [\[41\]](#page-14-1). Due to data protection regulations, these traces only contain the sequence length of user requests but not the actual contents. Hence, we use requests of dummy tokens with the same sequence length for evaluation. The summaries of these

<span id="page-8-1"></span>Table 3: Large language models used for evaluation.

| Model      | Parameters | L  | d    | G |
|------------|------------|----|------|---|
| LLaMA-33B  | 64.7 GB    | 60 | 6656 | 1 |
| LLaMA-65B  | 130.1 GB   | 80 | 8192 | 1 |
| LLaMA3-70B | 137.5 GB   | 80 | 8192 | 8 |

<span id="page-8-2"></span>traces, including the average prompt tokens (*lp*) and average generated tokens (*lg*), are listed in [Table 4.](#page-8-2)

Table 4: Request traces used for evaluation.

| Trace      | # Requests | lp      | lg    |
|------------|------------|---------|-------|
| Azure-Conv | 19366      | 1154.7  | 211.1 |
| Azure-Code | 8819       | 2047.8  | 27.9  |
| Kimi-Conv  | 12031      | 12035.1 | 342.6 |
| Kimi-TA    | 23608      | 8560.0  | 182.1 |

Baseline system. We compare with vLLM [\[28\]](#page-13-1), a state-ofthe-art LLM serving system optimized for high throughput. vLLM also integrates optimizations from other LLM inference systems, such as continuous batching from Orca [\[57\]](#page-15-3). We use vLLM with homogeneous H100 GPUs and use tensor parallel for multi-GPU inference. As Lamina only focuses on the decode phase, we modify vLLM to remove the prefill phase during evaluation for a fair comparison.

## 6.1 Serving Performance

We evaluate the serving performance of Lamina against vLLM using real-world request traces. We first use homogeneous and heterogeneous hardware settings of similar costs, listed in [Table 5,](#page-8-3) for vLLM and Lamina, respectively. Compared with vLLM, Lamina replaces half of the H100 devices to H20, which is cheaper but provides more memory capacity and bandwidth. We measure the token generation throughput, time between tokens (TBT), and average batch size.

<span id="page-8-3"></span>Table 5: Equal-cost hardware configurations for evaluation.

| Model                 | Lamina       | vLLM         |
|-----------------------|--------------|--------------|
|                       | DOP=(1,2)    | 2×H100       |
| LLaMA-33B             | (\$20.32/hr) | (\$22.12/hr) |
|                       | DOP=(2,4)    | 4×H100       |
| LLaMA-65B, LLaMA3-70B | (\$40.64/hr) | (\$44.24/hr) |

As illustrated in [Figure 10,](#page-9-0) Lamina consistently achieves 16.1 ∼ 90.1% higher throughput than vLLM among all models and traces, given comparable hardware costs. This enhancement is primarily attributed to the larger batch size attained by Lamina, which is 2.39× of vLLM on average. These

<span id="page-9-0"></span>![](_page_9_Figure_0.jpeg)

**Figure Description:**
**Figure Context:**
This image is a collection of bar charts and tables comparing the performance of various AI models, including LLa
**Figure Data (Q&A):**

Q: What is the size of the LLa
Q: How many parameters does L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L
Q: What is the size of the L


[描述已截斷以避免過長]



### Azure-Conv
| Model | Throughput (Tokens/s) |
| --- | --- |
| Lamina | 1200 |
| VLLM | 800 |

### Azure-Code
| Model | Throughput (Tokens/s) |
| --- | --- |
| Lamina | 1000 |
| VLLM | 600 |

### Kimi-Conv
| Model | Throughput (Tokens/s) |
| --- | --- |
| Lamina | 900 |
| VLLM | 500 |

### Kimi-TA
| Model | Throughput (Tokens/s) |
| --- | --- |
| Lamina | 800 |
| VLLM | 400 |

**TBT (ms)**
### Azure-Conv
| Model | TBT (ms) |
| --- | --- |
| Lamina | 50 |
| VLLM | 100 |

### Azure-Code
| Model | TBT (ms) |
| --- | --- |
| Lamina | 50 |
| VLLM | 100 |

### Kimi-Conv
| Model | TBT (ms) |
| --- | --- |
| Lamina | 50 |
| VLLM | 100 |

### Kimi-TA
| Model | TBT (ms) |
| --- | — |
| Lamina | 50 |
| VLLM | 100 |

**TBT (ms)**
### Azure-Conv
| Model | TBT (ms) |
| Lamina | 50 |
| VLLM | 100 |

### Azure-Code
| Model | TBT (ms) |
| Lamina | 50 |
| VLLM | 100 |

### Kimi-Conv
| Model | TBT (ms) |
| Lamina | 50 |
| VLLM | 100 |

### Kimi-TA
| Model | TBT (ms) |
| Lamina | 50 |
| VLLM | 100 |

### Azure-Code
| Model | TBT (ms) |
| Lamina | 50
| VLLM | 100 |

### Kimi-Conv
| Model | TBT (ms) |
| Lamina | 50
| VLLM | 100 |

### Kimi-TA
| Model | TBT (ms) |
| Lamina | 50
| VLLM | 100 |

**TBT (ms)**
### Azure-Conv
| Model | TBT (ms) |


The image appears to be a collection of multiple charts and graphs, each with its own set of data points and labels. The charts are organized in a grid-like structure, with each chart having its own set of labels and data points.

**Data Points:**

The data points in the charts are not explicitly labeled, but they appear to be related to various topics such as "Throughput (token/s)", "TBT (ms)", and "Mean batch size". The data points are not explicitly labeled, but they appear to be related to various topics.

**Chart Descriptions:**

The charts appear to be related to various topics, including:

* Throughput (token/s)
* TBT (ms)
* Mean batch size

The data points in the charts are not explicitly labeled, but they appear to be related to various topics, including:

* Throughput (token/s)
* TBT (ms)
* Mean batch

Figure 10: LLM decoding performance metrics of Lamina and vLLM, using hardware of approximately equal costs.

<span id="page-9-1"></span>![](_page_9_Figure_2.jpeg)

**Figure Description:**
**Figure Context:**
This image is a collection of charts and tables comparing the performance and carbon emissions of various AI models, including LLaMA, GShard, and others. The data is used to compare the performance, carbon emissions, and energy consumption of these models.

**Figure Data (Q&A):**

Q: What is the size of the LLaMA 65B model?

Q: How many parameters does LLaMA 65B have?

Q: What is the size of the LLaMA 33B model?
Q: How many parameters does LLaMA 33B have?

Q: What is the size of the GSh
Q: How many G

Q: What is the size of the G
Q: How many G

Q: What is the size of the G




If the image contains tables, I would:

1. Convert the table to Markdown table format.
2. Extract all cell values exactly as shown.
3. Maintain the table structure with proper headers and rows.
4. Use a table format with proper headers and rows.

**Chart/PLOT Extraction**

If the image contains charts or plots, I would:

1. List every visible data point in the format "Label: Value".
2. Extract specific numbers, percentages, and units.
3. Describe the X-axis and Y-axis labels and units.
4. If there is a legend, describe what each color/symbol represents.

**Diagrams and Text

If the image contains diagrams or text, I would:

1. Describe the flow, structure, and all visible labels.
2. Extract all text labels
3. Describe the relationships and

**Mathematical Formulas**

If there are mathematical formulas, I would:


**Output Format**

I will follow the output format guidelines:


**Critical Rules**

I will follow the critical rules:


**Table 1: LLaMA- 1.2B, LLaMA- 13B, LLaMA- 1.2B, LLaMA- 13B, LLaMA- 1.2B, LLaMA- 13B**

| Model | Throughput (tokens/s) | Price ($/hr) |
| : | : | : |
| LLaMA- 1.2B, LLaMA- 13B | 2,000 | 100 |
| LLaMA- 1.2B, LLaMA- 13B | 2,000 | 100 |

**Table 2: LLaMA- 1.2B, LLaMA- 13B, LLaMA- 1.2B, LLaMA- 13B, LLaMA- 1.2B, LLaMA- 13B**

| Model | Throughput (tokens/s) | Price ($/hr) |
| : | : | : |
| LLaMA- 1.2B, LLaMA- 13B | 2,000 | 100 |
| LLaMA- 1.2B, LLaMA- 13B | 2,000 | 100 |
| LLaMA- 1.2B, LLaMA- 13B | 2, 000 | 100 |
| LLaMA- 1.2B, LLaMA- 13B | 2, 000 | 100 |

**Table 3: LLaMA- 1.2B, LLaMA- 13B, LLaMA- 1.2B, LLaMA- 13B, LLaMA- 1.2B, LLaMA- 13B**

| Model | Through- 1.2B, LLa- 1.2B, LLa- 1.2B, L- 1.2B, L- 1.2B, L- 1.2B, L- 1.2B, L- 1.2B, L- 1.2B, L- 


[描述已截斷以避免過長]


Figure 12: Token generation latency breakdown.

#### 6.2 Latency Breakdown

Latency is a crucial indicator of the service quality offered by LLM applications. In this subsection, we measure the time between tokens (TBT) across various system configurations, as well as the execution time for model and attention workers and the networking overhead. We use requests with fixed sequence lengths (4096 or 8192) as the workload and disable rotational pipelining to better reveal the time breakdown.

As we can see from Figure 12, for smaller batch sizes, the model execution time dominates the token generation latency. The attention and networking latency rapidly increases for larger batch sizes, while the model execution time remains almost constant. This indicates that the computation resource utilization gets improved as batch size increases. Note that the observed TBT might be less than the sum of model worker time, attention worker time, and network time. This is due to the automated resource utilization overlapping optimization, which will be further profiled in subsection 6.4.

## 6.3 Network Stack Optimizations

We evaluate the effectiveness of our fully host-bypassed network (FHBN) stack with a microbenchmark. Specifically, we conduct a ping-pong test between two GPUs located on distinct nodes, using NCCL, NCCL without GPUDirect RDMA, Gloo, and FHBN as the networking engine. The initiator GPU sends a varying amount of data to the remote GPU. Upon receiving the complete data, the remote GPU immediately sends it back to the initiator. We measure the round-trip time from the initiator GPU's perspective, which encompasses the time interval from the completion of the kernel that generates the data for transmission to the start of the kernel that consumes

the received data.

<span id="page-10-1"></span>![](_page_10_Figure_8.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a comparison of various AI models' performance, including L-  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  




Note: The data is not provided in the original text, so I couldn't fill in the table. The actual data would depend on the specific data points and values for each protocol.


### (a) Round-trip time

|  | 10^2 | 10^4 | 10^6 | 10^8 |
| --- | --- | --- | --- | --- |
| **Gloo** | 100 | 100 | 100 | 100 |
| **NCCL (w/o GDR)** | 100 | 100 | 100 | 100 |
| **NCCL** | 100 | 100 | 100 | 100 |

### (b) Bandwidth utilization

### (c) Bandwidth utilization

|  | 10^2 | 10^4 | 10^6 | 10^8 |
| --- | --- | --- | — | — |
| **Gloo** | 100 | 100 | 100 | 100 |
| **NCCL (w/o GDR)** | 100 | 100 | 100 | 100 |
| **NCCL** | 100 | 100 | 100 | 100 |

### (d) Bandwidth utilization

|  | 10^2 | 10^4 | 10^6 | 10^8 |
| — | — | — | — | — |
| **Gloo** | 100 | 100 | 100 | 100 |
| **NCCL (w/o GDR)** | 100 | 100 | 100 | 100 |
| **NCCL** | 100 | 100 | 100 | 100 |

### (e) Band
|  | 10^2 | 10^4 | 10^6 | 10^8 |
| — | — | — | — | — |
| **Gloo** | 100 | 100 | 100 | 100 |
| **NCCL (w/o GDR)** | 100 | 100 | 100 | 100 |
| **NCCL** | 100 | 100 | 100 | 100 |

### (f) Band
|  | 10^2 | 10^4 | 10^6 | 10^8 |
| — | — | — | — | — |
| **Gloo** | 100 | 100 | 100 | 100 |
| **NCCL (w/o GDR)** | 100 | 100 | 100 | 100 |
| **NCCL** | 100 | 100 | 100 | 100 |

### (g) Band
|  | 10^2 | 100 | 100 | 100 |
| — | — | — | — | — |
| **Goo** | 100 | 100 | 100 | 100 |
| **NCCL (w/o GDR)** | 100 | 100 | 100 | 100 |
| **NCCL** | 100 | 100 | 100 | 100 |

### (h) Band
|  | 10^2 | 100 | 100 | 100 |
| — | — | — | — | — |
| **Goo** | 100 | 100 | 100 | 100 |
| **NCCL (w/o GDR)** | 100 | 100 | 100 | 100 |
| **NCCL** | 100 | 100 | 100 |


| **Gloo** | **NCCL (without GDR)** | **NCCL (with GDR)** |
| **(μs)** | **(μs)** | **(μs)** |
|  |  |  |

**Table 2:**

**Table 3:**

| **Glo** | **NCCL (without GDR)** | **NCCL (with GDR)** |
| **(μs)** | **(μs)** | **(μs)** |
|  |  |  |

**Figure 1:**

*   **X-axis:** Payload size (bytes)
*   **Y-axis:** Round-trip time (μs)
*   **Data points:**
    *   (1000, 1000)
    *   (1000, 100
   

Figure 13: Network ping-pong test between two GPUs on different nodes, interconnected with 400Gbps RoCE.

As illustrated in Figure 13, for smaller data sizes, the round-trip time is primarily determined by network latency. In this case, FHBN achieves an end-to-end latency of  $33.0\,\mu s$ , representing a 50.5% reduction compared to NCCL's  $66.6\,\mu s$  latency. This improvement is attributed to the removal of host CPU involvement in data transmission, eliminating expensive host-device synchronization and PCIe transactions. This improvement justifies the efficacy of our fully host-bypassed network stack design.

For larger payload sizes, the primary factor influencing networking time is the utilization of network bandwidth. In this scenario, FHBN reaches a peak network bandwidth of 45.7 GB/s, which corresponds to 91.4% of the line rate. Conversely, NCCL only attains a bandwidth of 35.5 GB/s. As a result, FHBN can also serve as a superior alternative to existing communication libraries for point-to-point transmission of large GPU memory blocks within DCNs.

## <span id="page-11-0"></span>6.4 Resource Utilization Overlapping

To assess the efficacy of resource utilization overlapping [\(sub](#page-6-2)[subsection 4.2.2\)](#page-6-2) implemented in our automated model converter, we conducted a series of experiments on the LLaMA-65B and LLaMA3-70B models, with the optimization either enabled or disabled. We use request batches of varying sizes and the context length of each request is fixed at 4096.

<span id="page-11-1"></span>![](_page_11_Figure_2.jpeg)

**Figure Description:**
**Figure Context:**
This image presents a comparison of the performance of two large language models, LLaMA-65B and LLaMA-3-70B, in terms of their ability to generate text and their energy and carbon emissions. The results show that LLaMA-65B has a higher performance in both text generation and energy and carbon emissions, but LLaMA-3-70B has a higher performance in terms of text generation.

**Figure Data (Q&A):**

Q: What is the batch size of LLaMA-65B?

Q: What is the batch size of LLaMA-3-70B?

Q: What is the batch size of LLa-3-70B?

Q: What is the batch size of L-3-70B?




Note: The data points are based on the provided image and may not be exact. The actual data may have been modified for the purpose of this response.


### Table 1: LLA-MA-65B Model

| Batch Size | LLA-MA-65B (ms) | LLA-MA-65B (ms) |
| :--- | :--- | :--- |
| 10 | 55.1 | 55.1 |
| 20 | 55.1 | 55.1 |
| 30 | 55.1 | 55.1 |
| 40 | 55.1 | 55.1 |
| 50 | 55.1 | 55.1 |
| 60 | 55.1 | 55.1 |

### Table 2: LLA-MA-3-70B Model

| Batch Size | LLA-MA-3-70B (ms) | LLA-MA-3-70B (ms) |
| :--- | :--- | :--- |
| 10 | 55.1 | 55.1 |
| 20 | 55.1 | 55.1 |
| 30 | 55.1 | 55.1 |
| 40 | 55.1 | 55.1 |
| 50 | 55.1 | 55.1 |
| 60 | 55.1 | 55.1 |

### Figure 2: LLA-MA-65B and LLA-MA-3-70B Models

**TBT (ms)**

*   LLA-MA-65B: 55.1
*   LLA-MA-3-70B: 55.1

### Figure 3: LLA-MA-65B and LLA-MA-3-70B Models

### Figure 4: LLA-MA-65B and LLA-MA-3-70B Models

### Figure 5: LLA-MA-65B and LLA-MA-3-70B Models

### Figure 6: LLA-MA-65B and LLA-MA-3-70B Models

### Figure 7: LLA-MA-65B and LLA-MA-3-70B Models

*   L- 55.1

### Figure 8: L- 55.1

**T- 55.1**

### Figure 9: L- 55.1

### Figure 10: L- 55.1

### Figure 11: L- 55.1

### Figure 12: L- 55.1

### Figure 13: L- 55.1

### Figure 14: L- 55.1


The first figure is a line plot with two lines representing the performance of two models, LLaMA-65B and LLaMA-3-70B, on a specific task. The x-axis represents the batch size, and the y-axis represents the time it takes to perform the task.

*   **LLaMA-65B:**
    *   At a batch size of 10, the model takes approximately 50-60 ms to perform the task.
    *   At a batch size of 20, the model takes approximately 30-40 ms to perform the task.
    *   At a batch size of 25, the model takes approximately 20-30 ms to perform the task.
    *   At a batch size of 50, the model takes approximately 10-20 ms to perform the task.
    *   At a batch size of 100, the model takes approximately 5-10 ms to perform the
    *   **LLaMA-3-70B:**
    *   At a batch size of 10, the model takes approximately 50-60 ms to perform the
    *   At a batch size of 20, the model takes approximately 30-40 ms to perform the
    *   At a batch size of 25, the model takes
    *   At a batch size of 50, the
    *   At a batch
   

Figure 14: Time between tokens (TBT) results with automatic resource utilization overlapping enabled and disabled.

As illustrated in [Figure 14,](#page-11-1) the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping. The speedup is particularly notable for larger batch sizes, which produce larger KV tensors and result in greater latency reduction. The effectiveness is less pronounced for the LLaMA3-70B model, where the maximum latency reduction is only 3.5%. This is because LLaMA3-70B adopts GQA, whose KV size is 8× smaller. Consequently, there is less room for resource utilization overlapping in LLaMA3-70B.

## 7 Discussion

Generality of our techniques. Although Lamina is built for model-attention disaggregation, relevant techniques can also be used to enable a wider range of fine-grained LLM disaggregation techniques in distributed heterogeneous environments. For example, LoRA [\[24\]](#page-13-2) and Mixture-of-Experts (MoE) [\[18,](#page-12-12) [53\]](#page-14-6) all add less computation-intensive operators to existing LLM architectures. Like Lamina, we may also offload the LoRA and MoE operators to less powerful but more economic remote accelerators to reduce the inference cost. Such operator-level disaggregations, unlike prefill-decode disaggregation, require frequent layer-wise communications and are considered not feasible unless an optimized networking stack like the one in Lamina is used.

Alternative heterogeneous devices. In Lamina, we may use more specialized accelerating devices for optimal performance and cost. For example, we anticipate that Processingin-Memory (PIM) devices [\[13,](#page-12-13)[22,](#page-13-3)[26,](#page-13-4)[29,](#page-13-5)[30,](#page-13-6)[32,](#page-13-7)[47,](#page-14-7)[54\]](#page-14-8) will be a more suitable candidate for memory-optimized devices as they demonstrate even greater cost advantages alongside their larger capacity and higher bandwidth. Besides, we can also

use CPU and DRAM for attention computation and KV cache storage. However, due to the relatively smaller bandwidth of host DRAM, it is preferable to also adopt sparse attention mechanisms [\[14,](#page-12-14) [52\]](#page-14-9) to reduce the size of data read during attention computation.

## 8 Related Work

System optimizations for LLM Inference. Splitwise [\[40\]](#page-14-0) and DistServe [\[59\]](#page-15-0) proposes prefill-decode disaggregation, which improves hardware utilization and minimizes the interference between the prefill and decode phases. Orca [\[57\]](#page-15-3) proposes *continuous batching*, that batches incoming requests in iteration granularity. Compared with whole-request batching, continuous batching greatly reduces resource waste caused by early termination during the decode phase. PagedAttention [\[28\]](#page-13-1) focuses on memory management optimizations, using fine-grained KV cache management to reduce memory waste. PagedAttention can also be used to optimize various decoding scenarios, like beam search and shared prefixes. These optimizations can all be used in our system. FlexGen [\[44\]](#page-14-10) is a heterogeneous LLM inference system employing layerand token-level task partitioning and scheduling. However, it does not account for the varying characteristics of different operators within a layer. LLM-tailored inference systems, like DeepSeed [\[11\]](#page-12-15), Megatron-LM [\[45\]](#page-14-11), and TensorRT-LLM [\[39\]](#page-14-12), use optimizations of various aspects including kernel optimization [\[17,](#page-12-16) [23\]](#page-13-8), advanced scheduling [\[8,](#page-12-17) [19,](#page-12-18) [33,](#page-13-9) [51\]](#page-14-13), and efficient memory management [\[19\]](#page-12-18).

Speculative Decoding The speculative decoding technology [\[31,](#page-13-10)[36,](#page-14-14)[38\]](#page-14-15) enables parallel generation of multiple tokens for a single request during the decoding phase. This is done by *guessing* the next few tokens using a smaller auxiliary model. These predicted tokens are then validated by the primary LLM. This validation of the predicted tokens can be executed in parallel, thereby enhancing the arithmetic intensity and reducing latency. However, speculative decoding can lead to a trade-off in throughput due to the auxiliary model's overhead and the potential need for re-execution in case of misprediction.

Variations of the Attention Operator. Researchers have developed many variations of the attention operator for large language models to mitigate the memory bottleneck. GQA [\[10\]](#page-12-19) and MLA [\[34\]](#page-13-11) are two recent attention mechanisms targeted for memory efficiency. Model quantization uses reduced-precision formats (e.g., FP8) to store KV caches. Various sparse attention mechanisms [\[14,](#page-12-14)[15,](#page-12-20)[27,](#page-13-12)[35,](#page-14-16)[37,](#page-14-17)[42,](#page-14-18)[43,](#page-14-19)[56\]](#page-15-4) have been adopted, focusing on a subset of all history keyvalue pairs during attention computation. All these modifications to the attention operator, however, might compromise the model quality.

## 9 Conclusion

In this paper, we present model-attention disaggregation, an innovative architectural approach to improve the efficiency of LLM decoding. This approach is motivated by the observation that the LLM decoding phase can be divided into computationintensive parts and memory-intensive parts (i.e., the attention operators). Hence, we may use computation- and memoryoptimized devices for each part to improve the hardware resource utilization. Moreover, by adjusting the To realize this idea, we design a revamped latency-optimized networking stack that facilitate the frequent data transfer between remote GPUs. We also develop automated tools for transforming and optimizing existing LLMs for model-attention disaggregation. We develop and deploy Lamina on a cluster comprising heterogeneous GPUs. Evaluation on traces collected from production systems show that Lamina provides 16.1 ∼ 90.1% higher throughput than heterogeneous solutions with similar hardware costs.

## References

- <span id="page-12-11"></span>[1] Azure llm inference trace 2023. [https:](https://github.com/Azure/AzurePublicDataset/blob/master/AzureLLMInferenceDataset2023.md) [//github.com/Azure/AzurePublicDataset/blob/](https://github.com/Azure/AzurePublicDataset/blob/master/AzureLLMInferenceDataset2023.md) [master/AzureLLMInferenceDataset2023.md](https://github.com/Azure/AzurePublicDataset/blob/master/AzureLLMInferenceDataset2023.md).
- <span id="page-12-3"></span>[2] Cloud Computing Services | Google Cloud. [https:](https://cloud.google.com/) [//cloud.google.com/](https://cloud.google.com/).
- <span id="page-12-6"></span>[3] GPUDirect RDMA. [https://docs.nvidia.com/](https://docs.nvidia.com/cuda/gpudirect-rdma/) [cuda/gpudirect-rdma/](https://docs.nvidia.com/cuda/gpudirect-rdma/).
- <span id="page-12-7"></span>[4] Mellanox adapters programmer's reference manual. [https://network.nvidia.com/files/doc-2020/](https://network.nvidia.com/files/doc-2020/ethernet-adapters-programming-manual.pdf) [ethernet-adapters-programming-manual.pdf](https://network.nvidia.com/files/doc-2020/ethernet-adapters-programming-manual.pdf).
- <span id="page-12-9"></span>[5] Ray. <https://www.ray.io/>.
- <span id="page-12-8"></span>[6] RDMA core userspace libraries and daemons. [https:](https://github.com/linux-rdma/rdma-core) [//github.com/linux-rdma/rdma-core](https://github.com/linux-rdma/rdma-core).
- <span id="page-12-2"></span>[7] TPU v6e specification. [https://cloud.google.com/](https://cloud.google.com/tpu/docs/v6e) [tpu/docs/v6e](https://cloud.google.com/tpu/docs/v6e).
- <span id="page-12-17"></span>[8] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills, 2023.
- <span id="page-12-10"></span>[9] AI@Meta. Llama 3 model card. 2024.
- <span id="page-12-19"></span>[10] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023.

- <span id="page-12-15"></span>[11] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and Yuxiong He. Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale, 2022.
- <span id="page-12-0"></span>[12] Anonymous. Hexgen-2: Disaggregated generative inference of LLMs in heterogeneous environment. In *Submitted to The Thirteenth International Conference on Learning Representations*, 2024. under review.
- <span id="page-12-13"></span>[13] Kazi Asifuzzaman, Narasinga Rao Miniskar, Aaron R Young, Frank Liu, and Jeffrey S Vetter. A survey on processing-in-memory techniques: Advances and challenges. *Memories-Materials, Devices, Circuits and Systems*, 4:100022, 2023.
- <span id="page-12-14"></span>[14] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.
- <span id="page-12-20"></span>[15] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*, 2019.
- <span id="page-12-4"></span>[16] Y. Choi, Y. Kim, and M. Rhu. Lazy batching: An sla-aware batching system for cloud machine learning inference. In *2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)*, pages 493–506, Los Alamitos, CA, USA, mar 2021. IEEE Computer Society.
- <span id="page-12-16"></span>[17] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference. [https://crfm.stanford.edu/2023/10/12/](https://crfm.stanford.edu/2023/10/12/flashdecoding.html) [flashdecoding.html](https://crfm.stanford.edu/2023/10/12/flashdecoding.html).
- <span id="page-12-12"></span>[18] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading, 2023.
- <span id="page-12-18"></span>[19] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. Turbotransformers: An efficient gpu serving system for transformer models. In *Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming*, PPoPP '21, page 389–402, New York, NY, USA, 2021. Association for Computing Machinery.
- <span id="page-12-5"></span>[20] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. Low latency rnn inference with cellular batching. In *Proceedings of the Thirteenth EuroSys Conference*, EuroSys '18, New York, NY, USA, 2018. Association for Computing Machinery.
- <span id="page-12-1"></span>[21] Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W Mahoney, and Kurt Keutzer. Ai and memory wall. *IEEE Micro*, 2024.

- <span id="page-13-3"></span>[22] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok Jeong, Seho Kim, Il Park, Mithuna Thottethodi, and T. N. Vijaykumar. Newton: A dram-maker's accelerator-inmemory (aim) architecture for machine learning. In *2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)*, pages 372–385, 2020.
- <span id="page-13-8"></span>[23] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang. Flashdecoding++: Faster large language model inference on gpus, 2023.
- <span id="page-13-2"></span>[24] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
- <span id="page-13-0"></span>[25] Wei Huang, Karthick Rajamani, Mircea R Stan, and Kevin Skadron. Scaling with design constraints: Predicting the future of big chips. *IEEE Micro*, 31(4):16–29, 2011.
- <span id="page-13-4"></span>[26] Jin Hyun Kim, Shin-Haeng Kang, Sukhan Lee, Hyeonsu Kim, Yuhwan Ro, Seungwon Lee, David Wang, Jihyun Choi, Jinin So, YeonGon Cho, JoonHo Song, Jeonghyeon Cho, Kyomin Sohn, and Nam Sung Kim. Aquabolt-xl hbm2-pim, lpddr5-pim with in-memory processing, and axdimm with acceleration buffer. *IEEE Micro*, 42(3):20–30, 2022.
- <span id="page-13-12"></span>[27] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.
- <span id="page-13-1"></span>[28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In *Proceedings of the 29th Symposium on Operating Systems Principles*, SOSP '23, page 611–626, New York, NY, USA, 2023. Association for Computing Machinery.
- <span id="page-13-5"></span>[29] Yongkee Kwon, Kornijcuk Vladimir, Nahsung Kim, Woojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo, Haerang Choi, Guhyun Kim, Byeongju An, Jeongbin Kim, Jaewook Lee, Ilkon Kim, Jaehan Park, Chanwook Park, Yosub Song, Byeongsu Yang, Hyungdeok Lee, Seho Kim, Daehan Kwon, Seongju Lee, Kyuyoung Kim, Sanghoon Oh, Joonhong Park, Gimoon Hong, Dongyoon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil Kang, Jungyeon Kim, Junyeol Jeon, Myeongjun Lee, Minyoung Shin, Minhwan Shin, Jaekyung Cha, Changson Jung, Kijoon Chang, Chunseok Jeong, Euicheol Lim, Il Park, Junhyun Chun, and Sk Hynix. System architecture and software stack for gddr6-aim. In *2022 IEEE Hot Chips 34 Symposium (HCS)*, pages 1–25, 2022.

- <span id="page-13-6"></span>[30] Ann Franchesca Laguna, Arman Kazemi, Michael Niemier, and X. Sharon Hu. In-memory computing based accelerator for transformer networks for long sequences. In *2021 Design, Automation and Test in Europe Conference & Exhibition (DATE)*, pages 1839–1844, 2021.
- <span id="page-13-10"></span>[31] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding, 2023.
- <span id="page-13-7"></span>[32] Wantong Li, Madison Manley, James Read, Ankit Kaul, Muhannad S. Bakir, and Shimeng Yu. H3datten: Heterogeneous 3-d integrated hybrid analog and digital compute-in-memory accelerator for vision transformer self-attention. *IEEE Transactions on Very Large Scale Integration (VLSI) Systems*, 31(10):1592–1602, 2023.
- <span id="page-13-9"></span>[33] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez, et al. AlpaServe: Statistical multiplexing with model parallelism for deep learning serving. In *17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)*, pages 663–679, 2023.
- <span id="page-13-11"></span>[34] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan

- Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.
- <span id="page-14-16"></span>[35] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context, 2023.
- <span id="page-14-14"></span>[36] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding, 2023.
- <span id="page-14-17"></span>[37] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In *International Conference on Machine Learning*, pages 22137–22176. PMLR, 2023.
- <span id="page-14-15"></span>[38] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification, 2023.
- <span id="page-14-12"></span>[39] NVIDIA. Tensorrt-llm: A tensorrt toolbox for optimized large language model inference. [https://github.](https://github.com/NVIDIA/TensorRT-LLM) [com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).
- <span id="page-14-0"></span>[40] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting, 2024.
- <span id="page-14-1"></span>[41] Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake: A kvcache-centric disaggregated architecture for llm serving. *arXiv preprint arXiv:2407.00079*, 2024.
- <span id="page-14-18"></span>[42] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. *arXiv preprint arXiv:1911.02972*, 2019.
- <span id="page-14-19"></span>[43] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. *Transactions of the Association for Computational Linguistics*, 9:53–68, 2021.

- <span id="page-14-10"></span>[44] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In *Proceedings of the 40th International Conference on Machine Learning*, ICML'23. JMLR.org, 2023.
- <span id="page-14-11"></span>[45] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020.
- <span id="page-14-4"></span>[46] Franyell Silfa, Jose Maria Arnau, and Antonio González. E-batch: Energy-efficient and high-throughput rnn batching. *ACM Trans. Archit. Code Optim.*, 19(1), jan 2022.
- <span id="page-14-7"></span>[47] Shrihari Sridharan, Jacob R. Stevens, Kaushik Roy, and Anand Raghunathan. X-former: In-memory acceleration of transformers. *IEEE Transactions on Very Large Scale Integration (VLSI) Systems*, 31(8):1223–1233, 2023.
- <span id="page-14-5"></span>[48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
- <span id="page-14-2"></span>[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Proceedings of the 31st International Conference on Neural Information Processing Systems*, NIPS'17, page 6000–6010, Red Hook, NY, USA, 2017. Curran Associates Inc.
- <span id="page-14-3"></span>[50] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: An insightful visual performance model for multicore architectures. *Commun. ACM*, 52(4):65–76, apr 2009.
- <span id="page-14-13"></span>[51] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models, 2023.
- <span id="page-14-9"></span>[52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024.
- <span id="page-14-6"></span>[53] Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh Marina. Moe-infinity: Offloading-efficient moe model serving, 2024.
- <span id="page-14-8"></span>[54] Xiaoxuan Yang, Bonan Yan, Hai Li, and Yiran Chen. Retransformer: Reram-based processing-in-memory architecture for transformer acceleration. In *Proceedings of*

- *the 39th International Conference on Computer-Aided Design*, ICCAD '20, New York, NY, USA, 2020. Association for Computing Machinery.
- <span id="page-15-1"></span>[55] Zhuoping Yang, Shixin Ji, Xingzhen Chen, Jinming Zhuang, Weifeng Zhang, Dharmesh Jani, and Peipei Zhou. Challenges and opportunities to enable largescale computing via heterogeneous chiplets. In *2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC)*, pages 765–770. IEEE, 2024.
- <span id="page-15-4"></span>[56] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. Bp-transformer: Modelling longrange context via binary partitioning. *arXiv preprint arXiv:1911.04070*, 2019.
- <span id="page-15-3"></span>[57] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for Transformer-Based generative models. In *16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)*, pages 521–538, 2022.
- <span id="page-15-2"></span>[58] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.
- <span id="page-15-0"></span>[59] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist-Serve: Disaggregating prefill and decoding for goodputoptimized large language model serving. In *18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)*, pages 193–210, 2024.