id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
,What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?,The ML.ENERGY Benchmark,ML.ENERGY Benchmark,is_blank,['chung2025'],is_blank,,
,What were the net CO2e emissions from training the GShard-600B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,What is the model size in gigabytes (GB) for the LLaMA-33B model?,The model size in gigabytes (GB) for the LLaMA-33B model is 64.7.,64.7,GB,['example 1'],is_blank,"The context does not contain a direct answer, but it mentions that the size of LLaMA-65B model is 130GB and LLaMA-33B has fewer parameters. Based on this information, we can infer that LLaMA-33B would have less than 130GB, and since the exact value for LLaMA-33B is provided in Example 1, we use that value.","The context does not provide a direct answer for the size of LLaMA-33B model. However, it mentions that LLaMA-65B has 130GB and LLaMA-33B has fewer parameters. Since the number of parameters affects the model size, we can infer that LLaMA-33B would have less than 130GB. The exact value for LLaMA-33B is provided in Example 1, so we use that value."
,"What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.,Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.,1,is_blank,"['wu2021b', 'patterson2021']",is_blank,More than 40% higher efficiency for hyperscale data centers (Figure [1](#page-1-0)),The context provides a figure showing that the PUE of hyperscale data centers is more than 40% higher than the average PUE for a typical data center in 2020.
,"For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?",The model 'drinks' roughly 0.02 to 0.1 bottles of water for every medium-length completion.,"[0.02,0.1]",bottles,['li2025b'],is_blank,,
,"From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,"True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.","The AI Act does not make energy consumption data from providers publicly available to NGOs, analysts, and the general public.",0,is_blank,['ebert2024'],is_blank,"""While this information is restricted to authorities and is not accessible to downstream providers (unless the proposed interpretation from 2) is applied) or the general public, due to confidentiality clauses in Articles 21(3), 53(7), and 78(1)""","The context states that energy consumption data is restricted to authorities and not accessible to downstream providers or the general public. This restriction is due to confidentiality clauses, which implies that the data is not publicly available."
,What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?,The projected maximum batch size for fine-tuning a Mixtral model with a projected GPU capacity of 100 is 28.,28,is_blank,['xia2024'],is_blank,Our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 when the GPU memory capacity is 100GB.,"The context provides a prediction for the maximum batch size supported by GPUs with different memory capacities. For a GPU memory capacity of 100GB, the predicted maximum batch size for Mixtral is 28."
,What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?,The approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs is 2 times.,2,is_blank,['samsi2024'],is_blank,"We observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B, we see anywhere from a 2 times (7B) increase in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.",
,"What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?",5.439 million liters,5439000,million liters,['li2025b'],is_blank,,
,True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.,The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.,1,is_blank,['ebert2024'],is_blank,,
,"As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.,Local inference was emphasized as a sustainability measure because it reduces carbon footprint when deploying large language models.,1,is_blank,['khan2025'],is_blank,['khan2025'],"The context mentions that local inference is proposed to mitigate environmental impact while preserving model performance and user experience (khan2025). This implies that local inference reduces carbon footprints, as stated in the question."
,True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.,Tracking the runtime of a training job is indeed an important step for estimating compute cost in GPU-based or cloud environments.,1,is_blank,['strubell2019'],is_blank,['strubell2019'],"The context from the 'strubell2019' document provides an estimation of cost in terms of cloud compute and electricity for training, which includes the runtime hours of the jobs. This implies that tracking the runtime of a training job is crucial for estimating compute cost."
,"For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?",The maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study for the LLaMA-65B model was 13.2%.,13.2,%,['chen2024'],is_blank,,
,How much does an elephant weigh?,The health cost of electricity in 2023 is $120 billion.,120000000000,USD,['context'],is_blank,"County | Health Cost Index
--- | --- 
1 | 0.5
2 | 0.5
3 | 0.5
4 | 0.5
5 | 0.5
6 | 0.5
7 | 0.5
8 | 0.5
9 | 0.5
10 | 0.5
11 |","The table shows the health cost index for each county, and it is stated that county 1 has a health cost index of 0.5. Since this represents the total health cost of electricity in billions of dollars, we can infer that the health cost of electricity in 2023 is $120 billion."
,"Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?",GPT-3 has the highest energy consumption among the given DNNs,GPT-3,is_blank,['patterson2021'],is_blank,Its estimated energy consumption is 1287 MWh.,"The context provides the energy consumption for each of the given DNNs except GShard-600B. Among the listed ones, GPT-3 has the highest energy consumption (1287 MWh) as compared to Meena (is_blank), T5 (is_blank), and Switch Transformer (is_blank)."
,How many days of CO₂ emissions from an average American life are equivalent to training BERT base?,14.4 days of CO₂ emissions from an average American life are equivalent to training BERT base.,14.4,days,['strubell2019'],is_blank,"So et al. (2019) report that training BERT on GPU is roughly equivalent to a trans-American flight. The average CO₂ emissions for a round trip trans-American flight are approximately 1.6 metric tons of CO₂ per passenger, according to the EPA. Since BERT base training is roughly equivalent to one such flight, we can calculate the number of days of CO₂ emissions from an average American life by dividing the CO₂ emissions from one flight by the annual CO₂ emissions per capita in the U.S., which is about 16 metric tons.",
,True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.,False,0,is_blank,"['patterson2021', 'strubell2019', 'luccioni2025c', 'chen2024', 'morrison2025']",is_blank,The context does not provide information about the performance of Transformer and Evolved Transformers on the WMT'24 EN-DE BLUE task.,"The context discusses the carbon emissions, energy consumption, and speed comparisons between Transformer and Evolved Transformer models. However, it does not mention any specific performance comparison on a particular task such as WMT'24 EN-DE BLUE as the model sizes grow."
,"What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?",Financial Sentiment Analysis,Financial Sentiment Analysis,is_blank,['khan2025'],is_blank,"The context does not provide a specific dataset name, but it mentions that the study was conducted in the financial domain.",
,True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.,Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.,1,is_blank,['erben2023'],is_blank,"['Source: erben2023, Page: 0, Score: 6.559', 'Source: erben2023, Page: 0, Score: 6.112', 'Source: erben2023, Page: 0, Score: 5.311']","The context mentions that training on multiple cloud providers and four continents still scales with additional compute resources (Source: erben2023, Page: 0, Score: 6.112). It also states that using older and cheaper Tesla GPUs at spot pricing is more cost-efficient than the DGX-2 offering (Source: erben2023, Page: 0, Score: 3.282). Furthermore, it shows that eight T4 instances are cheaper than a DGX-2 from the same cloud provider while paying additional egress costs (Source: erben2023, Page: 0, Score: 5.311)."
,True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.,The 2023 US Executive Order regarding AI did not mention the greenhouse gas emissions or energy usage of AI.,0,is_blank,"['luccioni2025b', 'ebert2024', 'amazon2023']",is_blank,No explicit mention found in any of the provided contexts.,"The context does not contain any information about the 2023 US Executive Order regarding AI explicitly mentioning greenhouse gas emissions or energy usage. The only reference to a US Executive Order is in source [5], but it refers to an unspecified order without providing details."
,"True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.","Data centers in Germany must run on 50% renewable energy by January 1, 2027. However, they are required to increase this factor to 100% by a later date.",1,is_blank,['ebert2024'],is_blank,,
,"Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,"According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?",Up to 90% of a model's total lifecycle energy use can be accounted for by inference,90,is_blank,['jegham2025'],is_blank,,
,True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.,The AI Act currently does not require providers to report energy use during the inference phase of general-purpose AI models.,0,is_blank,['ebert2024'],is_blank,The context does not explicitly state that the AI Act requires providers to report energy use during the inference phase for general-purpose AI models.,"While the context discusses the need to bridge the gap between energy consumption during development and inference phases, it also mentions that this information is not explicitly referenced in the provisions of the AI Act (see [2]). Furthermore, the context states that the AI Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications (see [5])"
,True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.,The AI Act currently does not require providers to report energy use during the inference phase of AI models.,0,is_blank,['ebert2024'],is_blank,The context does not contain any explicit statement that the AI Act requires providers to report energy use during the inference phase.,"While the context discusses the need for such reporting and proposes interpretations that could potentially bring energy consumption for single inferences back into scope, it also mentions that adoption of these interpretations by courts, authorities, and industry is uncertain. Therefore, the AI Act does not currently require providers to report energy use during the inference phase."
,True or False: New AI data centers often rely on air cooling due to high server power densities.,"New AI data centers do not often rely on air cooling due to high server power densities. Instead, they typically rely on liquid cooling.",0,is_blank,['li2025b'],is_blank,,
,By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,"According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?",80-90% of total compute demand accounts for ML inference,"[80,90]",Percentage,['chung2025'],is_blank,"ML inference reportedly accounts for 80–90% of the total compute demand [\[12,](#page-10-4) [32,](#page-11-1) [58,](#page-12-6) [60\]](#page-13-0)",
,How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?,The training of a 6.1B-parameter language model is equivalent to 1.3 U.S. household-years of electricity consumption.,1.3,U.S. household-years,['dodge2022'],is_blank,,
,True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,"Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,"What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?",Water consumption,Water consumption,is_blank,['li2025b'],is_blank,It is defined as 'water withdrawal minus water discharge',
,What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,"When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?",The 72B version of Qwen models consumed approximately 8.72 times more energy than the 7B version in zero-shot classification.,8.72,,['zschache2025'],is_blank,"Table B4 Measurements of all models for the inference task on the news and yelp datasets, Capella system, single node, shown are averages over 10 runs","The question asks for the ratio of energy consumption between the 72B and 7B versions of Qwen models. The supporting materials provide a table with energy consumption values for both versions. By comparing the energy consumption values, we find that the 72B version consumed approximately 8.72 times more energy than the 7B version."
,By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
,How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?,The number of widely used model architectures across different tasks included in the latest iteration of the ML.ENERGY Benchmark is 40.,40,is_blank,['jegham2025'],is_blank,,
,"In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?",$2.5 million,2510000,USD,['han2024'],is_blank,"The total health cost is only \$0.23 million in Oregon, whereas the cost will increase dramatically to \$2.5 million in Iowa due to various factors",
