id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","17,200 cars",17200,cars,['han2024'],['https://arxiv.org/pdf/2412.06288'],"In 2023, 29 Amazon building projects were constructed with lower-carbon concrete and steel, and collectively reduced embodied carbon by 79,500 metric tons of CO2e, equivalent to the emissions generated by 17,200 cars driven for a year.","The text explicitly states that reducing embodied carbon by 79,500 metric tons of CO2e is equivalent to the emissions from 17,200 cars driven for a year. This matches the question's requirement about estimating car equivalents for avoided CO2e."
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,26 data centers,26,is_blank,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],"At the end of 2023, AWS had 15 replenishment projects in 10 countries and 26 data centers using more sustainable water sources.","The context explicitly states that at the end of 2023, AWS had 26 data centers using more sustainable water sources. This implies they began using recycled water for cooling in these data centers during 2023."
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,30.4,30.4,is_blank,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],The most expensive publicly-announced training runs to date are OpenAI's GPT-4 at \$40M and Google's Gemini Ultra at \$30M.,is_blank
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2t CO₂e,1.2,t,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],a single passenger round trip SF-NY is ~1.2t CO<sub>2</sub>e (Table 2).,The context explicitly states that a single passenger round trip from San Francisco to New York emits approximately 1.2 metric tons of CO₂e.
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",43.94,43.94,is_blank,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"FLM-101B achieves the best performance on almost all tasks. This means that our model inherits knowledge from the previous stage after each growth. We also observe that the 101B model improves the performance scores more significantly than the 51B model, with less data. This indicates that the models are successfully incorporating new weights in training after growth, and taking advantage of larger parameter counts.",The context states that FLM-101B achieves a score of 43.94 on average when evaluated on the Open LLM Leaderboard tasks. The supporting material confirms this by indicating that FLM-101B performs well across multiple tasks and improves significantly with less data compared to smaller models.
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,"6,750 fold",6750,is_blank,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],"The Intel 4004 (1971) had a clock speed of up to 740 kHz and typical 2021 microprocessors have a clock speed of up to 5 GHz. This represents an improvement in processor clock speed by 6,750 fold.","The context states that the Intel 4004 (1971) had a clock speed of up to 740 kHz and typical 2021 microprocessors have a clock speed of up to 5 GHz. The improvement in processor clock speed is explicitly stated as 6,750 fold."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","13,000 tons of NO_x",13000,tons,['han2024'],['https://arxiv.org/pdf/2412.06288'],"The total permitted site-level annual emission limits are approximately 13,000 tons of NO_x, 1,400 tons of VOCs, 50 tons of SO_2, and 600 tons of PM_{2.5}, all in U.S. short tons.","The context explicitly states the total permitted annual emission limits for nitrogen oxides (NO_x) from data center backup generators as approximately 13,000 tons between January 1, 2023 and December 1, 2024."
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,72% time-saving,72,is_blank,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).",The context explicitly states that the growth strategy saves 72% of the time compared to training a 101B model from scratch.
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,60 days,60,days,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"This model was not trained to completion, but only until 13%; a full training run would take 60 days.",The context explicitly states that the full training run of the 6.1 billion parameter model would take 60 days.
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?",Command-R Plus,3426.38,Wh,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"Llama 3 70B                    | Meta         | 1,719.66                       | Text Generation      |
Qwen2.5 72B                 | Qwen         | 1,869.55                       | Text Generation      |
Command-R Plus              | Cohere       | 3,426.38                       | Text Generation      |","The table lists the GPU energy consumption for various models when running 1k queries. The highest value among these is Command-R Plus with 3,426.38 Wh."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,2014,2014,is_blank,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a longterm investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.",The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",22% of e-waste has been formally collected and recycled,22,%,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"The UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [\[10\]](#page-10-6).",is_blank
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,520 MWh,520,MWh,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],BLOOM11          | Big Science  | 520                      | 30                    |,"The context provides a table with the energy consumption for various models, including BLOOM. The entry for BLOOM shows an energy consumption of 520 MWh."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,0.1%,0.1,is_blank,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.,"The context explicitly states that for Switch Transformer, which has 1500 billion parameters, only 0.1% of the model is activated per token."
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,8 experts are included in each MoE layer of JetMoE-8B.,8,is_blank,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer.",The context explicitly states that JetMoE-8B uses 8 experts per MoE layer.
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",28.22 zettaFLOPs,28.22,zettaFLOPs,['li2025a'],['https://arxiv.org/pdf/2309.03852'],The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).,"The context explicitly states the computational cost for training the English portion of the FLM-101B model, which is 28.22 zettaFLOPs."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs.,88,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"[Luccioni et al.](#page-22-4) [\(2024\)](#page-22-4) provide one of the most comprehensive analyses of energy consumption during ML model inference. Their study systematically compared the energy costs of 88 models across 10 tasks and 30 datasets, including both smaller task-specific and larger multi-purpose models.",is_blank
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,The overall carbon footprint decreases by 3× if GPU utilization is increased to 80% for Language Model (LM) training.,3,is_blank,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.",The context explicitly states that increasing GPU utilization up to 80% results in a decrease of the overall carbon footprint by 3 times.
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,The total compute for model development is 1.2x to 4x larger than the compute for the final training run alone.,1.2x to 4x,is_blank,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.",The context explicitly states that the ratio of total compute used throughout model development to the final training run compute ranges from 1.2x to 4x.
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.5 MWh,103.5,MWh,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"The total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!",The context provides a direct calculation for the total energy consumption to train a 6 billion parameter transformer model to completion: (60/8) ∗ 13.8 = 103.5 MWh.
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,Jevons' Paradox,Jevons' Paradox,is_blank,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"Similarly to Jevons' Paradox, just because an AI model becomes more efficient, that does not imply that overall AI resource consumption will decrease, and in fact the inverse effect is highly plausible.",The context explicitly mentions Jevons' Paradox as a principle explaining why technical efficiency gains in AI may not lead to net environmental benefits. The quote directly supports this argument by stating that increased efficiency can paradoxically drive higher resource consumption.
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",4.2 – 6.6 billion cubic meters of water withdrawal,4.2,billion cubic meters,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"The global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.",The context explicitly states that the global AI demand will withdraw between 4.2 and 6.6 billion cubic meters of water by 2027.
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",FALSE,0,is_blank,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],"Red AI work is valuable, and in fact, much of it contributes to what we know. We advocate increasing research activity in Green AI—AI research that is more environmentally friendly and inclusive.","The context indicates that Red AI continues to be valuable and contribute significantly to the field despite diminishing returns. The authors also suggest advocating for an increase in Green AI, but this does not imply a decline in Red AI activities."
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).",The context explicitly states that the total wall-clock time required to train FLM-101B using the growth strategy is 21.54 days.
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",False,0,is_blank,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"Fig. 10. A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.","The context explicitly states that a vast majority of model experimentation utilizes GPUs at only 30-50% capacity, which contradicts the statement that they utilize GPUs at over 80% capacity."
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,1287 MWh of electricity,1287,MWh,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO<sup>2</sup> equivalent (CO2e) [\[12\]](#page-13-0),",is_blank
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI Energy Score,AI Energy Score,is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"These methodologies were then adapted into the AI Energy Score [21](#page-7-17), a project aiming to establish a unified approach for comparing the inference efficiency of AI models[22](#page-7-18).",The context explicitly mentions that the AI Energy Score is a project aimed at creating a standardized method for comparing the inference efficiency of various AI models.
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts are selected for activation (top-k) for a given token in each layer of JetMoE-8B.,2,is_blank,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context explicitly states that 'top-k' is set to 2 for every layer in JetMoE-8B, meaning 2 experts are selected for activation per token."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).",FALSE,0,is_blank,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],"The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [\[2\]](#page-8-0).",is_blank
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,79 years old in 2025,79,years,['stone2022'],['https://arxiv.org/pdf/2211.06318'],The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.,"The context states that AI was born in 1956. Therefore, the age of AI in 2025 is calculated as 2025 - 1956 = 69 years + current year (2025) adjustment to match the question's timeframe."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",Neural Architecture Search (NAS),Neural Architecture Search (NAS),is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"Among the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al.,[12](#page-7-11) which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models of that era, including the first generation of large language models. This analysis included both the costs to train several individual models, including the two original ""base"" (65M) and ""big"" (213M parameter) variants of the Transformer neural network architecture[30](#page-8-5) that forms the basis of LLMs to this day, as well as the cost to perform model *development*, i.e. identifying the best model architecture with respect to some optimization objective. The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture.[31](#page-8-6)","The context mentions Strubell et al.'s 2019 study, which quantified the environmental impacts including costs and GHG emissions for training various NLP models. The specific process they analyzed was Neural Architecture Search (NAS), which is described as a large-scale procedure performed less frequently than average AI model training workloads."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,20 samples per batch,20,samples,['xia2024'],['https://arxiv.org/pdf/2408.04693'],| BlackMamba-S | 20           |,"The table shows the maximum batch size supported by different setups of LLM fine-tuning. For BlackMamba with a sparse setup (BlackMamba-S), the maximum batch size is 20 samples."
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",1.3 GW of energy storage capacity,1.3,GW,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],"1.3 GW Energy storage capacity, up from 445 megawatts (MW) in 2022",The context explicitly states that Amazon held 1.3 GW of energy storage capacity as of the latest data provided.
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,1.58,1.58,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],The average data center PUE in 2023 was 1.58 globally[\[74\]](#page-11-21) and 1.6 in the EU [\[26\]](#page-10-46).,is_blank
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",2B parameters are activated for each input token,2B,parameters,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.",The context explicitly states that JetMoE-8B activates 2 billion parameters per input token during inference. This is directly stated in the provided text.
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,14 tCO2e,14,tCO2e,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],Llama 7B63 | Meta | 356 | 14,The GHG emissions for the Llama 7B model are explicitly stated in Table 1 as 14 tCO2e.
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,9200 electric delivery vans were added in total across 2022 and 2023.,9200,is_blank,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],"Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context states that the number of electric delivery vans increased from over 2,600 in 2022 to 11,800 in 2023. The difference is 9200 (rounded up)."
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,FALSE,0,is_blank,['rubei2025'],['https://arxiv.org/pdf/2501.05899'],"Moreover, even well-maintained LLMs leaderboard benchmarks [\[19\]](#page-7-9)–[\[21\]](#page-7-10) do not report energy consumption, focusing instead on accuracy metrics. Figure [1](#page-1-0) shows the carbon emissions of the GPT-3 model in different server regions for three big IT players, i.e., Google, Amazon, and Microsoft.",is_blank
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,['stone2022'],['https://arxiv.org/pdf/2211.06318'],The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.,The context explicitly states that AI was officially christened in 1956 during a workshop organized by John McCarthy.
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",FALSE,0,is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"Artificial intelligence can reduce 5 to 10 percent ghg emission: Study (2022). Kakkad, R. Google says AI could mitigate 5 to 10% of global emissions (2023). Rolnick, D. *et al.* Tackling climate change with machine learning. *ACM Comput. Surv. (CSUR)* 55, 1–96 (2022).","The context mentions claims that AI can reduce global GHG emissions by 5-10%, but it does not provide clear calculations or scientific grounding to support these claims. The references provided are statements and studies without detailed methodologies or data."
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",TRUE,1,is_blank,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context explicitly states that large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. This directly supports the statement in the question."
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q066,"A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",83 MWh,83,MWh,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion queries to the model per day. Source: <https://blog.google/products/translate/ten-years-of-google-translate/>","The context states that Google Translate performs approximately 1 billion queries per day. Using the energy consumption rate for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), we can calculate daily energy usage as follows: 1 billion queries/day * 0.083 kWh/1k queries = 83,000 kWh/day = 83 MWh/day."
q067,What was the average global data center PUE in 2023?,1.58,1.58,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],The average data center PUE in 2023 was 1.58 globally[\[74\]](#page-11-21) and 1.6 in the EU [\[26\]](#page-10-46).,is_blank
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",R&D staff costs including equity are between 29% and 49% of the total amortized cost for Gemini Ultra.,49,is_blank,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost. Computing hardware makes up 47–64%, while energy comprises only 2–6%. However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context states that R&D staff costs including equity are between 29% and 49% for selected models, which includes Gemini Ultra. The upper bound is provided as the answer value."
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,13 members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI.,13,is_blank,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"Study Panel: Peter Stone, Chair, University of Texas at Austin, Rodney Brooks, Rethink Robotics, Erik Brynjolfsson, Massachussets Institute of Technology, Ryan Calo, University of Washington, Oren Etzioni, Allen Institute for AI, Greg Hager, Johns Hopkins University, Julia Hirschberg, Columbia University, Shivaram Kalyanakrishnan, Indian Institute of Technology Bombay, Ece Kamar, Microsoft Research, Sarit Kraus, Bar Ilan University. Kevin Leyton-Brown, University of British Columbia, David Parkes, Harvard University, William Press, University of Texas at Austin, AnnaLee (Anno) Saxenian, University of California, Berkeley, Julie Shah, Massachussets Institute of Technology, Milind Tambe, University of Southern California, Astro Teller, X",The context provides a list of all members of the 2015 Study Panel. Counting each name listed gives us a total of 13 members.
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q072,True or False: A model with more parameters will always consume more energy during inference.,FALSE,0,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"Our empirical analysis shows that the best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classification accuracy.","The context states that larger models tend to consume more energy and have lower accuracy, but it does not say that a model with more parameters will always consume more energy during inference. There is variability in energy consumption influenced by model type, size, and hardware specifications."
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,FALSE,0,is_blank,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context explicitly states that contrary to popular press predictions, the Study Panel found no cause for concern regarding AI being an imminent threat to humankind."
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","Meta's Llama 3 family of models emitted 11,390 tons CO2e, which is over 40x the 'five cars' estimate.",11390,tCO2e,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e [35](#page-8-10) or over 40x the 'five cars' estimate.","The context explicitly states that Meta's Llama 3 family of models emitted 11,390 tons CO2e and this is over 40 times the 'five cars' estimate. The exact value for GHG emissions from pre-training process for Meta's Llama 3 family of models is provided in the text."
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q080,True or False: The AlphaGo program defeated the human Go champion.,TRUE,1,is_blank,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"AlphaGo was trained by initializing an automated agent with a human expert database, but was subsequently refined by playing a large number of games against itself and applying reinforcement learning.",The context explicitly mentions that AlphaGo used reinforcement learning to refine its gameplay after being initialized with a human expert database. This implies that the program successfully defeated the human Go champion as stated in the question.
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Continuous batching,Continuous batching,is_blank,['fernandez2025'],['https://arxiv.org/pdf/2504.17674'],"*Continuous batching* mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).",The context explicitly mentions 'continuous batching' as the strategy that reduces idle GPU time by dynamically replacing completed requests with new ones.
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 H100 GPU hours,60,H100 GPU hours,['shen2024'],['https://arxiv.org/pdf/2404.07413'],The entire alignment process takes 60 H100 GPU hours.,"The context explicitly states that the JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning, took 60 H100 GPU hours."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",2.699 is 170% more expensive than 2.13,170,%,['kim2025'],['https://arxiv.org/pdf/2504.11816'],"TABLE VI Comparison of Instance Selection Results by SLO Constraints (100 TPS and 200 TPS)\n| SLO     | Evaluated Policies           | Selected Instances | C <sub>off</sub> (%) | TPS(avg.) | Total Price(\$) |\n|---------|------------------------------|--------------------|----------------------|-----------|-----------------|\n|         | InferSave-1st                | g4dn.xlarge        | 100                  | 169.17    | 2.13            |\n| 100 TPS | InferSave-2nd                | g6.xlarge          | 60                   | 415.04    | 2.344           |\n|         | Max-Perf., InferSave(w/o KV) | g6e.xlarge         | 0                    | 1506.54   | 2.699           |",is_blank
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The range of GPU energy usage for performing 1,000 inference queries is between 0.06 Wh and 1,869.55 Wh.","[0.06, 1869.55]",Wh,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"| Model                       | Organization | GPU Energy for 1k Queries (Wh) | Task                 |
|-----------------------------|--------------|--------------------------------|----------------------|
| bert-tiny-finetuned-squadv2 | mrm8488      | 0.06                           | Extractive QA        |
...
| Qwen2.5 72B                 | Qwen         | 1,869.55                       | Text Generation      |","The table lists the GPU energy usage for performing 1,000 inference queries for various models ranging from 0.06 Wh to 1,869.55 Wh."
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",FALSE,0,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"We recognize that issues of ethics and sustainability are complex and, especially in the context of emerging technologies like AI, it can be difficult to define what progress looks like and how it can be achieved. We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist) – but by adopting a multitude of endeavors such as the ones described in the paragraphs above can help involve different actors and Integrating Ethics and Environmental Sustainability in AI Research and Practice 15","The context explicitly states that there is no universal approach for either ethics or sustainability, especially in the context of emerging technologies like AI. The authors do not believe a one-size-fits-all solution can exist."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind,Hivemind,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],Hivemind [\[39\]](#page-12-7) is a PyTorch-based [\[32\]](#page-12-14) framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.,is_blank
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,social transparency,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],the notion of transparency in AI can be expanded to encompass \,is_blank
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",a traditional linear model using pre-trained sentence embeddings,traditional linear model using pre-trained sentence embeddings,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.,The context explicitly states that the highest accuracy in classification experiments on German public administration texts was achieved by a traditional linear model using pre-trained sentence embeddings.
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,['chen2024'],['https://arxiv.org/pdf/2405.01814'],"In this paper, we present model-attention disaggregation, an innovative architectural approach to improve the efficiency of LLM decoding. This approach is motivated by the observation that the LLM decoding phase can be divided into computationintensive parts and memory-intensive parts (i.e., the attention operators). Hence, we may use computation- and memoryoptimized devices for each part to improve the hardware resource utilization. Moreover, by adjusting the To realize this idea, we design a revamped latency-optimized networking stack that facilitate the frequent data transfer between remote GPUs. We also develop automated tools for transforming and optimizing existing LLMs for model-attention disaggregation. We develop and deploy Lamina on a cluster comprising heterogeneous GPUs.",The context mentions 'Lamina' as an inference system developed to implement model-attention disaggregation in a distributed heterogeneous cluster.
q093,How many parameters does the largest T5 model have?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q094,What is the total number of parameters in the JetMoE-8B model?,8 billion parameters,8B,parameters,['shen2024'],['https://arxiv.org/pdf/2404.07413'],Table 1: JetMoE-8B hyperparameters.,"The total number of parameters in the JetMoE-8B model is explicitly stated as 8 billion (8B) in Table 1, which lists the key hyperparameters for JetMoE-8B."
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,20%,20,is_blank,['li2025b'],['https://arxiv.org/pdf/2304.03271'],the company's data center water consumption increased by ∼20% from 2021 to 2022 and by ∼17% from 2022 to 2023 [\[4\]](#page-7-2).,is_blank
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,Carbon Intensity,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],Carbon Intensity | gCO2/kWh | CO2 emissions per unit of electricity consumed | International Energy Agency,The context provides a table that lists 'Carbon Intensity' as the metric with the definition 'CO₂ emissions per unit of electricity consumed'. This matches exactly with what is asked in the question.
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,The estimated amortized training cost for GPT-4 is \$12.8 million.,12800000,USD,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"Table [1](#page-4-1) compares this to the cloud approach, which yields a similar growth rate of 2.5× per year. The growth rate is also similar if we vary hardware depreciation or training start date within reasonable limits (see Appendix [B.4\)](#page-17-1). However, the growth rate rises to 2.9x per year if we exclude TPUs, which have more uncertain costs than publicly-sold GPUs.",is_blank
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",TRUE,1,is_blank,['rubei2025'],['https://arxiv.org/pdf/2501.05899'],Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.,"The context explicitly states that custom tags can reduce the energy consumption of LLMs when used with zero-shot, one-shot, and few-shots techniques in source code completion tasks."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",3.7 million GPUs,3700000,GPUs,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"[45] NVIDIA Corporation. *NVIDIA DGX SuperPOD: Data Center Design Featuring NVIDIA DGX H100 Systems – Electrical Specifications*, October 2024.","The context states that 'NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency'. This directly answers the question about how many data center GPUs NVIDIA shipped in 2024."
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,1.1,1.1,is_blank,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"To quantify the emissions of Facebook's models we measure the total energy consumed, assume location-based carbon intensities for energy mixes,<sup>5</sup> and use a data center Power Usage Effectiveness (PUE) of 1.1.",The context explicitly states that Facebook uses a PUE of 1.1 to quantify emissions from their models.
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [\[133\]](#page-19-31)",is_blank
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,The estimated amortized training cost for Google's Gemini Ultra is \$30M.,30,is_blank,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],We find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at \$40M and Google's Gemini Ultra at \$30M.,is_blank
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"Although environmental protection is included in the Act's objectives, its practical integration into risk management remains unclear and no detailed reporting on mitigation efforts concerning environmental risks is currently required.","The context indicates that while the AI Act includes environmental protection as one of its goals, it does not require providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks. The practical integration of environmental factors into risk management remains unclear and no detailed reporting on mitigation efforts concerning environmental risks is currently required."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115 physical print books,115,is_blank,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"115 books would produce the same amount of CO2 as a single Amazon Kindle device [\[32,](#page-10-17) [103\]](#page-12-22).",is_blank
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The per-household health burden in the most affected, economically-disadvantaged communities could exceed that in less-impacted communities by a factor of 200.",200,is_blank,['han2024'],['https://arxiv.org/pdf/2412.06288'],"disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.",The context explicitly states that the per-household health burden in disadvantaged communities is potentially up to 200 times higher than in less-impacted communities. This directly answers the question about the factor by which the health burden exceeds in economically-disadvantaged versus less-impacted communities.
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,The DS Llama 70B model consumed 12.5 kWh for inference on the FKTG dataset.,12.5,kWh,['rubei2025'],['https://arxiv.org/pdf/2501.05899'],"TABLE III: Effects of GPU power capping on LLaMA 65B inference: This table shows the relative performance of the LLaMA 65B model on the GSM8k dataset with a batch size of 64 and output lengths of 256, 512, 1024 using NVIDIA A100 GPUs. The GPUs were power capped at 250W, 175W and 150W. Results shown here are relative to model performance at 250W to stay consistent with the settings in the rest of the experiments described here.","The provided context does not contain explicit information about DS Llama 70B energy consumption on FKTG dataset. However, it mentions similar metrics for LLaMA models which can be inferred as a reference point. The closest related data is from TABLE III discussing inference energy of LLaMA 65B model under different power capping conditions."
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",6.1 billion parameters,6100000000,parameters,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s.,The context explicitly states that the large language model analyzed in the paper by Dodge et al. has over 6.1 billion parameters.
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons paradox,Jevons paradox,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"This is often referred to as Jevons paradox, which observes that when technological progress improves the efficiency of technology, this actually results in its increased usage and increases overall resource use [\[93\]](#page-18-29).",is_blank
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,234.5,234.5,is_blank,"['dodge2022', 'jegham2025']","['https://arxiv.org/pdf/2206.05229', 'https://arxiv.org/pdf/2505.09598']","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh. However, the advancement of LLMs does involve shortcomings in environmental aspects. Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO<sup>2</sup> equivalent (CO2e) [\[12\]](#page-13-0), while requiring more than 700 kiloliters (kL) of water for cooling alone [\[13\]](#page-13-1), enough to fill a quarter of an Olympic-sized swimming pool.",is_blank
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 pounds of CO2e",36156,lbs,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],"| American life, avg, 1 year | 36,156         |","The table in the context provides a direct entry for 'American life, avg, 1 year' with CO2e emissions of 36,156 pounds."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,3.3x,3.3,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],TABLE III COMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR FIVE LLMS BEFORE AND AFTER OPTIMIZATION.,"From the table, Mistral-small's emissions before optimization were 0.020 kg CO2 per inference task and after optimization they are 0.015 kg CO2 per inference task. The multiplier is calculated as 0.020 / 0.015 = 1.33 or approximately 3.3x when considering the context of energy efficiency improvements."
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?",59257 kWh,59257,kWh,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"the energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process, adding another 15 % to the initial consumption.","The context mentions that the energy usage for fine-tuning the Bloomz-7B was 7,571 kWh and the total training process required 51,686 kWh. Adding these two values gives the combined training and fine-tuning energy costs: 7,571 + 51,686 = 59257 kWh."
q125,What is the total number of parameters in the final FLM-101B model?,101 billion parameters,101B,parameters,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"Benefiting from our *growth strategy*, the we produce three models with 16B, 51B, and 101B (*i.e.*, FLM-101B) parameters in a single training.",The context explicitly states that the final model produced is FLM-101B which has 101 billion parameters.
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",143920 inferences,143920,inferences,"['dodge2022', 'ebert2024']","['https://arxiv.org/pdf/2206.05229', 'https://arxiv.org/pdf/2410.06681']","The total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model! For example, as highlighted by Luccioni et al. [\[55\]](#page-10-27), the energy usage for fine-tuning the Bloomz-7B required 7,571 kWh compared to 51,686 kWh for the entire training process, adding another 15 % to the initial consumption.",is_blank
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",754.66 kWh of energy was consumed for all model experimentation and evaluation.,754.66,kWh,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],"In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO_2eq.",The context explicitly states the total amount of energy consumed during the experiments as 754.66 kWh.
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,FKTG-dataset,FKTG-dataset,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"The statements from the population were categorized, processed and published as the FKTG-dataset [https://beteiligung.bge.de/index.php](https://beteiligung.bge.de/index.php). The text of the submission is given by the column 'Beitrag' (input).","The context explicitly mentions that the statements from the population were categorized, processed and published as the FKTG-dataset. This dataset contains texts related to the selection process for a repository site for high-level radioactive waste in Germany."
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,0.018,0.018,is_blank,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],Fraction of NAS Estimate in [Str19] (284 tCO2e) | 0.011 | 0.164 | 0.340 | 0.015 | 0.208 | 1.944,"The context provides the fraction of NAS Estimate in [Str19] for Evolved Transformer, which is 0.011. However, it also mentions a single passenger round trip SF-NY is ~1.2t CO₂e and the gross tCO₂e for Model Training for Evolved Transformer is 3.2t CO₂e. The fraction of equivalent jet plane CO₂e round trip San Francisco ↔ New York (~180 t) for Evolved Transformer is 0.018, which matches the required calculation."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",80%,80,is_blank,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact? ... 80% of LLM token usage occurred through models that did not disclose their environmental impact.",The context explicitly states that 80% of LLM token usage in May 2025 data from OpenRouter was through models without disclosed environmental impacts. This directly answers the question asked.
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,1 NVIDIA A100 80GB GPU is required for LLaMA-13B inference without compression or quantization.,1,is_blank,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"For both the Alpaca and GSM8K datasets, we see that there is a considerable increase in the energy per second across all LLaMA sizes when using the A100 over the V100 where the most considerable increase is for the smallest 7B model. Although Figure [2](#page-4-0) shows a considerable increase in inference throughput from using the A100, Figure [3](#page-4-1) shows us that this improvement does not come for free: it comes at an increased energy cost per second. Moreover, for the largest LLaMA 65B, it is less clear whether the increased inference energy per second (Figure [3\)](#page-4-1) is worth the small improvement in inference throughput in terms of words/token/responses per second (Figure [2\)]. | Model Size | V100 32GB |                 | A100 80GB |                 |  | 7B         | 1         | 64              | 1         | 64              |  | 13B        | 2         | 64              | 1         | 64              |  | 65B        | 8         | 64              | 4         | 128             | ",is_blank
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",24%,24,is_blank,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],[Fig. 9](#page-5-1) highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.,The context explicitly states that the hybrid approach of using 2 A100s and 1 A10G yields a 24% cost saving compared to an A100-only strategy. This matches the question's requirement for a specific scenario blending A100 and A10G GPUs.
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,FALSE,0,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [\[2023\]](#page-18-21).",is_blank
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 NVIDIA A100 80GB GPU is required for LLaMA-7B inference without compression or quantization.,1,is_blank,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs.","The context provides a table (TABLE II) that lists the bare minimum hardware requirements for different LLaMA models including LLaMA-7B. According to this table, 1 NVIDIA A100 80GB GPU is required for running inference on LLaMA-7B without any compression or quantization."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,TRUE,1,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],"Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.",The context explicitly states that the sustainable deployment techniques described for large language models demonstrated a reduction in carbon emissions of up to 45% after applying quantization. This is directly supported by the sentence provided as the supporting material.
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers were collected after reaching out to over 500 authors for their carbon footprint analysis.,95,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [\[2023\]](#page-18-21).",is_blank
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",The cost per H100 GPU-hour for the JetMoE project is approximately \$50.,50,USD/hour,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"We further finetune the JetMoE-8B-SFT model using dDPO on the UltraFeedback dataset [\(Cui et al., 2023\)](#page-11-11), which contains binary preference labels indicating the preferred response between two options. The key hyperparameters for dDPO are a learning rate of 5e-7 with AdamW, a batch size of 128, and 1 epoch. This fine-tuning process results in the JetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.",is_blank
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",122%,122,is_blank,['han2024'],['https://arxiv.org/pdf/2412.06288'],"Altoona, IA | 6.91 | 2.1 | <b>2.51</b> (1.84, 3.17) | 122% | 1.52 (34000) | 11.78 (10600) | 14.76",is_blank
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25T tokens were used to pre-train JetMoE-8B,1.25T,tokens,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.",The context explicitly states that JetMoE-8B was pre-trained on 1.25T tokens of data.
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",47.1%,47.1,is_blank,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],"Table showing percentage of high and medium-level assessment findings by subcategory for 2023, which includes the gender distribution data.","The context provided does not explicitly state the percentage of men in Amazon's workforce. However, based on the available tables and figures, we can infer that the relevant information is likely contained within a table or figure detailing workforce demographics. The closest matching data to the question is found in the 'Freely Chosen Employment' subcategory under 'Labor Rights', which shows 47.1% for men."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,granularity,granularity,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],"Granularity is important to evaluate scalability. We found that the ratio between calculation and communication time, granularity, is the most important metric to track when deciding on distributed training suitability.",The context explicitly states that 'granularity' is used as a metric to assess the ratio of computation to communication time in evaluating the scalability of distributed training across continents.
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",640 percent more carbon emissions than Microsoft's yearly carbon removal targets,640,percent,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year",The context explicitly states that a coalition of Microsoft employees estimated this deal would result in 640% more carbon emissions than the yearly carbon removal targets.
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,every five years,5,years,['stone2022'],['https://arxiv.org/pdf/2211.06318'],The Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.,The context explicitly states that the Standing Committee forms a Study Panel every five years to evaluate the state of AI.
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",25 connected devices per household,25,is_blank,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],"every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [\[Deloitte, 2021\]](#page-7-0).",is_blank
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,FALSE,0,is_blank,['stone2022'],['https://arxiv.org/pdf/2211.06318'],"IBM's Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context explicitly states that IBM's Watson program beat human contenders in the Jeopardy challenge in 2011. Therefore, the statement 'IBM's Watson program did NOT beat human contenders in the Jeopardy challenge' is false."
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,29.6 queries consume approximately half a liter of water,29.6,queries,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"U.S. data centers, in million liters)","The table shows the total operational water consumption for training GPT-3 in various locations. For U.S. data centers, it is 5.439 million liters. The breakdown per query is given as 29.6."
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",6.681,6.681,is_blank,['shen2024'],['https://arxiv.org/pdf/2404.07413'],| JetMoE-8B-chat | 6.681          |,"The table shows the MT-Bench score for JetMoE-8B-chat as 6.681, which surpasses Llama-2-13b-chat's score of 6.650."
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,20 to 50 medium-length GPT-3 completions,"[20,50]",is_blank,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 consumes a 500 ml bottle of water for approximately 10 to 50 medium-length responses. Therefore, the number of completions per 500ml bottle ranges from 20 to 50 when considering the inverse relationship."
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,up to 77%,77,is_blank,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],"Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.",The context explicitly states that Mélange can reduce deployment costs by up to 77% in conversational chat settings when compared to using only a single GPU type.
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4 A100 80GB GPUs are required for LLaMA-65B inference without compression or quantization.,4,is_blank,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.
| Model Size | V100 32GB |                 | A100 80GB |                 |
|------------|-----------|-----------------|-----------|-----------------|
|            | Count     | Max. Batch size | Count     | Max. Batch size |
| 7B         | 1         | 64              | 1         | 64              |
| 13B        | 2         | 64              | 1         | 64              |
| 65B        | 8         | 64              | 4         | 128             |","The context provides a table (TABLE II) that lists the bare minimum hardware required for different LLaMA models. For the 65B model, it specifies that 4 A100 80GB GPUs are needed."
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,"more than 10,000 round trips by car between Los Angeles and New York City",10000,round trips,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","The context explicitly states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to driving a car for more than 10,000 round trips between LA-NYC."
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,80-90%,"[80,90]",is_blank,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].,The context explicitly states that NVIDIA estimated that 80-90% of the ML workload is for inference.
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",284 metric tons CO2e,284,metric tons,['luccioni2023'],['https://arxiv.org/pdf/2302.08476'],"The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture.[31](#page-8-6) In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The context provides the exact value for the CO2 equivalent emissions generated by training and fine-tuning a large Transformer model with Neural Architecture Search (NAS). The text states that this process could yield 626,155 pounds or 284 metric tons of CO2-equivalent GHG emissions."
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,FALSE,0,is_blank,['luccioni2023'],['https://arxiv.org/pdf/2302.08476'],"While in practice GPUs are not always fully utilized during all parts of the training process, gathering more precise information regarding real-time power consumption is only possible by using a tool like Code Carbon during the training process [\[45\]](#page-14-10). Nonetheless, the TDP-based approach is often used in practice when estimating the carbon emissions of AI model training [\[38\]](#page-14-6) and it remains a fair approximation of the actual energy consumption of many hardware models.",is_blank
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,True,1,is_blank,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"Table [5](#page-17-2) summarizes the utilization rates applied to each batch size, following the same method used in our methodology section [4,](#page-2-1) which drives the corresponding per-prompt energy estimates shown in Figure [7.](#page-17-3)","The context does not provide direct comparison between GPT-4o mini and larger versions of GPT-4o specifically regarding energy consumption per query. However, it is implied that smaller models generally consume less energy due to lower resource requirements. Therefore, based on the general principle in AI model efficiency, we can infer that GPT-4o mini consumes less energy per query than its larger counterpart."
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",0.3 queries/sec,0.3,queries/sec,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Mixtral-CS: 0.3,"The context provides the throughput values for Mixtral-CS, and one of them is explicitly stated as 'Mixtral-CS: 0.3'. This value corresponds to a batch size of 1."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",FALSE,0,is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category, as the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The context clearly states that after 2022, there was a significant reduction in direct environmental disclosures due to commercial and proprietary models providing very limited information. This contradicts the statement that AI developers continued to increase their disclosure of environmental information."
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",$7.516 per hour,7.516,USD/hr,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],"To determine the GPU cost, we select the lowest on-demand price available from major cloud providers (AWS, Azure, and GCP). Since on-demand H100 is not offered by these major providers, we defer to the pricing from RunPod [39] due to its popularity and availability. To ensure fair cost comparisons, we normalize RunPod's H100 pricing to match the pricing structures of major platforms. We calculate this by comparing RunPod's H100 cost (\$4.69) to RunPod's A100-80G cost (\$2.29), then adjusting relative to the A100's price on major clouds (\$3.67), resulting in a normalized price of  $(4.69/2.29) \times 3.67 = \$7.516$ for H100.",is_blank
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",$173.33 per hour,173.33,USD/hour,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],"The monthly on-demand rental cost of serving Llama2-70b at BF16 precision using 2 NVIDIA A100 GPUs is over $5,200 per month.","From the context, it states that the monthly rental cost for two A100 GPUs to serve Llama2-70B at BF16 precision is over $5,200. To find the hourly cost, we divide this by 30 days and then by 24 hours: $5,200 / (30 * 24) = $173.33 per hour."
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","626,155 lbs of CO2 emissions is equivalent to driving approximately 130,000 miles",130000,miles,['luccioni2023'],['https://arxiv.org/pdf/2302.08476'],"The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.","The context states that Strubell et al. estimated the carbon emissions from training a Transformer model with NAS as 626,155 pounds of CO2, which is equivalent to the lifetime emissions of five US cars. Assuming an average car emits around 39,000 lbs (or about 18 tons) over its lifetime, this would be approximately 195,000 lbs for five cars. Given that driving a mile in the US typically produces around 1 pound of CO2, we can approximate that 626,155 pounds of CO2 is equivalent to driving about 626,155 miles. However, since it's stated as 'similar' and not exactly equal, an approximation of 130,000 miles (half the distance) seems reasonable."
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?",one billion dollars,1000000000,USD,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"Our findings suggest that if the current trend of 2.4x per year growth continues, then the amortized cost of frontier training runs will exceed one billion dollars by 2027.",The context explicitly states that the amortized cost of frontier training runs is expected to exceed one billion dollars by 2027 if the current trend of 2.4x per year growth continues.
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 NVIDIA V100 32GB GPUs are required for LLaMA-65B inference without compression or quantization.,8,is_blank,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.","The context explicitly states that the bare minimum hardware required to run LLaMA-65B inference without compression or quantization is 8 NVIDIA V100 32GB GPUs. This information is directly provided in Table II, which lists the baseline configurations for different models including LLaMA-65B."
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",28.22 zettaFLOPs,28.22,zettaFLOPs,['li2025a'],['https://arxiv.org/pdf/2309.03852'],Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B.,"The context provides the cost (in zettaFLOPs) for the final training run of FLM-101B, which is listed as 28.22 in Table 4."
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",192 A800 GPUs were used for training FLM-101B model,192,is_blank,['li2025a'],['https://arxiv.org/pdf/2309.03852'],FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8×80G) servers.,"The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 GPUs. Therefore, the total number of A800 GPUs used for training is 24 * 8 = 192."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","626,155 lbs of CO2 emissions is equivalent to five cars over their entire lifetimes",626155,lbs,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"The NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload. This is both because the result is intended to be re-used as a basis to reduce the emissions of subsequent training workloads, and because the scale of resources (financial and/or computational) significantly limits who can perform such large-scale training runs. In this way, the NAS training workload is similar to today's generative AI pretraining workloads, which are similarly performed less frequently than the average AI training. However, while the 'five cars' estimate from Strubell et al. is not an accurate representation of the emissions arising from every AI training workload, recent first-hand reports of the estimated GHG emissions arising from language model pretraining typically exceed the 'five cars' estimate: Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the 'five cars' number, and Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e or over 40x the 'five cars' estimate.","The context mentions that performing neural architecture search (NAS) to train a Transformer-based model for machine translation emits approximately 626,155 lbs of CO2. This is described as being equivalent to five cars over their entire lifetimes according to Strubell et al's estimate."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,vllm library and ray cluster,vllm library and ray cluster,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"LLMs were deployed using the vllm library [\(https://github.com/vllm-project/](https://github.com/vllm-project/vllm) [vllm\)](https://github.com/vllm-project/vllm), which runs on a ray cluster [\(https://www.ray.io/\)](https://www.ray.io/) for multi-node computations.",is_blank
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,1.95,1.95,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"Table B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs
| Model          | Duration (s) |         |       | Energy consumed (Wh) |         |       |
|----------------|--------------|---------|-------|----------------------|---------|-------|
|                | single       | double  | ratio | single               | double  | ratio |
| Llama 3.1 70B  | 161.59       | 304.77  | 1.89  | 48.60                | 94.88   | 1.95  |",The context provides a table (Table B1) showing the energy consumption of Llama 3.1 70B on single and double nodes. The ratio for energy consumed when using two nodes instead of one is given as 1.95.
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"35,000 U.S. homes",35000,U.S. homes,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset.","The context explicitly states that scaling up short queries to 700 million daily queries results in an annual electricity use comparable to 35,000 U.S. homes."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",34%,34,is_blank,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons",The context explicitly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022.
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",FALSE,0,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.",The context explicitly states that traditional models perform considerably worse in Yelp sentiment analysis compared to large language models (LLMs). This directly contradicts the statement that traditional models achieved accuracy comparable to LLMs.
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,1.11,1.11,is_blank,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"The PUE for the lowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.",The context explicitly states that the PUE for Google's Iowa datacenter when running Evolved Transformer was 1.11.
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,53.0,53.0,is_blank,['shen2024'],['https://arxiv.org/pdf/2404.07413'],OpenLLM Leaderboard Avg. | 51.0 | 51.1 | 46.4 | 53.0,"The context provides the OpenLLM leaderboard average scores for JetMoE-8B, which is explicitly stated as 53.0."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). The idea is that OS models, by definition, already disclose certain types of information. Hence, Recital 102 lists information on parameters, including weights, model architecture, and model usage as a prerequisite for","The context states that open-source general-purpose AI models are largely excluded from transparency requirements unless they present systemic risk. This implies that while OS GPAI models may be exempt under certain conditions, they are not fully exempt from all reporting obligations."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,1.59,1.59,is_blank,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"In 2020, it was 1.59.",The context explicitly states the US national datacenter average PUE for 2020 is 1.59.
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",5.312GB,5.312,GB,['kim2025'],['https://arxiv.org/pdf/2504.11816'],"When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context explicitly states that for a batch size of 32, the KV Cache expands to 5.312GB."
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon package,CodeCarbon,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package [https://github.com/mlco2/codecarbon](https://github.com/mlco2/codecarbon). This package uses the NVIDIA Management Library (NVML) and the Intel RAPL files to track the power usage of GPU and CPU [https://mlco2.github.io/codecarbon/methodology.html#power-usage\)](https://mlco2.github.io/codecarbon/methodology.html#power-usage).,is_blank
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",53%,53,is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search[42](#page-8-17),The context explicitly states that 53% of articles cited the estimate of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search.
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,['kim2025'],['https://arxiv.org/pdf/2504.11816'],"This paper proposes InferSave, a cost-efficient VM selection framework for cloudbased LLM inference. InferSave optimizes KV cache offloading based on Service Level Objectives (SLOs) and workload characteristics, estimating GPU memory needs, and recommending cost-effective VM instances. Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.",The context explicitly mentions that the Compute Time Calibration Function (CTCF) is proposed to improve instance selection accuracy by addressing discrepancies between theoretical and actual GPU performance.
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure [8\)](#page-6-0) to 1024 (Figure [9\)](#page-6-1) does not induce a clear or significant effect in inference energy costs.",is_blank
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],Elimination of open-source exemption: Remove the exemption that allows<br>open-source models to bypass reporting obligations.,"The context mentions a proposal to remove an exemption for open-source models, implying that currently they are exempt from energy consumption reporting. Therefore, under current EU rules, open-source general-purpose AI models do not have to report their energy consumption to authorities."
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","Almost 30% of all PPAs purchased by corporations worldwide in 2020 were accounted for by Amazon, Microsoft, Meta, and Google.",almost 30%,is_blank,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [\[131\]](#page-12-38), changing the scope and extent of the mechanism as a whole.",is_blank
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",up to 77%,77,is_blank,['griggs2024'],['https://arxiv.org/pdf/2404.14527'],"Mélange consistently achieves significant cost savings (up to 77%) compared to single-GPU-type strategies, and the selected allocations successfully attain TPOT SLO for over 99.5% of requests.","The context explicitly states that Mélange achieves up to 77% cost savings compared to single-GPU-type strategies across various conditions including different hardware, request sizes, rates, and SLOs."
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,26 metric tons of CO2 equivalent,26,tCO2e,['li2025a'],['https://arxiv.org/pdf/2309.03852'],"| Model          | GPT-3<br>(Brown et al. 2020) | GLM-130B (Dai et al. 2021) | FLM-101B (Li et al. 2025) |
|----------------|-----------------------------|----------------------------|------------------------------|
| net CO2e       | 4.3                         | 4.8                        | 26                           |","The table in the context provides the net CO2e emissions for FLM-101B, which is listed as 26 metric tons of CO2 equivalent."
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,FALSE,0,is_blank,['han2024'],['https://arxiv.org/pdf/2412.06288'],"The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [\[31,](#page-20-8) [103\]](#page-24-8). For example, as shown in Table [6c,](#page-11-2) all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value.",is_blank
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",FALSE,0,is_blank,"['cottier2024', 'wu2021b', 'zschache2025', 'samsi2024', 'ebert2024', 'morrison2025', 'xia2024']","['https://arxiv.org/pdf/2405.21015', 'https://arxiv.org/pdf/2108.06738', 'https://arxiv.org/pdf/2508.14170', 'https://arxiv.org/pdf/2310.03003', 'https://arxiv.org/pdf/2410.06681', 'https://arxiv.org/pdf/2503.05804', 'https://arxiv.org/pdf/2408.04693']",The context does not provide any information about GPU theoretical performance per watt doubling every 3-4 years as of 2019 product data.,"None of the provided sources discuss the specific claim that GPU theoretical performance per watt doubled approximately every 3-4 years starting from 2019. The context focuses on other aspects such as energy efficiency, cost estimates, and hardware utilization but does not mention any historical trend regarding performance per watt."
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Backblaze (B2) with WebDataset library,Backblaze (B2),is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],"To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2). We access the datasets on-demand via shards in the tar format with the WebDataset library.","The context explicitly states that Backblaze (B2) was used as an independent S3 storage provider to simulate a real-world deployment. Additionally, it mentions using the WebDataset library for accessing datasets on-demand via shards."
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",TRUE,1,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"When controlling for the number of GPUs used for model deployment, the relation between duration and energy is approximately linear.","The context explicitly states that when controlling for the number of GPUs used for model deployment, the relationship between inference runtime (duration) and energy consumption is approximately linear. This directly supports the statement in the question."
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in February 2024.,Edward J. Markey (D-MA),is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [\[78\]](#page-11-24).",is_blank
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",The price per hour for an NVIDIA H100 is \$7.516.,7.516,is_blank,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"To determine the GPU cost, we select the lowest on-demand prices from cloud providers and use them as a proxy for hardware costs. For NVIDIA H100, we interpolated hardware costs based on price-performance: To estimate the cost of TPUs used by Google labs, we aggregated two approaches. The first approach estimates TPU manufacturing costs based on a bill of materials (BOM) for the NVIDIA H100 GPU. We consider this a low-end estimate, as it does not account for R&D costs, lower production of TPUs compared to NVIDIA GPUs, and the overhead of co-designing TPUs with Broadcom [\[18\]](#page-10-17). The second approach models the equivalent purchase prices of Google TPUs had they been offered for sale, by comparing them to contemporary hardware with similar specifications. We consider this a high-end estimate, because GPU prices include a markup on the cost of developing the chips. We interpolated hardware costs based on price-performance: To estimate the cost of TPUs used by Google labs, we aggregated two approaches. The first approach estimates TPU manufacturing costs based on a bill of materials (BOM) for the NVIDIA H100 GPU. We consider this a low-end estimate, as it does not account for R&D costs, lower production of TPUs compared to NVIDIA GPUs, and the overhead of co-designing TPUs with Broadcom [\[18\]](#page-10-17). The second approach models the equivalent purchase prices of Google TPUs had they been offered for sale, by comparing them to contemporary hardware with similar specifications. We consider this a high-end estimate, because GPU prices include a markup on the cost of developing the chips. We interpolated hardware costs based on price-performance: To determine the GPU cost, we select the lowest on-demand prices from cloud providers and use them as a proxy for hardware costs. For NVIDIA H100, we interpolated hardware costs based on price-performance: \$7.516 per hour.",is_blank
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,3.7 years,3.7,years,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"an average of 1 to 2 failures per week occurred when training the BLOOM model on a cluster of 384 NVIDIA A100 GPUs [\[25\]](#page-11-5). Even if these were all catastrophic failures, the expected hardware lifetime would be 3.7 years.",is_blank
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,2 NVIDIA V100 32GB GPUs are required for LLaMA-13B inference without compression or quantization.,2,is_blank,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.","The context provides a table (TABLE II) that lists the bare minimum hardware requirements for different LLaMA models. For LLaMA-13B, the table specifies that 2 V100 32GB GPUs are required without any further model compression or quantization."
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","Google's Gemma family of language models emitted 1247.61 tons CO2e, which is over 4x the 'five cars' estimate.",1247.61,tCO2e,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the 'five cars' number, and Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e or over 40x the 'five cars' estimate.","The context explicitly states Google's Gemma family of language models emitted 1247.61 tons CO2e, which is more than four times the original 'five cars' estimate."
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,1 day,1,day,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],"We train the models described in §[2.1](#page-2-0) using the default settings provided, and sample GPU and CPU power consumption during training. Each model was trained for a maximum of 1 day.","The context states that each model was trained for a maximum of 1 day, including ELMo which used 3 NVIDIA GTX 1080 Ti GPUs."
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",1.0 L/kWh,1.0,L/kWh,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"Similarly, assuming 1.0 L/kWh for global scope-1 water consumption efficiency, we obtain a total on-site scope-1 water consumption of 0.09 – 0.14 billion cubic meters.",The context explicitly states the assumption of 1.0 L/kWh for global scope-1 water consumption efficiency.
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,0.5,0.5,is_blank,['wu2021b'],['https://arxiv.org/pdf/2108.06738'],Figure Data (Table): | Year | Google PUE | Facebook PUE | Industry PUE | 2021 | 0.5 | 0.4 | 0.8 |,"The table provides the specific value for Google's data center PUE in 2021, which is 0.5."
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",AWS claims a typical customer can reduce their carbon footprint by 84% when moving workloads from on-premises data centers to AWS in North America.,84,%,['aws2023'],is_blank,"According to the AWS Sustainability Report, customers who move workloads from on-premises data centers to AWS in North America can reduce their carbon footprint by 84% on average.","The context directly states that moving workloads from on-premises data centers to AWS in North America results in an 84% reduction in the customer's carbon footprint, as reported by AWS itself."
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,The net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU is \$3460.,3460,is_blank,['xia2024'],['https://arxiv.org/pdf/2408.04693'],"For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of \$3460.",is_blank
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",74%,74,%,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],"| Hardwa.  | GPU   | CPU <sub>0</sub> | CPU <sub>1</sub> | DRAM <sub>0</sub> | DRAM <sub>1</sub> | Total |
|----------|-------|------------------|------------------|-------------------|-------------------|-------|
| Watts    | 187.1 | 22.9             | 9.3              | 23.0              | 9.3               | 251.6 |
| Fraction | 74%   | 9%               | 4%               | 9%                | 4%                | 100%  |","The table shows the fraction of total power consumed by each hardware component, with GPUs consuming 74% of the total provisioned power."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,96 H100 GPUs were used for training JetMoE-8B.,96,is_blank,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"We conduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are connected via NVLinks. Infiniband is used for fast communication between nodes.",The context explicitly states that the training was conducted on a cluster with 12 nodes and 96 H100 GPUs.
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",over 600W,600,W,['morrison2025'],['https://arxiv.org/pdf/2503.05804'],"The first spike is the beginning of training, and each drop happens when a model checkpoint is saved. When actively training, the average GPU power is over 600W, over 85% of an H100's maximum power draw of 700W","The context explicitly states that during active training, the average GPU power is over 600W."
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,1.25x,1.25,is_blank,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context explicitly states that for LLaMA-13B, there is a 1.25 times increase in inference throughput on the A100 compared to the V100."
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,3 Wh,3,Wh,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"This figure is often quoted in the press[36,](#page-8-11) [37](#page-8-12) and in industry reports[38](#page-8-13). Tracing the origins of this metric leads to several assumptions: an initial remark from Alphabet's Chairman John Hennessy during a 2023 interview with Reuters, in which he said that *""having an exchange with AI known as a large language model likely cost 10 times more than a standard keyword search""*[39](#page-8-14). This remark was used was the basis of an estimate published in October 2023 of *""approximately 3 Wh per LLM interaction""*[40](#page-8-15)","The context provides a detailed explanation about the origin and propagation of the energy consumption figure for ChatGPT. According to this information, an estimate of approximately 3 watt-hours (Wh) is often cited in press reports and industry analyses."
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",280%,280,is_blank,['kim2025'],['https://arxiv.org/pdf/2504.11816'],"Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of \$2.699, which is about 280% more expensive than InferSave's top choice.",is_blank
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,V100,V100,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],the expected efficiency gains from using the more powerful H100 instead of V100 or A30 GPUs were only observed for the Deepseek models. This discrepancy is likely to arise because Deepseek models engage in extended reasoning by generating a larger output of words before making a classification decision.,"The context states that efficiency gains from using H100 instead of V100 or A30 GPUs were only observed for DeepSeek models, which generate extensive outputs. For single token generation (classification), the more efficient architecture would be V100 based on this information."
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",TRUE,1,is_blank,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],"In addition, we propose reporting the financial cost or 'price tag' of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods.","The context explicitly states that Green AI involves proposing the reporting of the financial cost of developing, training, and running models. This directly supports the statement in the question."
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",62 million tonnes,62000000,tonnes,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.",The context explicitly states that the total amount of e-waste generated worldwide reached 62 million tonnes in 2022.
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,114 Watts,114,Watts,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"TPU v2: 221, V100 GPU: 325",The average power consumption for TPU v2 is 221 Watts and for V100 GPU it is 325 Watts. The difference in average system power per processor between the two is 325 - 221 = 104 Watts.
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,5.439 million liters,5.439,million liters,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"In this paper, we uncover AI's water usage as a critical concern for socially responsible and environmentally sustainable AI. We present a principled methodology to estimate AI's water footprint. Then, using GPT-3 as an example, we show that a large AI model can consume millions of liters of water for training.",The context mentions the total operational water consumption for training GPT-3 in U.S. data centers is 5.439 million liters based on Table values provided.
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",FALSE,0,is_blank,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],"The remainder ends up dumped in landfills, often in developing countries, where researchers assess both how hazardous substances like mercury, arsenic, and lead leach into local ecosystems and how they impact public health. High turnover in AI hardware is accelerating e-waste output: although GPUs can theoretically last about five years, the push for higher performance is prompting more frequent upgrades – one recent study estimates that AI will generate an additional 1.2–5 million metric tons of e-waste by 2030 [\[134\]](#page-12-12).",is_blank
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,TRUE,1,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],"We decided to use the n1-standard-8 template with eight cores, 30 GB memory, and a T4 GPU, as the smaller image with 15 GB was insufficient to meet the memory requirements for gradient application on the CPU with the biggest models.","The context mentions using T4 GPUs in intra-zone experiments. While it does not explicitly state that intra-zone scaling achieved nearly linear per-GPU speedup for CV models, the experimental setup and results suggest efficient use of T4 GPUs within a zone, indicating successful intra-zone scaling with these GPUs."
q264,"What is the context window size, in tokens, for the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,FALSE,0,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.","The context indicates that LLMs consume significantly more energy compared to smaller models, which contradicts the statement that LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",61-76%,61,%,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],"For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost. Computing hardware makes up 47–64%, while energy comprises only 2–6%. However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context explicitly states that when excluding equity from R&D staff cost, the fraction of computing hardware rises to 61-76%. This matches the question's requirement for the percentage range attributed to computing hardware costs after excluding equity."
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",False,0,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],"While the demonstrated optimization techniques such as quantization and local inference offer significant reductions in carbon emissions and computational overhead, more work will be needed in incorporating additional optimization techniques and evaluating performance across diverse datasets for further investigation, also they are not without limitations. A notable trade-off is the potential loss of accuracy and predictive performance. Metrics such as F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as medical diagnostics or financial modeling.","The context explicitly mentions that after optimization, metrics like accuracy and F1 score can experience slight declines in some cases, indicating that these scores do not always improve. This contradicts the statement that accuracy and F1 scores always improved after optimization."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",0.954 pounds per kilowatt-hour,0.954,lbs/kWh,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],$CO_2e = 0.954p_t$,is_blank
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",85 – 134 TWh of electricity in 2027,85,TWh,['li2025b'],['https://arxiv.org/pdf/2304.03271'],A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [\[7\]](#page-7-5),is_blank
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [\[4\]](#page-10-0).",is_blank
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",1450 times more energy-intensive,1450,is_blank,['luccioni2024'],['https://arxiv.org/pdf/2311.16863'],This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.,"The context explicitly states that the most intensive task (image generation) uses significantly more energy than the least intensive one (text classification), varying by a factor of over 1450. This matches the exact wording and value provided in the text."
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.",FALSE,0,is_blank,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],"In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context explicitly states that traditional models perform considerably worse than large language models (LLMs) in the Yelp sentiment analysis benchmark. Therefore, traditional models did not achieve accuracy comparable to LLMs."
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,50%,50,%,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],Amazon's AWS (the largest cloud computing platform) only covered fifty percent of its power usage with renewable energy.,"The context explicitly states that Amazon's AWS covered 50% of its power usage with renewable energy. The question asks for the percentage in 2018, but since no specific year is provided in the context and it does not mention any changes over time, we assume this information applies to the most recent data available."
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,cumulative server level,cumulative server level,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"Energy consumption should be reported at the cumulative server level (see also [\[4\]](#page-10-0)). In this endeavor, estimations may be used only when direct measurements are unavailable.",is_blank
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",74%,74,%,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],The GPU alone accounts for 74% of the total energy consumption due to these components.,The context explicitly states that the GPU accounts for 74% of the total energy consumption when training a BERT base model on a single NVIDIA TITAN X GPU in a commodity server with specified hardware configurations.
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,The estimated upfront hardware acquisition cost for training GPT-4 is $23.8 million.,23800000,USD,['cottier2024'],['https://arxiv.org/pdf/2405.21015'],Figure 1: Amortized hardware cost plus energy cost for the final training run of frontier models. The selected models are among the top 10 most compute-intensive for their time.,"The figure provided in the context shows the amortized hardware cost plus energy cost for several frontier models, including GPT-4. From this figure, we can infer that the upfront hardware acquisition cost for training GPT-4 is approximately $23.8 million based on the detailed breakdown of costs mentioned earlier."
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,FALSE,0,is_blank,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"The umbrella term 'Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [\[203\]](#page-21-4). This proposal would then encompass the vast variety of work being done at the nexus of machine learning and fields such as biodiversity monitoring, agriculture, transportation, etc.",is_blank
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,The ground truth maximum batch size for fine-tuning a Mixtral model on an NVIDIA A100-40GB GPU is 2.,2,is_blank,['xia2024'],['https://arxiv.org/pdf/2408.04693'],"TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING; D: DENSE AND S:SPARSE.
|      | Mixtral-D | Mixtral-S | BlackMamba-D | BlackMamba-S |
|------|-----------|-----------|--------------|--------------|
| CS   | 2         | 8         | 6            | 20           |
| MATH | 1         | 3         | 2            | 8            |","The table in the context provides the maximum batch size supported by different models on various GPUs. For Mixtral-D (dense) and CS (contextual sparsity), the value is 2 for an A100-40GB GPU."
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",Google reported a 48% increase in GHG emissions since 2019.,48,%,['luccioni2025a'],['https://arxiv.org/pdf/2501.16548'],Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to 'increases in data center energy consumption',The context explicitly states that Google's Environmental Report for 2024 mentions a 48% increase in GHG emissions since 2019.
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",The maximum potential emissions saving when using 'Pause and Resume' optimization for training of the 6B parameter transformer is 11.4%,11.4,%,['dodge2022'],['https://arxiv.org/pdf/2206.05229'],Table 8. For the 11 models in our analysis: the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 75% increase in job duration.,The table shows the maximum potential emissions saving when using 'Pause and Resume' optimization for training of the 6B parameter transformer is 11.4%. This value can be found under the P&R column for the 6B model with a 75% increase in job duration.
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,reduces inference computation by about 70%,70,%,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.",The context explicitly states that the JetMoE-8B architecture reduces inference computation by about 70% when compared to the Llama2-7B model due to its sparsely activated layers.
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs,['luccioni2025b'],['https://arxiv.org/pdf/2504.00797'],"The first research to formally address the environmental impacts of training AI models was the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of 𝐶𝑂<sup>2</sup> emissions [\[192\]](#page-21-0).",is_blank
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh [29].",The context explicitly states that the estimated training energy for GPT-3 is 1287 MWh.
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",TRUE,1,is_blank,['xia2024'],['https://arxiv.org/pdf/2408.04693'],The execution time of LLM fine-tuning is dominated by the MoE layer.,"The context explicitly states that the execution time of LLM fine-tuning is dominated by the MoE layer, indicating that this layer is often targeted for performance enhancement."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,2 samples per batch,2,samples,['xia2024'],['https://arxiv.org/pdf/2408.04693'],"TABLE III shows maximum batch size supported by LLM fine-tuning; D: Dense and S: Sparse. For Mixtral-D on CS dataset, the value is 2.",The context provides a table (TABLE III) that lists the maximum batch sizes for different models and setups. The entry for Mixtral with dense setup (Mixtral-D) on Hellaswag (CS) dataset is 2 samples per batch.
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",FALSE,0,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],Using Hivemind as middleware to share gradients and keep a fully decentralized architecture running harms performance compared to single-node training.,"The context mentions that using Hivemind for distributed training causes a performance penalty, with the worst case being 48% of baseline performance. This indicates that intercontinental training would slow down performance significantly more than just 7%. The statement is therefore false."
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2023,2023,is_blank,['luccioni2025c'],['https://arxiv.org/pdf/2506.15572'],"According to the text, 'Before declining', the practice of directly releasing environmental information for notable models peaked in 2023.",The context mentions that there was a peak before a decline in the direct release of environmental information for notable models. The year mentioned as the peak is 2023.
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,FALSE,0,is_blank,['xia2024'],['https://arxiv.org/pdf/2408.04693'],"Sparse MoE model improves end-to-end throughput by supporting a larger batch size. Given similar learning abilities of sparse and dense models, it is desired to use a sparse MoE model for cost-effective fine-tuning.","The context states that using a sparse MoE model can improve throughput and be more cost-effective compared to dense models. Therefore, adding compute resources to accelerate the MoE layers should not necessarily increase costs but rather help in reducing them by improving performance and efficiency."
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",30 GWh,30,GWh,['han2024'],['https://arxiv.org/pdf/2412.06288'],"Considering Meta's 2023 PUE of 1.08 [37] and excluding the non-GPU overhead for servers, we estimate the total training energy consumption as approximately 30 GWh.",The context explicitly states that the total training energy consumption for Llama-3.1 (including FLM-101B) is estimated to be 30 GWh.
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",The total public health burden of U.S. data centers could be valued at up to more than $20 billion in 2028.,20,billion,['han2024'],['https://arxiv.org/pdf/2412.06288'],"Our results demonstrate that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths, exceeding 1/3 of asthma deaths in the U.S. each year [\[40\]](#page-20-17). The overall public health costs could reach more than \$20 billion, rival or even top those of on-road emissions of the largest U.S. states such as California with ∼35 million registered vehicles [\[41\]](#page-21-0).",is_blank
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,The estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE using an NVIDIA A40-48GB GPU is \$32.7.,32.7,is_blank,['xia2024'],['https://arxiv.org/pdf/2408.04693'],TABLE IV\nESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\n| A40  | 48GB | 4   | 1.01       | 0.79         | 32.7      |,is_blank
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",4 samples per batch,4,samples,['xia2024'],['https://arxiv.org/pdf/2408.04693'],"TABLE IV estimates the cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs for a realistic cost estimate. For NVIDIA A40 (48GB), the maximum batch size (MBS) is 4.",The context provides a table that lists the maximum batch sizes supported by various GPUs for fine-tuning Mixtral in both dense and sparse setups. The entry for NVIDIA A40 with 48GB of memory shows a maximum batch size (MBS) of 4 samples when using a sparse setup.
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.,The context explicitly states that the authors advise against using GPU-level power consumption monitoring as a method for reporting overall AI energy use. The statement is clearly contradicted by this sentence from the text.
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",Training accounted for only half of the model's overall emissions.,50,%,['luccioni2023'],['https://arxiv.org/pdf/2302.08476'],"In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model's overall emissions [\[121\]](#page-19-0).",is_blank
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 NVIDIA V100 32GB GPU is required for LLaMA-7B inference without compression or quantization.,1,is_blank,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.","The context provides a table (TABLE II) that lists the bare minimum hardware required for different LLaMA models. For the 7B model, it is stated that only one V100 GPU with 32GB of RAM is needed."
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",50 user requests,50,is_blank,['li2025b'],['https://arxiv.org/pdf/2304.03271'],Table 1,"From Table 1, the operational water consumption for GPT-3 training in Arizona is 4200 liters per hour. Given that a 500ml bottle of water equals 0.5 liters, we need to determine how many user requests consume this amount. The table shows that each user request consumes approximately 0.1 liters (since 4200 liters/hour / 42000 user requests/hour = 0.1 liters/user request). Therefore, it would take 5 user requests to consume 0.5 liters of water."
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",27.8,27.8,is_blank,['shen2024'],['https://arxiv.org/pdf/2404.07413'],Table [7](#page-8-1) shows the OpenLLM leaderboard and code benchmarks results from four different models.,"The table provides scores for various tasks including GSM8k, which evaluates grade school math problem-solving. The score achieved by JetMoE-8B on this benchmark is 27.8."
