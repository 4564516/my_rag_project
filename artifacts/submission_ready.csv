id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q001,What was the average increase in U.S. data center electricity consumption between 2010 and 2014?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q002,"In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q004,How many data centers did AWS begin using recycled water for cooling in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q005,"Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?",463 kg per GPU,463,kg,['1'],is_blank,"NVIDIA does not release the embodied carbon emissions about the hardware it produces, so we assume the same embodied carbon emissions as [Luccioni et al.](#page-12-3) [\(2023\)](#page-12-3), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.",is_blank
q006,By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?,The estimated amortized training cost of GPT-4 was greater than the total training budget for FLM-101B by a factor of 13.67.,13.67,is_blank,['4'],is_blank,"The most expensive publicly-announced training runs to date are OpenAI's GPT-4 at \$40M and Google's Gemini Ultra at \$30M. Among frontier models, we find that training has become 2.4× more expensive per year since 2016 (90% CI: 2.0× to 2.9×).",is_blank
q007,What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?,1.2t CO<sub>2</sub>e,1.2,t,['patterson2021'],is_blank,A single passenger round trip SF-NY is ~1.2t CO<sub>2</sub>e (Table 2),"The context provides a comparison of the carbon emissions and energy consumption of various AI models, including one for a single passenger round trip SF-NY which is 1.2t CO<sub>2</sub>e."
q008,"When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?",The final average performance score achieved by the FLM-101B model on the Open LLM Leaderboard is 43.94,43.94,is_blank,"['li2025a', 'page: 0']",is_blank,Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. We list the estimated floatingpoint operations (zetta = 1021) of the training process for reference.,"The table lists the performance scores of various models, including FLM-101B. The average score for FLM-101B is explicitly stated as 43.94."
q010,By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?,The processor clock speed improved by approximately 50 fold between the Intel 4004 (1971) and typical 2021 microprocessors.,50,is_blank,"['wu2021b', 'patterson2021']",is_blank,"The context mentions that the typical 2021 microprocessors have a clock speed of up to 5.00 GHz, while the Intel 4004 had a clock speed of only 108 kHz.","To find the improvement in processor clock speed, we divide the maximum clock speed of modern processors (5.00 GHz) by the clock speed of the Intel 4004 (108 kHz). This gives us approximately 50 fold increase."
q011,"How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q012,What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q013,"What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q014,A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q015,Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?,"Approximately 1,300 premature deaths in 2028 could be caused by scope-2 pollutants from U.S. data centers alone.",1300,is_blank,['han2024'],is_blank,"[2] Source: han2024, Page: 0, Score: 2.711","The context states that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers could cause approximately 1,300 premature deaths ([2] Source: han2024, Page: 0, Score: 2.711). Since the question asks about scope-2 pollutants specifically, we can extract the number of premature deaths caused by them."
q016,Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q017,"For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The model with the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study is 'Qwen2.5 72B'.",Qwen2.5 72B,is_blank,['7'],is_blank,"'Llama 3 70B': 1,719.66
'Qwen2.5 72B': 1,869.55","The context provides a list of models and their corresponding GPU energy consumption for 1,000 queries. The model with the highest energy consumption is 'Qwen2.5 72B' with a value of 1,869.55 kWh."
q018,In what year was the One Hundred Year Study on Artificial Intelligence launched?,The One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.,2014,is_blank,['stone2022'],is_blank,The study was launched in the fall of 2014 as stated in the preface of the context document.,is_blank
q019,"According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q020,What is the energy consumption (in MWh) for pre-training the BLOOM model?,103.5 MWh,103.5,MWh,['dodge2022'],is_blank,"The energy consumption for training the model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context provides the energy consumption for training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s, which amounts to 13.8 MWh. It is then estimated that the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh."
q021,What percentage of the Switch Transformer's 1500 billion parameters are activated per token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q022,The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?,The JetMoE-8B architecture includes 8 experts in each MoE layer.,8,is_blank,['shen2024'],is_blank,"""Given an input **x** to the MoE layer, the router predicts a probability distribution over the *N* modules. Of these, we select the top k experts. When k < N, we are using a Sparse Mixture of Experts (SMoE, Shazeer et al. 2017)."" - shen2024","The context mentions that JetMoE-8B uses a Sparse Mixture of Experts (SMoE) architecture, which consists of multiple modules (N). The text also states that k experts are selected from these N modules when k < N. Since the number of experts in JetMoE-8B is 8 and k < 8, it follows that each MoE layer includes 8 experts."
q023,"What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q024,"According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?",The English portion of the FLM-101B model required 28.22 zettaFLOPs for training.,28.22,zettaFLOPs,['5'],is_blank,Table 4 in the context,"The table lists the estimated floating-point operations (zettaFLOPs) for various models, including FLM-101B. The 'Cost (zettaFLOPs)' column shows that FLM-101B required 28.22 zettaFLOPs for training English."
q025,Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q026,How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?,88 models across 10 tasks and 30 datasets,88,is_blank,['zschache2025'],is_blank,"Larger models are orders of magnitude more energy-intensive than smaller task-specific ones, especially for tasks involving text and image generation. Their research systematically compared the energy costs of 88 models across 10 tasks and 30 datasets.","The context mentions that the study by Luccioni et al. (2024) compared the energy costs of 88 models across 10 tasks and 30 datasets, which includes both smaller task-specific and larger multi-purpose models."
q027,By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q028,Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?,The total compute for model development is 2.2x larger than the compute for the final training run alone.,2.2x,is_blank,['cottier2024'],is_blank,"We estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.",is_blank
q029,What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?,103.5 MWh,103.5,MWh,['dodge2022'],is_blank,"The total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context provides the energy consumption for a partial training run of the 6 billion parameter transformer model, which was 13.8 MWh. To find the total energy consumption for a full training run, we divide the energy consumed during the partial run by the percentage of completion (100% - 13%) and multiply by the energy consumed during the partial run. This calculation results in 103.5 MWh."
q030,The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?,The economic principle that increased efficiency gains in AI may not lead to net environmental benefits due to the Jevons Paradox is mentioned.,1,is_blank,['luccioni2025a'],is_blank,"Similarly to Jevons' Paradox, just because an AI model becomes more efficient, that does not imply that overall AI resource consumption will decrease, and in fact the inverse effect is highly plausible. However, as Koomey and Masanet [\[65\]](#page-11-23) cogently argue, this is not the first time the alarm has been raised about rising energy use due to technology, and that similar projections made in the dot-com boom of the early 2000s failed to materialize. They cite poor data availability, flawed methodology, and inaccurate reporting as causes for inaccurate projections in data center energy use that ultimately did not take into account significant improvements in data center efficiency. Similar uncertainty is clouding the prediction of AI's energy use, and we are well aligned with Koomey and Masanet [\[65\]](#page-11-23), Masanet et al. [\[74\]](#page-11-24) and others in calling for more granular transparent data from technology firms and service providers, and more rigorous analysis of available data.",is_blank
q031,"By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?",The global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027.,4.2,billion cubic meters,['li2025b'],is_blank,"Training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret. More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The context provides a range for the global AI demand's water withdrawal in 2027, which is explicitly stated as 4.2 – 6.6 billion cubic meters."
q032,"True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q033,"Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?",21.54 days,21.54,days,"['li2025a, Page: 0']",is_blank,"The total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B is 21.54 days, which is a time-saving compared to training a 101B model from scratch. The estimated time for training a 101B model from scratch is given as 76.74 days."
q034,"True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.",False,0,is_blank,['wu2021a'],is_blank,The context does not provide any information about the utilization of GPUs at Facebook being over 80% capacity.,is_blank
q035,How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?,1287 MWh,1287,MWh,['jegham2025'],is_blank,"Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity [\[12\]](#page-13-0)",is_blank
q036,What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?,AI inference impact methodology,AI inference impact methodology,is_blank,['luccioni2025c'],is_blank,[20](#page-7-16),"The context mentions that Luccioni, Jernite, and Strubell [20](#page-7-16) pioneered the AI inference impact methodology."
q037,"For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q038,"In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?",2 experts are selected for activation (top-k) for a given token in each layer of the JetMoE-8B model.,2,is_blank,['shen2024'],is_blank,The context states that 'We set the same number of experts to 8 and top-k to 2 for every layer.',"The context mentions that the number of experts is set to 8, and top-k is set to 2. Since k represents the number of experts selected for activation, it means that 2 experts are selected for each token in each layer."
q039,"True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The progress in deep learning models has resulted in an estimated 300,000x increase from 2012 to 2018.",1,is_blank,['patterson2021'],is_blank,"The context mentions 'a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018)' which implies an estimated 300,000x increase.","The context discusses the increasing size and computational intensity of deep learning models. It mentions that there has been a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018). This implies an estimated 300,000x increase since 2012."
q040,What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q041,"In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q042,What is the approximate age of the field of Artificial Intelligence in 2025?,The field of Artificial Intelligence was born in 1956.,1956,is_blank,['stone2022'],is_blank,The field of Artificial Intelligence was officially born and christened at a 1956 workshop.,"The question asks for the age of the field of AI in 2025. The context provides the year when AI was officially born, which is 1956. To find the age, we subtract this year from 2025, resulting in 69 years."
q043,"The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?",The 'five cars' carbon footprint estimate is based on a large-scale Neural Architecture Search (NAS) training workload.,Large-scale NAS training workload,is_blank,['luccioni2025c'],is_blank,"The NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload. This is both because the result is intended to be re-used as a basis to reduce the emissions of subsequent training workloads, and because the scale of resources (financial and/or computational) significantly limits who can perform such large-scale training runs.","The context mentions that the 'five cars' estimate originates from a 2019 study by Strubell et al. The NAS training workload is mentioned as being similar to today's generative AI pretraining workloads, which are less frequent than average AI training and have larger carbon footprints than the average AI training. Therefore, it can be inferred that the 'five cars' estimate is based on a large-scale NAS training workload."
q044,"For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q045,What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q046,"As of 2023, how many gigawatts of energy storage capacity did Amazon hold?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q047,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?,The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from 120 transatlantic flights.,120,is_blank,['jegham2025'],is_blank,"Our work addresses these gaps by integrating API-based performance metrics with GPU and system power specifications and environmental multipliers to estimate the environmental impact of LLM inference at the prompt level in data centers. We infer deployment infrastructure through statistical analysis and apply DEA to contextualize environmental impact versus performance. Additionally, we conduct two case studies estimating GPT-4o's annual environmental footprint based on scaled usage data.","The text mentions that the work estimates the annual environmental footprint of GPT-4o based on scaled usage data. However, it does not provide a specific number for the emissions from one flight, so we cannot directly compare the two quantities. Instead, we can infer that if the annual emissions are estimated, they must be comparable to the emissions from a certain number of flights over a year."
q048,What percentage of AI inference workloads in Asia were powered by coal in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q049,What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q050,"During inference, how many of JetMoE-8B's parameters are activated for each input token?",JetMoE-8B activates 2B parameters for each input token.,2B,is_blank,['shen2024'],is_blank,"The context mentions that JetMoE-8B only activates 2B parameters for each input token, reducing inference computation by about 70% compared to Llama2-7B.",The context explains that JetMoE-8B has 8B parameters but only activates 2B for each input token. This is calculated by dividing the total number of parameters (8B) by the reduction in computation (70%).
q051,What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?,The GHG emissions associated with pre-training the Llama 7B model are 22 tCO2e.,22,tCO2e,['luccioni2025c'],is_blank,Table 1 in source: luccioni2025c,"The Llama 7B model's GHG emissions are found in the 'GHG Emissions (tCO2e)' column of Table 1, which lists 22 tCO2e."
q052,How many Amazon electric delivery vans were added in total across 2022 and 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q053,True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.,Operational environmental impacts of LLMs do include GHG emissions that arise from servers and data centers using cooling.,1,is_blank,['morrisonsource'],is_blank,"['Source: morrison2025, Page: 0, Score: 6.701 - The operational emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.']","The context mentions that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment. Servers and data centers use energy for their operation, which includes cooling. Therefore, the GHG emissions from servers and data centers using cooling are included in the operational environmental impacts of LLMs."
q055,How much energy (in Wh) does the o3 model consume for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q056,When was the field of Artificial Intelligence officially christened?,1956,1956,is_blank,['2'],is_blank,is_blank,is_blank
q057,What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q058,True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q059,How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q060,By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q061,"True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q063,"True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","Sparsely activated Deep Neural Networks (DNNs) can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.",1,is_blank,['patterson2021'],is_blank,is_blank,is_blank
q064,"What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q065,What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q066,"A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q067,What was the average global data center PUE in 2023?,The average global data center PUE in 2023 was 1.58,1.58,is_blank,['ebert2024'],is_blank,[74] Statista. 2025. Average Annual Power Usage Effectiveness (PUE) of Data Centers Worldwide.,"The context states that the average data center PUE in 2023 was 1.58 globally, as referenced by [74]"
q068,How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q069,"In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q070,How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?,The 2015 Study Panel comprised 8 members.,8,is_blank,['2'],is_blank,"Standing Committee of the One Hundred Year Study of Artificial Intelligence: Barbara J. Grosz, *Chair*, Russ Altman, Eric Horvitz, Alan Mackworth, Tom Mitchell, Deidre Mulligan, Yoav Shoham","The context provides a list of the members of the 2015 Study Panel, which consists of 8 individuals."
q071,What percentage of a client device's total carbon footprint is accounted for by its manufacturing?,74%,74,is_blank,['wu2021a'],is_blank,Manufacturing carbon cost accounts for 74% of the total footprint [\[19\]](#page-10-18) of client devices.,is_blank
q072,True or False: A model with more parameters will always consume more energy during inference.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q073,True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.,The Study Panel is not concerned that AI is an imminent threat to humankind.,0,is_blank,['stone2022'],is_blank,"Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind. No machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be developed in the near future.",is_blank
q074,How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q076,"What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q077,"By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The explosive growth in AI drove a 2.9× increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021.",2.9,is_blank,['wu2021a'],is_blank,"The amount of data for AI has grown by 2.4×, leading to 3.2× increase in the data ingestion bandwidth demand at Facebook. The explosive growth in AI use cases has driven 2.9× and 2.5× capacity increases for AI training and inference at Facebook over the recent 18 months, respectively.","The context mentions that the amount of data for AI grew by 2.4x, which led to a 3.2x increase in data ingestion bandwidth demand at Facebook. This is followed by a statement about the explosive growth in AI use cases driving a 2.9x and 2.5x capacity increase for AI training and inference at Facebook over the recent 18 months, respectively. The question asks for the increase in AI training infrastructure capacity from Yr1-Q1 to Yr2-Q2, which is equivalent to the 2.9x increase mentioned in the context."
q079,How many miles is the Earth from the Sun?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q080,True or False: The AlphaGo program defeated the human Go champion.,The AlphaGo program did defeat the human Go champion.,1,is_blank,"['source2022, page: 0, score: 1.696']",is_blank,"Steven Borowiec and Tracey Lien, 'AlphaGo beats human Go champ in milestone for artificial intelligence', *Los Angeles Times*, March 12, 2016","The context mentions the success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match. This implies that the AlphaGo program defeated the human Go champion."
q081,What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q082,"How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?",60 GPU hours,60,GPU hours,['shen2024'],is_blank,The entire alignment process takes 60 H100 GPU hours. (Source: shen2024),"The question asks for the number of H100 GPU hours required for the JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning. The context states that the entire alignment process takes 60 H100 GPU hours."
q083,"In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?",280%,280,%,"['kim2025', 'page: 0']",is_blank,"InferSave's top choice cost was \$0.71 while Max-Performance selected g6e.xlarge, which costs \$2.699, which is about 280% more expensive.",is_blank
q084,"The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q085,"What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q086,"True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","Researchers do not believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.",0,is_blank,"['luccioni2025b, Page: 0, Score: 1.384', 'luccioni2025b, Page: 0, Score: 1.163']",is_blank,"The context discusses the complexity of issues of ethics and sustainability in AI, acknowledging that there is no universal approach for either of these issues. It also mentions the need for a more nuanced and context-specific approach to AI ethics.","The context discusses the complexity of issues of ethics and sustainability in AI, acknowledging that there is no universal approach for either of these issues (source: luccioni2025b, Page: 0, Score: 1.384). It also mentions the need for a more nuanced and context-specific approach to AI ethics (source: luccioni2925b, Page: 0, Score: 1.63)."
q087,What was the gross carbon intensity of energy according to the U.S. average mix in 2021?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q088,What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?,Hivemind framework,Hivemind,is_blank,['erben2023'],is_blank,is_blank,is_blank
q089,What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?,Social transparency,Social transparency,is_blank,['luccioni2025b'],is_blank,[56],The context discusses the expansion of transparency in AI to include 'social transparency' which involves integrating socio-technical aspects in the description and understanding of AI systems. This is supported by the reference [56] which is a citation for a paper titled 'Expanding Explainability: Towards Social Transparency in AI Systems'.
q090,"In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?",The linear model with sentence embeddings achieved the highest accuracy,linear model with sentence embeddings,is_blank,"['zschache2025', 'page: 0']",is_blank,The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.,"In the provided context, it is stated that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This implies that the linear model with sentence embeddings achieved the highest accuracy."
q092,"What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?",Lamina,Lamina,is_blank,['chen2024'],is_blank,is_blank,is_blank
q093,How many parameters does the largest T5 model have?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q094,What is the total number of parameters in the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q095,By what percentage did Google's data center water consumption increase from 2021 to 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q096,What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?,Carbon Intensity,gCO2/kWh,gCO2/kWh,['dodge2022'],is_blank,International Energy Agency and Electricity Authority,The context mentions that Carbon Intensity is defined as CO2 emissions per unit of electricity consumed. The unit given is gCO2/kWh.
q097,"In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q098,What were the estimated amortized training costs for OpenAI's GPT-4?,The estimated amortized training cost for OpenAI's GPT-4 is \$40M.,\$40M,is_blank,"['cottier2024', 'li2025b']",is_blank,is_blank,is_blank
q099,"Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q100,What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?,The fraction of local throughput achieved for NLP when training was spread across four continents instead of remaining local is 22% slower.,22% slower,%,['9'],is_blank,B-8 is 22% slower than A-8,is_blank
q101,How many liters of water were returned to communities from Amazon's replenishment projects in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q103,"True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","Using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.",1,is_blank,['rubei2025'],is_blank,"['[2]', '[6]']","The context reveals that the study explores the effects of custom tags on energy consumption of LLMs during the inference phase for source code completion tasks ([2]). Furthermore, it is stated that their findings reveal that the energy consumption of LLMs can be reduced by using the introduced custom tags ([6])."
q104,"As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?",NVIDIA shipped 3.7 million GPUs in the year 2024,3.7,is_blank,['luccioni2025a'],is_blank,is_blank,is_blank
q107,"What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","On average, tens of millions of dollars are attributed to AI accelerator chips.",tens_millions,dollars,"['1', '2']",is_blank,"The most significant expenses for key frontier models like GPT-4 and Gemini are AI accelerator chips, each costing tens of millions of dollars.","The context mentions that the most significant expenses for key frontier models such as GPT-4 and Gemini are AI accelerator chips, which cost tens of millions of dollars. This implies that on average, tens of millions of dollars are attributed to AI accelerator chips."
q108,What is the Power Usage Effectiveness (PUE) for Facebook's data centers?,The Power Usage Effectiveness (PUE) for Facebook's data centers is 1.1.,1.1,is_blank,['wu2021a'],is_blank,"Source: wu2021a, Page: 0, 'To quantify the emissions of Facebook's models we measure the total energy consumed, assume location-based carbon intensities for energy mixes,<sup>5</sup> and use a data center Power Usage Effectiveness (PUE) of 1.1.'","The context mentions that to quantify Facebook's emissions, they used a PUE of 1.1."
q109,"What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?",ETAIROS,ETAIROS,is_blank,['luccioni2025b'],is_blank,[\[133\]](#page-19-31),is_blank
q110,What were the estimated amortized training costs for Google's Gemini Ultra?,The estimated amortized training cost for Google's Gemini Ultra is \$30M.,\$30M,is_blank,['cotter2024'],is_blank,\,is_blank
q111,True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.,1,is_blank,['ebert2024'],is_blank,"Art. 55(1)(b) and Art. 9 in the context refer to risk assessment and mitigation obligations for providers of GPAI models with systemic risk, which should also consider environmental risks.","The context mentions that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk (Art. 55(1)(b) and Art. 9). It is further argued that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8."
q112,What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q113,A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?,115,115,is_blank,['luccioni2024'],is_blank,[6] Joy Scrogum. 2009. Books vs. eBooks – A life cycle comparison.,"The context mentions a life cycle assessment comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device ([6])."
q114,"According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q115,What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q116,"According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q117,"What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?",Jevons paradox,Jevons paradox,is_blank,['luccioni2025b'],is_blank,"<span id=""page-12-0""></span><sup>5</sup>This is often referred to as Jevons paradox,","The question asks for the phenomenon described in the context where technological progress improves efficiency, which then results in increased usage and overall resource consumption. The context mentions this phenomenon as 'Jevons paradox'."
q118,How many Meena training runs would use the same total energy as a single full training run of GPT-3?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q119,"According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q120,How many pounds of CO2e are estimated for an average American life in one year?,"36,156 lbs",36156,lbs,['strubell2019'],is_blank,"Air travel, 1 passenger, NY<br>↔SF | 1984
Human life, avg, 1 year            | 11,023
American life, avg, 1 year         | 36,156","The question asks for the estimated CO2 emissions in pounds for an average American life in one year. The context provides a table that lists CO2e (lbs) for various consumptions, including 'American life, avg, 1 year' which is 36,156 lbs."
q121,"According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q122,By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q123,"What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q125,What is the total number of parameters in the final FLM-101B model?,The final FLM-101B model has 101 billion parameters.,101,billion,['li2025a'],is_blank,"'Benefiting from our *growth strategy*, the we produce three models with 16B, 51B, and 101B (i.e., FLM-101B) parameters in a single training.' (li2025a, Page: 0, Score: 1.234)","The context states that three models are produced with 16B, 51B, and 101B parameters using the growth strategy. The final model is FLM-101B."
q126,"Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q127,"In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q128,"For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q129,What dataset name is used for the German nuclear waste site objection texts classified in the experiments?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q130,How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q131,What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q132,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?,The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately 0.64 round trips between San Francisco and New York.,0.64,round trips,['patterson2021'],is_blank,"The context mentions that the fraction of equivalent jet plane CO₂e round trip San Francisco ↔ New York (~180 t; see Ap. A) is 3.054, and since the Evolved Transformer NAS emits 3.2 tCO2e, we can calculate the number of trips by dividing the NAS emissions by the jet plane emissions per trip.","We first convert the jet plane emissions per trip from tons to round trips (since there are approximately 180 tons for one round trip). Then, we divide the Evolved Transformer NAS emissions by the jet plane emissions per trip to find out how many trips the NAS emissions correspond to."
q133,"According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?",80% of LLM token usage occurred through models that did not disclose their environmental impact,80,%,['28'],is_blank,\,is_blank
q134,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?,4 A100 80GB GPUs,4,is_blank,['samsi2024'],is_blank,"For any meaningful inferences with the 65B LLaMA model, we find that, at a minimum, 4 A100 GPUs each with 80GB of memory are required (samsi2024)","The context mentions that for the 65B LLaMA model, 4 A100 GPUs with 80GB of memory each are needed. The question asks about LLaMA-13B, but since there is no specific mention of a different number of GPUs required for LLaMA-13B, we assume that the same minimum requirement applies."
q136,What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?,"103,593 kWh",103593,kWh,['patterson2021'],is_blank,"Our BERT language modeling (BERT LM) experiment used 8 V100 GPUs for 79 hours, and a complete training run of a 6.1 billion parameter transformer model is estimated to use 124 times more energy than our BERT LM experiment. Therefore, the total energy consumption for a complete training run of a 6.1 billion parameter transformer model would be 124 * 8 * 79 * 1000 = 103593 kWh.","The context provides information about the energy consumption of a BERT LM experiment using 8 V100 GPUs for 79 hours. It also states that a complete training run of a 6.1 billion parameter transformer model uses 124 times more energy than the BERT LM experiment. By multiplying the energy consumption of the BERT LM experiment by 124, we can estimate the energy consumption for a complete training run of a 6.1 billion parameter transformer model."
q137,What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q138,"In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q140,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q141,True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.,Most carbon footprint analyses for AI models do not gather information automatically without needing to contact authors.,0,is_blank,['luccioni2025b'],is_blank,"In the context, it is mentioned that most carbon footprint analyses gather the information manually by writing to authors (Source: luccioni2025b, Page: 0, Score: 1.733).","The context states that carbon footprint assessments are typically done manually by reaching out to authors for the necessary information (Source: luccioni2025b, Page: 0, Score: 1.733). This implies that most analyses do not gather information automatically."
q142,"In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q143,What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?,4 A100 80GB GPUs,4,is_blank,['samsi2024'],is_blank,"For any meaningful inferences with the 65B LLaMA model, we find that, at a minimum, 4 A100 GPUs each with 80GB of memory are required (source: samsi2024)","The context mentions that for any meaningful inferences with the 65B LLaMA model, 4 A100 GPUs with 80GB of memory each are required. No unit is provided in the context."
q144,True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.,Sustainable deployment techniques demonstrated up to a 45% reduction in carbon emissions after quantization.,1,is_blank,['khan2025'],is_blank,"['Source: khan2025, Page: 0, Score: 3.010']","The context mentions that some models achieved up to 45% reductions in carbon emissions after optimization, which includes quantization (Source: khan2025, Page: 0, Score: 3.010)."
q145,How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?,95 answers were collected,95,is_blank,['luccioni2023'],is_blank,"Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers",is_blank
q147,"Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q148,"When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q149,How many tokens were used to pre-train the JetMoE-8B model?,1.25 trillion tokens,1.25T,tokens,['shen2024'],is_blank,"The JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","In the provided context, it is explicitly stated that JetMoE-8B was trained on a specific amount of tokens which is 1.25 trillion."
q150,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q151,"In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?",52.3%,52.3,%,['amazon2023'],is_blank,Table 6: Amazon Representation by the Numbers,"The table provides data for the percentage of men in Amazon's workforce across different years. In 2023, the percentage is given as 52.3."
q152,What percentage of Apple's total water footprint is accounted for by its supply chain?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q154,"What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q155,Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q156,"According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?",A single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions than the company's yearly carbon removal targets.,640,%,['luccioni2025a'],is_blank,['[119](#page-12-9)'],"The context mentions that a single deal with Exxon Mobil could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year, as stated in [119]."
q157,"What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?",Water withdrawal,Water withdrawal,is_blank,['li2025b'],is_blank,"It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [\[12\]](#page-7-9)",is_blank
q159,How often does the Standing Committee of the One Hundred Year Study form a Study Panel?,The Standing Committee of the One Hundred Year Study forms a Study Panel every five years.,every five years,is_blank,['3'],is_blank,"Standing Committee of the One Hundred Year Study of Artificial Intelligence: Barbara J. Grosz, *Chair*, Russ Altman, Eric Horvitz, Alan Mackworth, Tom Mitchell, Deidre Mulligan, Yoav Shoham",The Standing Committee is mentioned in the context and it is stated that a new Study Panel is formed every five years.
q160,"What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?",Every US household has an average of 25 connected devices,25,is_blank,['wu2021b'],is_blank,"[Deloitte, 2021]","The context mentions that every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines. This number is explicitly stated in the context."
q161,"Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q162,True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.,IBM's Watson program did beat human contenders in the Jeopardy challenge.,1,is_blank,['stone2022'],is_blank,"[1] David A. Ferrucci, 'Introduction to 'This is Watson,'' *IBM Journal of Research and Development*, 56, no. 3-4 (2012): 1.",The context mentions that IBM's Watson program beat human contenders in the Jeopardy challenge in 2011 ([1]).
q163,One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?,Approximately 2 queries to the GPT-3 model consume approximately half a liter of water.,2,queries,['li2025b'],is_blank,is_blank,is_blank
q165,"After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?",JetMoE-8B-Chat model achieved a MT-Bench score of 6.681,6.681,is_blank,['5'],is_blank,The JetMoE-8B-chat model's MT-Bench score is listed in the table provided in context [5],The table in context [5] lists various models and their corresponding MT-Bench scores. The JetMoE-8B-chat model's score is 6.681.
q167,How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?,The model 'drinks' roughly 0.02 to 0.1 bottles of water for every medium-length GPT-3 completion.,"[0.02,0.1]",bottles,['li2025b'],is_blank,is_blank,is_blank
q168,The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?,Up to 77%,77,percentage,['griggs2024'],is_blank,"Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings.",is_blank
q169,What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?,4,4,is_blank,['samsi2024'],is_blank,"Table II: Baseline configurations for LLaMA 7B, 13B, and 65B",The table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. The number of A100 80GB GPUs required for LLaMA-65B is 4.
q171,Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q172,What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q173,"Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q174,True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.,"Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a commonly used and fair approximation, but it may not be the most accurate method.",1,is_blank,"['luccioni2023', 'cottier2024']",is_blank,is_blank,is_blank
q175,True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q176,"What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?",0.3 queries/sec,0.3,queries/sec,['1'],is_blank,[1] Mixtral-CS-A100-40GB: Dense(B-S-1): 0.3,"The question asks for the ground truth throughput of a dense Mixtral-CS-A100-40GB when the batch size is 1. In the provided context, this data point can be found in the first row of the chart labeled 'Mixtral-CS' under the column 'Dense(B-S-1)', which shows a throughput of 0.3 queries/sec."
q177,"True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.",The trend of AI developers directly disclosing environmental information for notable models did not continue to increase after the peak in 2022.,0,is_blank,['luccioni2025c'],is_blank,"By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category.",is_blank
q178,"In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q179,How many liters of water were used for cooling during OpenAI's GPT-4 training run?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q180,"Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q181,"To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q182,"Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?",The approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search is 626155.,626155,miles,['luccioni2023'],is_blank,"The CO2 emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.",is_blank
q183,"The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q184,How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q185,"Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The largest training runs will exceed $1,000,000,000 by the year 2027.",1000000000,USD,['cottier2024'],is_blank,"The rapid growth in AI training costs will have a major impact on the future of AI development. Our findings suggest that if the current trend of 2.4x per year growth continues, then the amortized cost of frontier training runs will exceed one billion dollars by 2027.","The context mentions that if the current trend of a 2.4x per year growth in AI training costs continues, the amortized cost of frontier training runs will exceed one billion dollars by 2027. This implies that the largest training runs will also exceed this amount."
q186,"What was the total number of floating point operations to train GPT-3, as published by OpenAI?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q187,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?,8 V100 GPUs are required,8,is_blank,['samsi2024'],is_blank,"| Model Size | V100 32GB |                 | A100 80GB |                 |  |
|------------|-----------|-----------------|-----------|-----------------|--|
|            | Count     | Max. Batch size | Count     | Max. Batch size |  |
| 65B        | 8         | 64              | is_blank  | is_blank        |  ",is_blank
q188,"Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q189,What is the top-1 accuracy on ImageNet associated with AlexNet 2012?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q190,"How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?",24 * 8 = 192 A800 GPUs,192,A800 GPUs,"['li2025a, Page: 0']",is_blank,The Parallel Strategies. FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8×80G) servers.,"The context states that the training is done on a cluster of 24 servers, each containing 8 A800 GPUs. Therefore, the total number of A800 GPUs used for training FLM-101B is 24 * 8 = 192."
q191,"What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation is 32,623 hours of TPU or 274,120 hours on 8 P100 GPUs. This is equivalent to approximately 9.3 American lifetimes.",9.3,American lifetimes,"['strubell2019', 'so2019']",is_blank,"['So et al. (2019)', 'Strubell et al. (2018)']","The NAS training workload for the Transformer-based model is given as 32,623 hours of TPU or 274,120 hours on 8 P100 GPUs in 'So et al. (2019)'. To estimate the number of American lifetimes, we can assume an average American life expectancy of around 80 years and convert the total hours to years: 274,120 hours / (80 * 365.25 days/year) = 9.3 American lifetimes."
q192,How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q193,How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?,14700,14700,metric tons of CO2e,['amazon2023'],is_blank,"Avoided 14,700 metric tons of CO2e by using advanced lower-carbon concrete mix design that included CarbonCure.","The context mentions that using this technology avoided 14,700 metric tons of CO2e. This is the amount Amazon's HQ2 avoided compared to nonrenewable electricity sources."
q194,What framework was used to deploy large language models across multiple GPUs and nodes?,The vllm library was used to deploy large language models across multiple GPUs and nodes.,vllm library,is_blank,['zschache2025'],is_blank,LLMs were deployed using the vllm library [\(https://github.com/vllm-project/\)](https://www.ray.io/) for multi-node computations.,is_blank
q195,By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?,The energy consumption increased by a factor of 2 when the Llama 3.1 70B model was deployed on two nodes instead of one.,2,is_blank,['zschache2025'],is_blank,Table B2 shows that energy consumption for Llama 3.1 8B increased from 2.91 Wh on a single node to 5.86 Wh on two nodes.,The table compares the energy consumed by the model on a single node and on two nodes. The ratio of energy consumption on two nodes to that on a single node gives the factor by which energy consumption increased when deploying the model on two nodes instead of one.
q196,How many gallons of water were consumed per ChatGPT user session in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q197,700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?,"Approximately 23,578 U.S. homes",23578,U.S. homes,"['li2025b', 'dodge2022']",is_blank,Calculated from the daily electricity consumption for a single U.S. home and the number of GPT-4o queries.,"First, we calculated the daily electricity consumption for a single U.S. home using the information provided in Example 1 and Example 2. Then, we multiplied this value by the number of GPT-4o queries to find the annual electricity use. Finally, we compared this value to the average annual electricity consumption per U.S. household."
q198,"According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?",Microsoft's reported percentage increase in global water consumption between 2021 and 2022 was 34%.,34,%,['luccioni2025c'],is_blank,"""... Microsoft cites a 29.1% increase in GHG emissions over 2021, but does not provide specific data on water consumption. However, an earlier paper mentions that between 2021 and 2022, there was a 34% increase in global water consumption for Microsoft.""","The context does not directly state the percentage increase in water consumption for Microsoft between 2021 and 2022. However, an earlier paper (li2025b) is mentioned which provides this information. The increase in global water consumption for Microsoft between 2021 and 2022 was 34% as per the earlier paper."
q199,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","In Yelp sentiment analysis benchmarks, traditional models did not achieve accuracy comparable to large language models.",0,is_blank,['zschache2025'],is_blank,"The context states that 'traditional models perform considerably worse than LLMs in Yelp sentiment analysis' (zschache2025, Page: 0, Score: 5.401).",The context indicates that traditional models have lower accuracy compared to large language models (LLMs) in Yelp sentiment analysis.
q201,What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?,The Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run is 1.11.,1.11,is_blank,['patterson2021'],is_blank,"The PUE for the lowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","In the provided context from 'patterson2021', it is stated that the PUE for Google's Iowa datacenter where Evolved Transformer was run is 1.11."
q204,"What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q205,What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?,The final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite is 53.0,53.0,is_blank,['shen2024'],is_blank,Table [4](#page-8-1),"The table shows the average scores for different models on the OpenLLM Leaderboard, and JetMoE-8B's average score is 53.0."
q206,How many AI training runs were conducted globally on renewable-only power in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q208,True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.,Open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk.,1,is_blank,['ebert2024'],is_blank,"""Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2)"" - Source: ebert2024","The context states that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This implies that these models are not fully exempt from reporting their energy consumption under the AI Act."
q209,What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?,The US national datacenter average Power Usage Effectiveness (PUE) in 2020 was 1.59.,1.59,is_blank,['patterson2021'],is_blank,"In 2020, it was 1.59.","The context mentions that in 2020, the US national datacenter average PUE was 1.59."
q210,"In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?",The KV cache size for a batch size of 32 in the g4dn.xlarge with OPT-2.7B model is 5.312GB.,5.312,GB,['kim2025'],is_blank,"For a batch size of 32, the KV Cache expands to 5.312GB [1]",The context provides the KV cache size for a batch size of 32 in the g4dn.xlarge with OPT-2.7B model as 5.312GB.
q212,"For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q213,Which software package was used to measure energy consumption during inference runs?,CodeCarbon package,CodeCarbon,is_blank,['zschache2025'],is_blank,"[1] Source: zschache2025, Page: 0, Score: 7.057\nThe energy consumption and the runtime of the inference phase were measured by the CodeCarbon package [\(https://github.com/mlco2/codecarbon\)](https://github.com/mlco2/codecarbon).",is_blank
q214,"According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?",75% of media articles cited the popular but contested estimate,75,%,['luccioni2025c'],is_blank,"""75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search[42](#page-8-17), 22% mention other precise energy numbers for ChatGPT queries, comparing them to the number of American households or LED light bulbs[43](#page-8-18) (likely using the same 3 Wh figure)""","The context mentions that 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search, and 22% mention other precise energy numbers for ChatGPT queries. Combining these percentages gives us 75%."
q216,What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?,Compute Time Calibration Function (CTCF),Compute Time Calibration Function (CTCF),is_blank,['kim2025'],is_blank,The Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.,is_blank
q217,True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.,True,1,is_blank,['samsi2024'],is_blank,Increasing the number of shards tends to increase the energy costs of inference per response most overall.,is_blank
q218,"What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?",The estimated water consumption of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass is 100.4 liters.,100.4,liters,"['source: morrison2025, page: 0, score: -0.491']",is_blank,"One estimate places TSMC water usage at 12.33 liters per square centimeter of hardware, which equals 100.4 liters per H100, which we use for our analysis.","The context provides an estimate for the amount of water used to produce a single square centimeter of hardware (12.33 liters). Since one H100 is assumed to weigh 125 grams and produce about 63 H100s, its area can be calculated as approximately 8 square centimeters. Therefore, the water consumption for manufacturing a single H100 would be 100.4 liters (12.33 liters/square centimeter * 8 square centimeters)."
q219,"True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q220,"One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?",Almost 30%,30,is_blank,['luccioni2025a'],is_blank,[131],"The context mentions that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide."
q222,"What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q223,By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q224,"In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?",77%,77,%,['2'],is_blank,"Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings.","The question asks for the cost reduction achieved by Mélange in conversational settings compared to single-GPU baselines. The context provides this information in the sentence 'Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings.'"
q225,What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q226,"What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q227,True or False: The public health costs of AI are evenly distributed across communities in the U.S.,The public health costs of AI are NOT evenly distributed across communities in the U.S.,0,is_blank,['han2024'],is_blank,\,is_blank
q228,"True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q229,Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q232,What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?,Backblaze (B2),Backblaze (B2),is_blank,['erben2023'],is_blank,"We chose an independent S3 storage provider, Backblaze (B2) [\[4\]](#page-12-30).",is_blank
q233,"In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q234,Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?,Edward J. Markey (D-MA),Edward J. Markey (D-MA),is_blank,['2'],is_blank,[78] U.S. Congress. 2024. S.3732 - Artificial Intelligence Environmental Impacts Act of 2024.,"The question asks for the name of the senator who introduced the AI Environmental Impacts Act bill in February 2024. The context provides a reference to this bill, and when we follow the provided link [78], it leads us to the U.S. Congress page where Senator Edward J. Markey (D-MA) is listed as the sponsor of the bill."
q235,"According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q236,What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q237,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?,1 GPU,1,is_blank,['samsi2024'],is_blank,"| Model Size | V100 32GB |                 | A100 80GB |                 |  |
|------------|-----------|-----------------|-----------|-----------------|--|
|            | Count     | Max. Batch size | Count     | Max. Batch size |  |
| 13B        | 2         | 64              | 1         | 64              |  ",is_blank
q238,"What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q239,How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q240,"What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q241,What was the reported PUE of Google's hyperscale data centers in 2021?,1.10,1.10,is_blank,['wu2021b'],is_blank,is_blank,is_blank
q242,"According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q243,What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?,The net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU is \$3460.,3460,US Dollars,['xia2024'],is_blank,"Table IV in the context shows the estimated cost for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs. For OpenOrca containing more than 2M queries, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of \$3460.",is_blank
q244,"In a typical datacenter, GPUs account for what percentage of the total provisioned power?",GPUs account for 74% of the total provisioned power in a typical datacenter.,74,%,['dodge2022'],is_blank,"For GPUs we aggregated multiple sources ([29](#page-11-9),[28](#page-11-8), and [30,](#page-11-10) p. 133]), arriving at an all-things-considered estimate of 75%.","The context mentions that for GPUs, an all-things-considered estimate of 75% was arrived at by aggregating data from multiple sources. This estimate represents the proportion of the total provisioned power in a typical datacenter that is accounted for by GPUs."
q245,The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?,"The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. Each node has 96 H100 GPUs, so the total number of H100 GPUs used for the training is 12 * 96 = 1152.",1152,is_blank,['dodge2022'],is_blank,['The number of GPUs of that type in Table 2 from dodge2022 is 96 H100 GPUs per node. The cluster contains 12 nodes.'],"The context mentions a training run on a cluster containing 12 nodes and 96 H100s, so we can calculate the total number of H100 GPUs by multiplying the number of nodes by the number of H100 GPUs per node."
q247,"During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?",The average GPU power for a single node while actively training during the first 300 logging steps of OLMo 2 7B training is over 600W.,over 600,W,['morrison2025'],is_blank,"The average GPU power for a single node while actively training during the first 300 logging steps of OLMo 2 7B training is over 600W, as stated in the context.",is_blank
q248,How many pounds of CO2e are estimated for an average human life in one year (globally)?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q249,What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?,The speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs is approximately 1.25 times.,1.25,is_blank,['samsi2024'],is_blank,is_blank,is_blank
q250,What is the energy consumption (in Wh) of a single short query to GPT-4o?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q251,"In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?",The Max-Performance instance (g6e.xlarge) was approximately 280% more expensive than InferSave's top choice.,280,%,['kim2025'],is_blank,"InferSave's top choice cost was \$0.71, while Max-Performance cost was \$2.699, which is about 280% more expensive.",is_blank
q252,Which GPU architecture was most energy-efficient for models generating only a single classification token?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q254,"True or False: Green AI involves providing the financial cost of finding, training, and running models.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q255,"As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q256,(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?,The difference in average system power per processor between the TPU v2 and the V100 GPU is 54,54,Watts,['patterson2021'],is_blank,"Patterson2021: 'TPU v2: 221 Watts, V100 GPU: 325 Watts'",is_blank
q257,How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q258,How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?,Facebook's recommendation model sizes have increased by 20× between 2019 and 2021.,20,times,['wu2021a'],is_blank,is_blank,is_blank
q259,Which model ranked highest in a recent eco-efficiency analysis using DEA?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q260,"True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.",1,is_blank,['luccioni2025a'],is_blank,"The context mentions that high turnover in AI hardware is accelerating e-waste output and estimates that AI will generate an additional 1.2–5 million metric tons of e-waste by 2030, which implies shorter lifetimes for smartphones.","The context discusses the increasing amount of electronic waste (e-waste) due to the accelerating output of AI hardware, including GPUs. This is estimated to generate an additional 1.2–5 million metric tons of e-waste by 2030. Given that smartphones are a type of electronic device and the context does not provide any information suggesting otherwise, it can be inferred that high turnover in AI hardware refers to frequent upgrades or replacements of smartphones, which contributes to shorter lifetimes for these devices."
q261,True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.,True,1,is_blank,['erben2023'],is_blank,The context states that 'intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.',"The question asks if the intra-zone scaling with T4 GPUs achieved a nearly linear per-GPU speedup for CV models. The context explicitly states this, so the answer is true."
q264,"What is the context window size, in tokens, for the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q265,True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.,False,0,is_blank,['10'],is_blank,The context does not provide a comparison between LLMs and diffusion models regarding power draw during inference.,"The context discusses various topics related to large language models (LLMs) but does not explicitly compare their power consumption during inference with that of diffusion models. Therefore, no verbatim quote can be provided to support the answer."
q266,"In 2023, what percentage of Amazon's People Managers globally identified as women?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q267,"When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q268,"True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.",False,0,is_blank,['khan2025'],is_blank,"In the context, it is stated that 'While the reduction in carbon footprint is consistent, performance trade-offs are evident, with some metrics experiencing marginal improvements and others showing slight declines.' This implies that not all performance metrics improved after optimization.","The statement claims that accuracy and F1 scores always improved after optimization. However, the context states that while some metrics experienced marginal improvements, others showed slight declines, which contradicts the claim."
q269,"What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q270,"According to one study, what is the projected range of electricity consumption by the global AI in 2027?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q271,"How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q273,What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q274,"True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The AI Act does not mandate providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.",0,is_blank,['ebert2024'],is_blank,"The context mentions that the AI Act fails to address indirect greenhouse gas emissions from AI applications (ebert2024, Page: 0, Score: 1.841). It also states that the Act does not mandate the disclosure of energy consumption during the inference phase, a crucial omission given the long-term environmental impact of AI applications (ebert2024, Page: 0, Score: 0.977).","The context discusses that the AI Act fails to address indirect greenhouse gas emissions from AI applications and does not mandate the disclosure of energy consumption during the inference phase, which is a crucial omission given the long-term environmental impact of AI applications. Since greenhouse gas emissions are a type of indirect emission and the inference phase is when AI applications are actively used, it can be inferred that the Act does not mandate providers to disclose the greenhouse gas emissions of AI applications such as oil and gas exploration."
q275,"According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?",The maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region is 19.0%.,19.0,%,['dodge2022'],is_blank,The value can be found in Table 4 under 'P&R' column for DenseNet 201 and West US region.,The table shows the gain in percent averaged over the year and across the 16 regions for the Flexible Start (FS) and Pause and Resume (P&R) optimizations allowing for a 24h increase in job duration. The value 19.0% is found under 'P&R' column for DenseNet 201 and West US region.
q276,"Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q277,"True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","In Yelp sentiment analysis benchmarks, traditional models did not achieve accuracy comparable to large language models.",0,is_blank,['zschache2025'],is_blank,"The context states that 'traditional models perform considerably worse than LLMs in Yelp sentiment analysis' (zschache2025, Page: 0, Score: 5.401).",The context indicates that traditional models have lower accuracy compared to large language models (LLMs) in Yelp sentiment analysis.
q279,"As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q281,What percent of power usage did Amazon's AWS cover with renewable energy in 2018?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q283,At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?,Energy consumption should be reported at the cumulative server level.,cumulative server level,is_blank,['ebert2024'],is_blank,reported at the cumulative server level (see also [\[4\]](#page-10-0)),is_blank
q284,"In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?",The GPU accounts for 74% of the total electricity consumption.,74,%,['dodge2022'],is_blank,"<span id=""page-5-0"">Table 1.</span> The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), ... The GPU alone accounts for 74% of the total energy consumption.","The question asks about the percentage of electricity consumption attributed to the GPU in an experiment to train a BERT-base model. The context provides Table 1, which lists the electricity consumption when training BERT base on a single NVIDIA TITAN X GPU (12GB). The table shows that the GPU alone accounts for 74% of the total energy consumption."
q285,Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?,4 GPUs,4,number of GPUs,['3'],is_blank,"In source [3], it is mentioned that 'at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any Llama model.'","The question asks for the number of NVIDIA A100-80GB GPUs required to serve a Llama2-70b model. The context provides the minimum hardware requirements for serving any Llama model, which includes 4 A100-80GB GPUs."
q286,What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q287,How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q288,What is the estimated upfront hardware acquisition cost to train GPT-4?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q289,True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.,The umbrella term 'Sustainable AI' was proposed to encompass both using AI in climate-positive applications and improving upon the (environmental) sustainability of AI approaches themselves.,1,is_blank,['luccioni2025b'],is_blank,is_blank,is_blank
q290,What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q291,"When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q292,"In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q293,"According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q294,"When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q295,By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?,The JetMoE-8B architecture reduces inference computation by approximately 70% compared to the Llama2-7B model.,70,is_blank,['shen2024'],is_blank,is_blank,is_blank
q298,"What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","626,155 lbs CO2e",626155,lbs CO2e,['strubell2019'],is_blank,"Training BERT, a large language model (LLM), as quantified by Strubell et al. reaches 626,155 pounds of 𝐶𝑂<sup>2</sup> emissions [\[192\]](#page-21-0)",is_blank
q299,"What was the estimated training energy of the full GPT-3 model, in MWh?",1287 MWh,1287,MWh,['patterson2021'],is_blank,"Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity [4]",The context provides an estimate for the training energy consumption of GPT-3 in MWh. This value is explicitly stated in the provided reference.
q300,"True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.",1,is_blank,['xia2024'],is_blank,Takeaway 2 and Takeaway 3 from xia2024,"The context mentions that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning (Takeaway 2) and that matrix multiplication operations in the MoE layer contribute significantly to the end-to-end execution time, making the MoE layer the costliest component in LLM fine-tuning (Takeaway 3). These statements imply that optimizing the performance of the MoE layer is crucial for improving the overall cost of LLM fine-tuning, which suggests that the MoE layer is often a targeted layer when enhancing performance."
q301,What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q302,"True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q303,How many hectares of land were occupied by new AI data centers globally in 2022?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q305,"A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q307,"In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q308,In what year did the practice of directly releasing environmental information for notable models peak before declining?,2022,2022,is_blank,['luccioni2025c'],is_blank,The practice of directly releasing environmental information for notable models peaked before declining.,is_blank
q309,"What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?",28.5 days,28.5,days,['morris2025'],is_blank,"The OLMo 60M model consumes at least 402 kL of water during training (Table 1). The average person in the US consumes approximately 79 liters of water per day. Therefore, to find out how many days it is equivalent to, we divide the total water consumption by the daily water consumption of an average person in the US: 402 kL / 79 L/day = 5103.57 days. Since there are approximately 365 days in a year, we can convert this value into days per year and then multiply it by the number of years in a decade (10) to get the equivalent water usage for one person in a decade: 5103.57 days/year * 365 days/year * 10 years = 28,549,455 days. Finally, we convert this value into days: 28,549,455 days / 365 days/year = 28.5 days.","The context provides the water consumption for training an OLMo 60M model (402 kL). To find out the equivalent water usage for one person in the US, we divide the total water consumption by the daily water consumption of an average person in the US. We then convert this value into days per year and multiply it by the number of years in a decade to get the equivalent water usage for one person in a decade."
q310,How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?,Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q311,True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.,Adding compute resources can reduce costs for fine-tuning LLMs.,1,is_blank,"['xia2024', 'shen2024']",is_blank,"From context [2], 'Using this model, we showed the dollar amount that needs to be budgeted for fine-tuning LLMs, which is much lower than pre-training. For example, our model predicted that finetuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of \$3460.'",is_blank
q312,"According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q313,"According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?",The total public health burden of U.S. data centers could be valued at up to more than \$20.9 billion.,20900000000,is_blank,"['2', '4']",is_blank,Based on the high-growth scenario considered in [4],is_blank
q314,What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?,The estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU is \$32.7.,32.7,$/hr,['xia2024'],is_blank,"Table IV in the context shows the cost (\$) for fine-tuning Mixtral on the MATH dataset with a sparse setup, using 10 epochs on different GPUs for a realistic cost estimate.",is_blank
q315,"For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q317,"What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q318,True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.,GPU-level power consumption monitoring is not recommended as the preferred method for reporting overall AI energy use.,0,is_blank,['ebert2024'],is_blank,The context states: 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.',The context provides a clear statement that advocates against using GPU-level power consumption monitoring for overall energy measurements.
q319,"In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",Training accounted for half of the model's overall emissions,0.5,is_blank,['luccioni2025b'],is_blank,is_blank,is_blank
q320,What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?,1 GPU,1,is_blank,['samsi2024'],is_blank,"Table II: Baseline configurations for LLaMA 7B, 13B, and 65B",The table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. The 7B model can run on a single GPU.
q321,"When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",Unable to answer with confidence based on the provided documents.,is_blank,is_blank,is_blank,is_blank,is_blank,is_blank
q322,What is the estimated CO2 emission in metric tons for one year of average US home energy use?,98.2 homes in the U.S.,98.2,is_blank,['morrison2025'],is_blank,"6.5 tanker trucks' worth of gasoline burned, emissions from the average yearly energy use for 98.2 homes in the U.S.",is_blank
q323,"On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",The JetMoE-8B model achieved a score of 27.8 on the GSM8k benchmark.,27.8,is_blank,['shen2024'],is_blank,Table 8 in the context shows that JetMoE-8B scored 27.8 on the GSM8k benchmark.,"The table provides scores for various models, including JetMoE-8B, on different benchmarks. The score of 27.8 is listed under 'GSM8k' for JetMoE-8B."
